{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c760a235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Environment configured\n"
     ]
    }
   ],
   "source": [
    "# Disable widget-based progress bars\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # Add this!\n",
    "\n",
    "# Use simple progress bars instead of widgets\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "print(\"✓ Environment configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a117fc5d",
   "metadata": {},
   "source": [
    "### Clear GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f987c645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory available: 94.97 GB total\n",
      "GPU Memory allocated: 0.00 GB\n",
      "GPU Memory reserved: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Force cleanup\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check memory\n",
    "print(f\"GPU Memory available: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB total\")\n",
    "print(f\"GPU Memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "print(f\"GPU Memory reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628ebe35",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6171159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model with MXFP4 quantization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce312d372894feabb7f74f542312b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "✓ Model loaded successfully!\n",
      "GPU Memory allocated: 38.96 GB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Point to local model directory\n",
    "model_id = \"./gpt-oss-20b/\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "print(\"Loading model with MXFP4 quantization...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "print(\"✓ Model loaded successfully!\")\n",
    "print(f\"GPU Memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3738915d",
   "metadata": {},
   "source": [
    "## Run inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c19a1758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating...\n",
      "analysisThe user asks: \"What is 2+2?\" It's a simple arithmetic question. The answer is 4. We should respond with the answer. Possibly also mention that it's 4. The user didn't ask for anything else. So answer: 4.assistantfinal4\n",
      "\n",
      "Response: analysisThe user asks: \"What is 2+2?\" It's a simple arithmetic question. The answer is 4. We should respond with the answer. Possibly also mention that it's 4. The user didn't ask for anything else. So answer: 4.assistantfinal4\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "# Test inference\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is 2+2?\"}\n",
    "]\n",
    "\n",
    "# Apply chat template (harmony format)\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate\n",
    "print(\"Generating...\")\n",
    "\n",
    "\n",
    "# Use streamer to see tokens as they're generated\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=False,\n",
    "    streamer=streamer  # Add this\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "print(\"\\nResponse:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54c8b02",
   "metadata": {},
   "source": [
    "# Steering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70be89d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import timez\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import circuitsvis as cv\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from IPython.display import display\n",
    "from jaxtyping import Float\n",
    "from nnsight import CONFIG, LanguageModel\n",
    "from openai import OpenAI\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from torch import Tensor\n",
    "\n",
    "# Hide some info logging messages from nnsight\n",
    "logging.disable(sys.maxsize)\n",
    "\n",
    "t.set_grad_enabled(False)\n",
    "device = t.device(\n",
    "    \"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "\n",
    "from plotly_utils import imshow\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94fba5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of heads: 64\n",
      "Number of layers: 24\n",
      "Model (hidden) dimension: 2880\n",
      "Head dimension: 64\n",
      "\n",
      "Entire config:  GptOssConfig {\n",
      "  \"architectures\": [\n",
      "    \"GptOssForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 200002,\n",
      "  \"experts_per_token\": 4,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2880,\n",
      "  \"initial_context_length\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2880,\n",
      "  \"layer_types\": [\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"gpt_oss\",\n",
      "  \"num_attention_heads\": 64,\n",
      "  \"num_experts_per_tok\": 4,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_local_experts\": 32,\n",
      "  \"output_router_logits\": false,\n",
      "  \"pad_token_id\": 199999,\n",
      "  \"quantization_config\": {\n",
      "    \"modules_to_not_convert\": [\n",
      "      \"model.layers.*.self_attn\",\n",
      "      \"model.layers.*.mlp.router\",\n",
      "      \"model.embed_tokens\",\n",
      "      \"lm_head\"\n",
      "    ],\n",
      "    \"quant_method\": \"mxfp4\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"beta_fast\": 32.0,\n",
      "    \"beta_slow\": 1.0,\n",
      "    \"factor\": 32.0,\n",
      "    \"original_max_position_embeddings\": 4096,\n",
      "    \"rope_type\": \"yarn\",\n",
      "    \"truncate\": false\n",
      "  },\n",
      "  \"rope_theta\": 150000,\n",
      "  \"router_aux_loss_coef\": 0.9,\n",
      "  \"sliding_window\": 128,\n",
      "  \"swiglu_limit\": 7.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.56.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 201088\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel(\"./gpt-oss-20b\", device_map=\"auto\", torch_dtype=t.bfloat16)\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "print()\n",
    "\n",
    "N_HEADS = model.config.num_attention_heads\n",
    "N_LAYERS = model.config.num_hidden_layers\n",
    "D_MODEL = model.config.hidden_size\n",
    "D_HEAD = model.config.head_dim\n",
    "\n",
    "print(f\"Number of heads: {N_HEADS}\")\n",
    "print(f\"Number of layers: {N_LAYERS}\")\n",
    "print(f\"Model (hidden) dimension: {D_MODEL}\")\n",
    "print(f\"Head dimension: {D_HEAD}\\n\")\n",
    "\n",
    "print(\"Entire config: \", model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2cac15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
