{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c760a235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Environment configured\n"
     ]
    }
   ],
   "source": [
    "# Disable widget-based progress bars\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # Add this!\n",
    "\n",
    "# Use simple progress bars instead of widgets\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "print(\"✓ Environment configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a117fc5d",
   "metadata": {},
   "source": [
    "### Clear GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f987c645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory available: 94.97 GB total\n",
      "GPU Memory allocated: 0.00 GB\n",
      "GPU Memory reserved: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Force cleanup\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check memory\n",
    "print(f\"GPU Memory available: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB total\")\n",
    "print(f\"GPU Memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "print(f\"GPU Memory reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628ebe35",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6171159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model with MXFP4 quantization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6ed95145d546179e805d34e8273d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "✓ Model loaded successfully!\n",
      "GPU Memory allocated: 38.96 GB\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import torch\n",
    "\n",
    "# # Point to local model directory\n",
    "# model_id = \"./gpt-oss-20b/\"\n",
    "\n",
    "# # Load model and tokenizer\n",
    "# print(\"Loading model with MXFP4 quantization...\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=\"auto\",\n",
    "#     device_map=\"auto\",\n",
    "#     trust_remote_code=True,\n",
    "#     local_files_only=True,\n",
    "#     low_cpu_mem_usage=True,\n",
    "# )\n",
    "\n",
    "# print(\"Loading tokenizer...\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     model_id,\n",
    "#     local_files_only=True\n",
    "# )\n",
    "\n",
    "# print(\"✓ Model loaded successfully!\")\n",
    "# print(f\"GPU Memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3738915d",
   "metadata": {},
   "source": [
    "## Run inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19a1758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating...\n",
      "analysisThe user asks: \"What is 2+2?\" It's a simple arithmetic question. The answer is 4. We should respond with the answer. Possibly also mention that it's 4. The user didn't ask for anything else. So answer: 4.assistantfinal4\n",
      "\n",
      "Response: analysisThe user asks: \"What is 2+2?\" It's a simple arithmetic question. The answer is 4. We should respond with the answer. Possibly also mention that it's 4. The user didn't ask for anything else. So answer: 4.assistantfinal4\n"
     ]
    }
   ],
   "source": [
    "# from transformers import TextStreamer\n",
    "# # Test inference\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"What is 2+2?\"}\n",
    "# ]\n",
    "\n",
    "# # Apply chat template (harmony format)\n",
    "# text = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     tokenize=False,\n",
    "#     add_generation_prompt=True\n",
    "# )\n",
    "\n",
    "# inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# # Generate\n",
    "# print(\"Generating...\")\n",
    "\n",
    "\n",
    "# # Use streamer to see tokens as they're generated\n",
    "# streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "# outputs = model.generate(\n",
    "#     **inputs,\n",
    "#     max_new_tokens=256,\n",
    "#     do_sample=False,\n",
    "#     streamer=streamer  # Add this\n",
    "# )\n",
    "\n",
    "# response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "# print(\"\\nResponse:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54c8b02",
   "metadata": {},
   "source": [
    "# Steering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70be89d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import timez\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import circuitsvis as cv\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from IPython.display import display\n",
    "from jaxtyping import Float\n",
    "from nnsight import CONFIG, LanguageModel\n",
    "from openai import OpenAI\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from torch import Tensor\n",
    "\n",
    "# Hide some info logging messages from nnsight\n",
    "logging.disable(sys.maxsize)\n",
    "\n",
    "t.set_grad_enabled(False)\n",
    "device = t.device(\n",
    "    \"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "\n",
    "from plotly_utils import imshow\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94fba5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of heads: 64\n",
      "Number of layers: 24\n",
      "Model (hidden) dimension: 2880\n",
      "Head dimension: 64\n",
      "\n",
      "Entire config:  GptOssConfig {\n",
      "  \"architectures\": [\n",
      "    \"GptOssForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 200002,\n",
      "  \"experts_per_token\": 4,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2880,\n",
      "  \"initial_context_length\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2880,\n",
      "  \"layer_types\": [\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"gpt_oss\",\n",
      "  \"num_attention_heads\": 64,\n",
      "  \"num_experts_per_tok\": 4,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_local_experts\": 32,\n",
      "  \"output_router_logits\": false,\n",
      "  \"pad_token_id\": 199999,\n",
      "  \"quantization_config\": {\n",
      "    \"modules_to_not_convert\": [\n",
      "      \"model.layers.*.self_attn\",\n",
      "      \"model.layers.*.mlp.router\",\n",
      "      \"model.embed_tokens\",\n",
      "      \"lm_head\"\n",
      "    ],\n",
      "    \"quant_method\": \"mxfp4\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"beta_fast\": 32.0,\n",
      "    \"beta_slow\": 1.0,\n",
      "    \"factor\": 32.0,\n",
      "    \"original_max_position_embeddings\": 4096,\n",
      "    \"rope_type\": \"yarn\",\n",
      "    \"truncate\": false\n",
      "  },\n",
      "  \"rope_theta\": 150000,\n",
      "  \"router_aux_loss_coef\": 0.9,\n",
      "  \"sliding_window\": 128,\n",
      "  \"swiglu_limit\": 7.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.56.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 201088\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel(\"./gpt-oss-20b\", device_map=\"auto\", torch_dtype=t.bfloat16)\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "print()\n",
    "\n",
    "N_HEADS = model.config.num_attention_heads\n",
    "N_LAYERS = model.config.num_hidden_layers\n",
    "D_MODEL = model.config.hidden_size\n",
    "D_HEAD = model.config.head_dim\n",
    "\n",
    "print(f\"Number of heads: {N_HEADS}\")\n",
    "print(f\"Number of layers: {N_LAYERS}\")\n",
    "print(f\"Model (hidden) dimension: {D_MODEL}\")\n",
    "print(f\"Head dimension: {D_HEAD}\\n\")\n",
    "\n",
    "print(\"Entire config: \", model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6de2300",
   "metadata": {},
   "source": [
    "### Test tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb2cac15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 2500,  2804,   413, 12453]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
      "This must be Thursday\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(\"This must be Thursday\", return_tensors=\"pt\"))\n",
    "\n",
    "print(tokenizer.decode([ 2500,  2804,   413, 12453]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd36d93",
   "metadata": {},
   "source": [
    "## Access Hidden States\n",
    "\n",
    "### GPT-OSS-20B Quick Reference\n",
    "\n",
    "- 24 layers | 2880 hidden | 64 heads | 201K vocab\n",
    "- **Key difference**: `model.model.layers[i]` not `model.transformer.h[i]`\n",
    "\n",
    "### Basic Usage\n",
    "```python\n",
    "with model.trace(prompt, remote=False):\n",
    "    # Hidden states from layer i\n",
    "    hidden = model.model.layers[i].output[0].save()\n",
    "    \n",
    "    # Logits (last token)\n",
    "    logits = model.lm_head.output[0, -1].save()\n",
    "```\n",
    "\n",
    "### Steering\n",
    "```python\n",
    "with model.trace(prompt, remote=False):\n",
    "    hidden = model.model.layers[i].output[0]\n",
    "    hidden[:, -1, :] += steering_vector\n",
    "    logits = model.lm_head.output[0, -1].save()\n",
    "```\n",
    "\n",
    "### Multi-Layer Extraction\n",
    "```python\n",
    "with model.trace(prompt, remote=False):\n",
    "    all_hidden = [model.model.layers[i].output[0].save() for i in range(24)]\n",
    "```\n",
    "\n",
    "## Layer Components\n",
    "```python\n",
    "model.model.layers[i].self_attn     # Attention block\n",
    "model.model.layers[i].mlp           # MoE block\n",
    "model.model.embed_tokens            # Embeddings\n",
    "model.model.norm                    # Final norm\n",
    "model.lm_head                       # Output layer\n",
    "```\n",
    "\n",
    "## Expected Shapes\n",
    "\n",
    "- Hidden: `(1, seq_len, 2880)`\n",
    "- Logits: `(201088,)` for single token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "358b5ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501ea0fe6287497792dcc1dcb21294cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape = torch.Size([201088]) = (vocab_size,)\n",
      "Predicted token ID = 12650\n",
      "Predicted token = ' Paris'\n",
      "\n",
      "resid.shape = torch.Size([8, 2880]) = (batch_size, seq_len, d_model)\n"
     ]
    }
   ],
   "source": [
    "# If you have an API key & want to work remotely, then set REMOTE = True and replace \"YOUR-API-KEY\"\n",
    "# with your actual key. If not, then leave REMOTE = False.\n",
    "REMOTE = False\n",
    "prompt = \"The Eiffel Tower is in the city of\"\n",
    "\n",
    "with model.trace(prompt, remote=REMOTE):\n",
    "    # Save the model's hidden states\n",
    "    hidden_states = model.model.layers[-1].output[0].save()\n",
    "\n",
    "    # Save the model's logit output\n",
    "    logits = model.lm_head.output[0, -1].save()\n",
    "\n",
    "# Get the model's logit output, and it's next token prediction\n",
    "print(f\"logits.shape = {logits.shape} = (vocab_size,)\")\n",
    "print(\"Predicted token ID =\", predicted_token_id := logits.argmax().item())\n",
    "print(f\"Predicted token = {tokenizer.decode(predicted_token_id)!r}\")\n",
    "\n",
    "# Print the shape of the model's residual stream\n",
    "print(f\"\\nresid.shape = {hidden_states.shape} = (batch_size, seq_len, d_model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72536a09",
   "metadata": {},
   "source": [
    "# Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2d63c0",
   "metadata": {},
   "source": [
    "## Load ToM Datasets\n",
    "\n",
    "Instead of using the ICL dataset, we'll load two Theory of Mind (ToM) datasets:\n",
    "1. **ToM Dataset** (`first_order_1_tom_prompts.jsonl`) - Questions that require theory of mind reasoning\n",
    "2. **No-ToM Dataset** (`first_order_1_no_tom_prompts.jsonl`) - Questions that don't require theory of mind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04dd3a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToM Dataset (requires ToM):\n",
      "  ToMDataset(path=first_order_1_tom_prompts.jsonl, size=404, requires_tom=True)\n",
      "  Example prompt:\n",
      "Story:\n",
      "Isabella entered the den.\n",
      "Olivia entered the den.\n",
      "Isabella dislikes the pumpkin\n",
      "The broccoli is in the blue_pantry.\n",
      "Isabella exited the den.\n",
      "Olivia moved the broccoli to the red_drawer.\n",
      "Abigail...\n",
      "  Example answer: blue_pantry\n",
      "\n",
      "No-ToM Dataset (doesn't require ToM):\n",
      "  ToMDataset(path=first_order_1_no_tom_prompts.jsonl, size=595, requires_tom=False)\n",
      "  Example prompt:\n",
      "Story:\n",
      "Aria entered the front_yard.\n",
      "Aiden entered the front_yard.\n",
      "The grapefruit is in the green_bucket.\n",
      "Aria moved the grapefruit to the blue_container.\n",
      "Aiden exited the front_yard.\n",
      "Noah entered the ...\n",
      "  Example answer: blue_container\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "class ToMDataset:\n",
    "    \"\"\"\n",
    "    Dataset for Theory of Mind (ToM) tasks from JSONL files.\n",
    "    \n",
    "    Inputs:\n",
    "        jsonl_path: Path to the JSONL file containing prompts\n",
    "        size: Optional limit on number of examples to load (None = load all)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, jsonl_path: str, size: int = None):\n",
    "        self.jsonl_path = Path(jsonl_path)\n",
    "        self.data = []\n",
    "        self.prompts = []\n",
    "        self.completions = []\n",
    "        self.answers = []\n",
    "        self.question_types = []\n",
    "        self.story_types = []\n",
    "        self.requires_tom = []\n",
    "        \n",
    "        # Load data from JSONL file\n",
    "        with open(self.jsonl_path, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if size is not None and i >= size:\n",
    "                    break\n",
    "                item = json.loads(line)\n",
    "                self.data.append(item)\n",
    "                self.prompts.append(item['prompt'])\n",
    "                # Completions are the answers with a leading space\n",
    "                self.completions.append(' ' + item['answer'])\n",
    "                self.answers.append(item['answer'])\n",
    "                self.question_types.append(item['question_type'])\n",
    "                self.story_types.append(item['story_type'])\n",
    "                self.requires_tom.append(item['requires_tom'])\n",
    "        \n",
    "        self.size = len(self.data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"ToMDataset(path={self.jsonl_path.name}, size={self.size}, requires_tom={self.requires_tom[0] if self.size > 0 else 'N/A'})\"\n",
    "\n",
    "\n",
    "# Load the two datasets\n",
    "tom_dataset = ToMDataset(\n",
    "    \"tom_benchmarks/tomi/tomi_pairs/first_order_1_tom_prompts.jsonl\"\n",
    ")\n",
    "no_tom_dataset = ToMDataset(\n",
    "    \"tom_benchmarks/tomi/tomi_pairs/first_order_1_no_tom_prompts.jsonl\"\n",
    ")\n",
    "\n",
    "print(\"ToM Dataset (requires ToM):\")\n",
    "print(f\"  {tom_dataset}\")\n",
    "print(f\"  Example prompt:\\n{tom_dataset.prompts[0][:200]}...\")\n",
    "print(f\"  Example answer: {tom_dataset.answers[0]}\")\n",
    "\n",
    "print(\"\\nNo-ToM Dataset (doesn't require ToM):\")\n",
    "print(f\"  {no_tom_dataset}\")\n",
    "print(f\"  Example prompt:\\n{no_tom_dataset.prompts[0][:200]}...\")\n",
    "print(f\"  Example answer: {no_tom_dataset.answers[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b49e31c",
   "metadata": {},
   "source": [
    "### Usage Notes\n",
    "\n",
    "The `ToMDataset` class has the same interface as `ICLDataset`, so it can be used interchangeably:\n",
    "- `.prompts` - list of prompt strings\n",
    "- `.completions` - list of completion strings (with leading space)\n",
    "- `.answers` - list of answer strings\n",
    "- `.size` - number of examples\n",
    "\n",
    "**Key difference from ICLDataset**: \n",
    "- ToMDataset does **not** have a `create_corrupted_dataset()` method\n",
    "- If you need corrupted data for contrastive analysis, you'll need to implement that separately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b543a562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXAMPLE FROM TOM DATASET (requires ToM reasoning):\n",
      "================================================================================\n",
      "Story:\n",
      "Isabella entered the den.\n",
      "Olivia entered the den.\n",
      "Isabella dislikes the pumpkin\n",
      "The broccoli is in the blue_pantry.\n",
      "Isabella exited the den.\n",
      "Olivia moved the broccoli to the red_drawer.\n",
      "Abigail entered the garden.\n",
      "Isabella entered the garden.\n",
      "\n",
      "Question: Where will Isabella look for the broccoli?\n",
      "Answer:\n",
      "Correct answer: blue_pantry\n",
      "Story type: false_belief\n",
      "Requires ToM: True\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE FROM NO-TOM DATASET (doesn't require ToM):\n",
      "================================================================================\n",
      "Story:\n",
      "Aria entered the front_yard.\n",
      "Aiden entered the front_yard.\n",
      "The grapefruit is in the green_bucket.\n",
      "Aria moved the grapefruit to the blue_container.\n",
      "Aiden exited the front_yard.\n",
      "Noah entered the playroom.\n",
      "\n",
      "Question: Where will Aiden look for the grapefruit?\n",
      "Answer:\n",
      "Correct answer: blue_container\n",
      "Story type: true_belief\n",
      "Requires ToM: False\n"
     ]
    }
   ],
   "source": [
    "# Example: View a complete example from each dataset\n",
    "print(\"=\" * 80)\n",
    "print(\"EXAMPLE FROM TOM DATASET (requires ToM reasoning):\")\n",
    "print(\"=\" * 80)\n",
    "print(tom_dataset[0]['prompt'])\n",
    "print(f\"Correct answer: {tom_dataset[0]['answer']}\")\n",
    "print(f\"Story type: {tom_dataset[0]['story_type']}\")\n",
    "print(f\"Requires ToM: {tom_dataset[0]['requires_tom']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE FROM NO-TOM DATASET (doesn't require ToM):\")\n",
    "print(\"=\" * 80)\n",
    "print(no_tom_dataset[0]['prompt'])\n",
    "print(f\"Correct answer: {no_tom_dataset[0]['answer']}\")\n",
    "print(f\"Story type: {no_tom_dataset[0]['story_type']}\")\n",
    "print(f\"Requires ToM: {no_tom_dataset[0]['requires_tom']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "048b77ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text only: We have a story with characters: Isabella, Olivia, Abigail. Items: broccoli, pumpkin, blue_pantry, red_drawer. The question: \"Where will Isabella look for the broccoli?\" We need to infer from the story. Let's parse the story:\n",
      "\n",
      "- Isabella entered the den.\n",
      "- Olivia entered the den.\n",
      "- Isabella dislikes the pumpkin.\n",
      "- The broccoli is in the blue_pantry.\n",
      "- Isabella exited the den.\n",
      "- Olivia moved the broccoli to the red_drawer.\n",
      "- Abigail entered the garden.\n",
      "- Isabella entered the garden.\n",
      "\n",
      "We need to answer: Where will Isabella look for the broccoli? The story says the broccoli was originally in the blue_pantry. Then Olivia moved the broccoli to the red_drawer. So the broccoli is now in the red_drawer. Isabella is in the garden. So where will she look? She will look in the red_drawer. But maybe she will look in the garden? But the broccoli is not in the garden. The question: \"Where will Isabella look for the broccoli?\" The answer: She will look in the red_drawer. But maybe she will look in the garden? Let's think: The story: Isabella is in the garden. She might look for the broccoli in the garden. But the broccoli is in the red_drawer. So she might look in the red_drawer. The question is ambiguous: \"Where will Isabella look for the broccoli?\" The answer: She will look in the red_drawer. But maybe she will look in the garden? But the broccoli is not in the garden. The question likely expects: She will look in the red_drawer. So answer: The red_drawer. So the answer: She will look in the red_drawer. So the answer: The red_drawer. So the answer: She will look in the red_drawer. So the answer: She will look in the red_drawer. So the answer: She will look in the red_drawer. So the answer: She will look in the red_drawer. So the answer: She will look in the red_drawer. So the answer: She will look in the red_drawer. So the answer: She will look in the red_drawer. So the answer: She will look in the red_drawer. So the answer: She will look in the red_drawer. So the answer: She will look in the red_drawer. So the answer: She will look in the red_draw\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Isabella entered the den.\n",
    "Olivia entered the den.\n",
    "Isabella dislikes the pumpkin\n",
    "The broccoli is in the blue_pantry.\n",
    "Isabella exited the den.\n",
    "Olivia moved the broccoli to the red_drawer.\n",
    "Abigail entered the garden.\n",
    "Isabella entered the garden.\n",
    "\n",
    "Question: Where will Isabella look for the broccoli?\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the prompt and move to GPU\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Use nnsight's generate method (pass tokenized inputs)\n",
    "output = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_new_tokens=500,\n",
    "    do_sample=False,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "\n",
    "# Or decode just the new tokens\n",
    "new_tokens = tokenizer.decode(output[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "print(\"\\nGenerated text only:\", new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d433604e",
   "metadata": {},
   "source": [
    "## Calculate Function Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6822b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_fn_vector(\n",
    "    model: model,\n",
    "    dataset: no_tom_dataset,\n",
    "    head_list: list[tuple[int, int]],\n",
    ") -> Float[Tensor, \"D_MODEL\"]:\n",
    "    \"\"\"\n",
    "    Returns a vector of length `D_MODEL`, containing the sum of vectors written to the residual\n",
    "    stream by the attention heads in `head_list`, averaged over all inputs in `dataset`.\n",
    "\n",
    "    Inputs:\n",
    "        model: LanguageModel\n",
    "            the transformer you're doing this computation with\n",
    "        dataset: ICLDataset\n",
    "            the dataset of clean prompts from which we'll extract the function vector (we'll also\n",
    "            create a corrupted version of this dataset for interventions)\n",
    "        head_list: list[tuple[int, int]]\n",
    "            list of attention heads we're calculating the function vector from\n",
    "    \"\"\"\n",
    "    # Turn head_list into a dict of {layer: heads we need in this layer}\n",
    "    head_dict = defaultdict(set)\n",
    "    for layer, head in head_list:\n",
    "        head_dict[layer].add(head)\n",
    "\n",
    "    fn_vector_list = []\n",
    "\n",
    "    with model.trace(dataset.prompts, remote=REMOTE):\n",
    "        for layer, head_list in head_dict.items():\n",
    "            # Get the output projection layer\n",
    "            out_proj = model.transformer.h[layer].attn.out_proj\n",
    "\n",
    "            # Get the mean output projection input (note, setting values of this tensor will not\n",
    "            # have downstream effects on other tensors)\n",
    "            hidden_states = out_proj.input[:, -1].mean(dim=0)\n",
    "\n",
    "            # Zero-ablate all heads which aren't in our list, then get the output (which\n",
    "            # will be the sum over the heads we actually do want!)\n",
    "            heads_to_ablate = set(range(N_HEADS)) - head_dict[layer]\n",
    "            for head in heads_to_ablate:\n",
    "                hidden_states.reshape(N_HEADS, D_HEAD)[head] = 0.0\n",
    "\n",
    "            # Now that we've zeroed all unimportant heads, get the output & add it to the list\n",
    "            # (we need a single batch dimension so we can use `out_proj`)\n",
    "            out_proj_output = out_proj(hidden_states.unsqueeze(0)).squeeze()\n",
    "            fn_vector_list.append(out_proj_output.save())\n",
    "\n",
    "    # We sum all attention head outputs to get our function vector\n",
    "    fn_vector = sum([v for v in fn_vector_list])\n",
    "\n",
    "    assert fn_vector.shape == (D_MODEL,)\n",
    "    return fn_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862f827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intervene_with_fn_vector(\n",
    "    model: LanguageModel,\n",
    "    word: str,\n",
    "    layer: int,\n",
    "    fn_vector: Float[Tensor, \"d_model\"],\n",
    "    prompt_template='The word \"{x}\" means',\n",
    "    n_tokens: int = 5,\n",
    ") -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Intervenes with a function vector, by adding it at the last sequence position of a generated\n",
    "    prompt.\n",
    "\n",
    "    Inputs:\n",
    "        model: LanguageModel\n",
    "            the transformer you're doing this computation with\n",
    "        word: str\n",
    "            The word substituted into the prompt template, via prompt_template.format(x=word)\n",
    "        layer: int\n",
    "            The layer we'll make the intervention (by adding the function vector)\n",
    "        fn_vector: Float[Tensor, \"d_model\"]\n",
    "            The vector we'll add to the final sequence position for each new token to be generated\n",
    "        prompt_template:\n",
    "            The template of the prompt we'll use to produce completions\n",
    "        n_tokens: int\n",
    "            The number of additional tokens we'll generate for our unsteered / steered completions\n",
    "\n",
    "    Returns:\n",
    "        completion: str\n",
    "            The full completion (including original prompt) for the no-intervention case\n",
    "        completion_intervention: str\n",
    "            The full completion (including original prompt) for the intervention case\n",
    "    \"\"\"\n",
    "    prompt = prompt_template.format(x=word)\n",
    "\n",
    "    with model.generate(\n",
    "        remote=REMOTE, max_new_tokens=n_tokens, repetition_penalty=1.2\n",
    "    ) as generator:\n",
    "        with model.all():\n",
    "            with generator.invoke(prompt):\n",
    "                tokens = model.generator.output.save()\n",
    "\n",
    "            with generator.invoke(prompt):\n",
    "                model.transformer.h[layer].output[0][:, -1] += fn_vector\n",
    "                tokens_intervention = model.generator.output.save()\n",
    "\n",
    "    completion, completion_intervention = tokenizer.batch_decode(\n",
    "        [tokens.squeeze().tolist(), tokens_intervention.squeeze().tolist()]\n",
    "    )\n",
    "    return completion, completion_intervention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
