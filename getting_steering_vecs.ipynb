{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c760a235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Environment configured\n"
     ]
    }
   ],
   "source": [
    "# Disable widget-based progress bars\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # Add this!\n",
    "\n",
    "# Use simple progress bars instead of widgets\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "print(\"✓ Environment configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a117fc5d",
   "metadata": {},
   "source": [
    "### Clear GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f987c645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory available: 94.97 GB total\n",
      "GPU Memory allocated: 0.00 GB\n",
      "GPU Memory reserved: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Force cleanup\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check memory\n",
    "print(f\"GPU Memory available: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB total\")\n",
    "print(f\"GPU Memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "print(f\"GPU Memory reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628ebe35",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6171159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import torch\n",
    "\n",
    "# # Point to local model directory\n",
    "# model_id = \"./gpt-oss-20b/\"\n",
    "\n",
    "# # Load model and tokenizer\n",
    "# print(\"Loading model with MXFP4 quantization...\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=\"auto\",\n",
    "#     device_map=\"auto\",\n",
    "#     trust_remote_code=True,\n",
    "#     local_files_only=True,\n",
    "#     low_cpu_mem_usage=True,\n",
    "# )\n",
    "\n",
    "# print(\"Loading tokenizer...\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     model_id,\n",
    "#     local_files_only=True\n",
    "# )\n",
    "\n",
    "# print(\"✓ Model loaded successfully!\")\n",
    "# print(f\"GPU Memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3738915d",
   "metadata": {},
   "source": [
    "## Run inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c19a1758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TextStreamer\n",
    "# # Test inference\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"What is 2+2?\"}\n",
    "# ]\n",
    "\n",
    "# # Apply chat template (harmony format)\n",
    "# text = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     tokenize=False,\n",
    "#     add_generation_prompt=True\n",
    "# )\n",
    "\n",
    "# inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# # Generate\n",
    "# print(\"Generating...\")\n",
    "\n",
    "\n",
    "# # Use streamer to see tokens as they're generated\n",
    "# streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "# outputs = model.generate(\n",
    "#     **inputs,\n",
    "#     max_new_tokens=256,\n",
    "#     do_sample=False,\n",
    "#     streamer=streamer  # Add this\n",
    "# )\n",
    "\n",
    "# response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "# print(\"\\nResponse:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54c8b02",
   "metadata": {},
   "source": [
    "# Steering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70be89d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import timez\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import circuitsvis as cv\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from IPython.display import display\n",
    "from jaxtyping import Float\n",
    "from nnsight import CONFIG, LanguageModel\n",
    "from openai import OpenAI\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from torch import Tensor\n",
    "\n",
    "# Hide some info logging messages from nnsight\n",
    "logging.disable(sys.maxsize)\n",
    "\n",
    "t.set_grad_enabled(False)\n",
    "device = t.device(\n",
    "    \"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Import from local plotly_utils.py file (not the pip package)\n",
    "import importlib.util\n",
    "notebook_dir = Path(__file__).parent if \"__file__\" in globals() else Path.cwd()\n",
    "plotly_utils_path = notebook_dir / \"plotly_utils.py\"\n",
    "if not plotly_utils_path.exists():\n",
    "    plotly_utils_path = Path(\"/root/Agent-Robustness-Via-ToM/plotly_utils.py\")\n",
    "spec = importlib.util.spec_from_file_location(\"plotly_utils_local\", str(plotly_utils_path))\n",
    "plotly_utils_local = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(plotly_utils_local)\n",
    "imshow = plotly_utils_local.imshow\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fba5d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load the configuration of './gpt-oss-20b'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure './gpt-oss-20b' is the correct path to a directory containing a config.json file",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHFValidationError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/transformers/utils/hub.py:478\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:106\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mrepo_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfrom_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mto_id\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m arg_name == \u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:160\u001b[39m, in \u001b[36mvalidate_repo_id\u001b[39m\u001b[34m(repo_id)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX.match(repo_id):\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    161\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must use alphanumeric chars or \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m--\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m\u001b[33m are\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    162\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m forbidden, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m cannot start or end the name, max length is 96:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    163\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m--\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m..\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
      "\u001b[31mHFValidationError\u001b[39m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './gpt-oss-20b'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mHFValidationError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/transformers/configuration_utils.py:721\u001b[39m, in \u001b[36mPretrainedConfig._get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    720\u001b[39m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m721\u001b[39m     resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/transformers/utils/hub.py:321\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    269\u001b[39m \u001b[33;03mTries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    270\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    319\u001b[39m \u001b[33;03m```\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/transformers/utils/hub.py:530\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    529\u001b[39m \u001b[38;5;66;03m# Now we try to recover if we can find all files correctly in the cache\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m resolved_files = \u001b[43m[\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_get_cache_file_to_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfull_filenames\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m resolved_files):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/transformers/utils/hub.py:531\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    529\u001b[39m \u001b[38;5;66;03m# Now we try to recover if we can find all files correctly in the cache\u001b[39;00m\n\u001b[32m    530\u001b[39m resolved_files = [\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m     \u001b[43m_get_cache_file_to_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m full_filenames\n\u001b[32m    533\u001b[39m ]\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m resolved_files):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/transformers/utils/hub.py:144\u001b[39m, in \u001b[36m_get_cache_file_to_return\u001b[39m\u001b[34m(path_or_repo_id, full_filename, cache_dir, revision, repo_type)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_cache_file_to_return\u001b[39m(\n\u001b[32m    137\u001b[39m     path_or_repo_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    138\u001b[39m     full_filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    142\u001b[39m ):\n\u001b[32m    143\u001b[39m     \u001b[38;5;66;03m# We try to see if we have a cached version (not up to date):\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     resolved_file = \u001b[43mtry_to_load_from_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m resolved_file != _CACHED_NO_EXIST:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:106\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mrepo_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfrom_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mto_id\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m arg_name == \u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:160\u001b[39m, in \u001b[36mvalidate_repo_id\u001b[39m\u001b[34m(repo_id)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX.match(repo_id):\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    161\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must use alphanumeric chars or \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m--\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m\u001b[33m are\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    162\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m forbidden, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m cannot start or end the name, max length is 96:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    163\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m--\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m..\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
      "\u001b[31mHFValidationError\u001b[39m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './gpt-oss-20b'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mLanguageModel\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./gpt-oss-20b\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m tokenizer = model.tokenizer\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/nnsight/modeling/language.py:90\u001b[39m, in \u001b[36mLanguageModel.__init__\u001b[39m\u001b[34m(self, config, tokenizer, automodel, import_edits, *args, **kwargs)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mself\u001b[39m.repo_id: \u001b[38;5;28mstr\u001b[39m = args[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[32m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(args[\u001b[32m0\u001b[39m], \u001b[33m'\u001b[39m\u001b[33mname_or_path\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     88\u001b[39m \u001b[38;5;28mself\u001b[39m.revision: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28mgetattr\u001b[39m(args[\u001b[32m0\u001b[39m], \u001b[33m'\u001b[39m\u001b[33mrevision\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmain\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m import_edits:\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(import_edits, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/nnsight/modeling/mixins/meta.py:40\u001b[39m, in \u001b[36mMetaMixin.__init__\u001b[39m\u001b[34m(self, dispatch, meta_buffers, rename, *args, **kwargs)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m init_empty_weights(include_buffers=meta_buffers):\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m         model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m     NNsight.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, rename=rename)\n\u001b[32m     44\u001b[39m \u001b[38;5;28mself\u001b[39m.args = args\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/nnsight/modeling/language.py:215\u001b[39m, in \u001b[36mLanguageModel._load_meta\u001b[39m\u001b[34m(self, repo_id, revision, tokenizer_kwargs, patch_llama_scan, **kwargs)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28mself\u001b[39m.repo_id = repo_id\n\u001b[32m    213\u001b[39m \u001b[38;5;28mself\u001b[39m.revision = revision\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28mself\u001b[39m._load_tokenizer(repo_id, revision=revision, **tokenizer_kwargs)\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    220\u001b[39m     patch_llama_scan\n\u001b[32m    221\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.config, LlamaConfig)\n\u001b[32m    222\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.config.rope_scaling, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m    223\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mrope_type\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.rope_scaling\n\u001b[32m    224\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/nnsight/modeling/language.py:186\u001b[39m, in \u001b[36mLanguageModel._load_config\u001b[39m\u001b[34m(self, repo_id, **kwargs)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_load_config\u001b[39m(\u001b[38;5;28mself\u001b[39m, repo_id: \u001b[38;5;28mstr\u001b[39m, **kwargs):\n\u001b[32m    184\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m         \u001b[38;5;28mself\u001b[39m.config = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1288\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1285\u001b[39m trust_remote_code = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mtrust_remote_code\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1286\u001b[39m code_revision = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1288\u001b[39m config_dict, unused_kwargs = \u001b[43mPretrainedConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1289\u001b[39m has_remote_code = \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoConfig\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1290\u001b[39m has_local_code = \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/transformers/configuration_utils.py:662\u001b[39m, in \u001b[36mPretrainedConfig.get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    660\u001b[39m original_kwargs = copy.deepcopy(kwargs)\n\u001b[32m    661\u001b[39m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m config_dict, kwargs = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    664\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/transformers/configuration_utils.py:744\u001b[39m, in \u001b[36mPretrainedConfig._get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    741\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    742\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    743\u001b[39m         \u001b[38;5;66;03m# For any other exception, we throw a generic error.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m744\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    745\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt load the configuration of \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. If you were trying to load it\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    746\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m from \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/models\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, make sure you don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt have a local directory with the same\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    747\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m name. Otherwise, make sure \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is the correct path to a directory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    748\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m containing a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfiguration_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m file\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    749\u001b[39m         )\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gguf_file:\n",
      "\u001b[31mOSError\u001b[39m: Can't load the configuration of './gpt-oss-20b'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure './gpt-oss-20b' is the correct path to a directory containing a config.json file"
     ]
    }
   ],
   "source": [
    "# Determine the correct path to the model\n",
    "model_path = Path(\"./gpt-oss-20b\")\n",
    "if not model_path.exists():\n",
    "    model_path = Path(\"/root/Agent-Robustness-Via-ToM/gpt-oss-20b\")\n",
    "    \n",
    "print(f\"Loading model from: {model_path}\")\n",
    "model = LanguageModel(str(model_path), device_map=\"auto\", torch_dtype=t.bfloat16)\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "print()\n",
    "\n",
    "N_HEADS = model.config.num_attention_heads\n",
    "N_LAYERS = model.config.num_hidden_layers\n",
    "D_MODEL = model.config.hidden_size\n",
    "D_HEAD = model.config.head_dim\n",
    "\n",
    "print(f\"Number of heads: {N_HEADS}\")\n",
    "print(f\"Number of layers: {N_LAYERS}\")\n",
    "print(f\"Model (hidden) dimension: {D_MODEL}\")\n",
    "print(f\"Head dimension: {D_HEAD}\\n\")\n",
    "\n",
    "print(\"Entire config: \", model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd36d93",
   "metadata": {},
   "source": [
    "## Access Hidden States\n",
    "\n",
    "### GPT-OSS-20B Quick Reference\n",
    "\n",
    "- 24 layers | 2880 hidden | 64 heads | 201K vocab\n",
    "- **Key difference**: `model.model.layers[i]` not `model.transformer.h[i]`\n",
    "\n",
    "### Basic Usage\n",
    "```python\n",
    "with model.trace(prompt, remote=False):\n",
    "    # Hidden states from layer i\n",
    "    hidden = model.model.layers[i].output[0].save()\n",
    "    \n",
    "    # Logits (last token)\n",
    "    logits = model.lm_head.output[0, -1].save()\n",
    "```\n",
    "\n",
    "### Steering\n",
    "```python\n",
    "with model.trace(prompt, remote=False):\n",
    "    hidden = model.model.layers[i].output[0]\n",
    "    hidden[:, -1, :] += steering_vector\n",
    "    logits = model.lm_head.output[0, -1].save()\n",
    "```\n",
    "\n",
    "### Multi-Layer Extraction\n",
    "```python\n",
    "with model.trace(prompt, remote=False):\n",
    "    all_hidden = [model.model.layers[i].output[0].save() for i in range(24)]\n",
    "```\n",
    "\n",
    "## Layer Components\n",
    "```python\n",
    "model.model.layers[i].self_attn     # Attention block\n",
    "model.model.layers[i].mlp           # MoE block\n",
    "model.model.embed_tokens            # Embeddings\n",
    "model.model.norm                    # Final norm\n",
    "model.lm_head                       # Output layer\n",
    "```\n",
    "\n",
    "## Expected Shapes\n",
    "\n",
    "- Hidden: `(1, seq_len, 2880)`\n",
    "- Logits: `(201088,)` for single token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "358b5ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca39afb55eb45f8982dd4446b3fa701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape = torch.Size([201088]) = (vocab_size,)\n",
      "Predicted token ID = 12650\n",
      "Predicted token = ' Paris'\n",
      "\n",
      "resid.shape = torch.Size([8, 2880]) = (batch_size, seq_len, d_model)\n"
     ]
    }
   ],
   "source": [
    "# If you have an API key & want to work remotely, then set REMOTE = True and replace \"YOUR-API-KEY\"\n",
    "# with your actual key. If not, then leave REMOTE = False.\n",
    "REMOTE = False\n",
    "prompt = \"The Eiffel Tower is in the city of\"\n",
    "\n",
    "with model.trace(prompt, remote=REMOTE):\n",
    "    # Save the model's hidden states\n",
    "    hidden_states = model.model.layers[-1].output[0].save()\n",
    "\n",
    "    # Save the model's logit output\n",
    "    logits = model.lm_head.output[0, -1].save()\n",
    "\n",
    "# Get the model's logit output, and it's next token prediction\n",
    "print(f\"logits.shape = {logits.shape} = (vocab_size,)\")\n",
    "print(\"Predicted token ID =\", predicted_token_id := logits.argmax().item())\n",
    "print(f\"Predicted token = {tokenizer.decode(predicted_token_id)!r}\")\n",
    "\n",
    "# Print the shape of the model's residual stream\n",
    "print(f\"\\nresid.shape = {hidden_states.shape} = (batch_size, seq_len, d_model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72536a09",
   "metadata": {},
   "source": [
    "# Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2d63c0",
   "metadata": {},
   "source": [
    "## Load ToM Datasets\n",
    "\n",
    "Instead of using the ICL dataset, we'll load two Theory of Mind (ToM) datasets:\n",
    "1. **ToM Dataset** (`first_order_1_tom_prompts.jsonl`) - Questions that require theory of mind reasoning\n",
    "2. **No-ToM Dataset** (`first_order_1_no_tom_prompts.jsonl`) - Questions that don't require theory of mind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04dd3a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToM dataset: 10 samples\n",
      "No-ToM dataset: 10 samples\n",
      "ToM Dataset (requires ToM):\n",
      "  ToMDataset(path=first_order_1_tom_prompts.jsonl, size=10, requires_tom=True)\n",
      "  Example prompt:\n",
      "Story:\n",
      "Isabella entered the den.\n",
      "Olivia entered the den.\n",
      "Isabella dislikes the pumpkin\n",
      "The broccoli is in the blue_pantry.\n",
      "Isabella exited the den.\n",
      "Olivia moved the broccoli to the red_drawer.\n",
      "Abigail...\n",
      "  Example answer: blue_pantry\n",
      "\n",
      "No-ToM Dataset (doesn't require ToM):\n",
      "  ToMDataset(path=first_order_1_no_tom_prompts.jsonl, size=10, requires_tom=False)\n",
      "  Example prompt:\n",
      "Story:\n",
      "Aria entered the front_yard.\n",
      "Aiden entered the front_yard.\n",
      "The grapefruit is in the green_bucket.\n",
      "Aria moved the grapefruit to the blue_container.\n",
      "Aiden exited the front_yard.\n",
      "Noah entered the ...\n",
      "  Example answer: blue_container\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "class ToMDataset:\n",
    "    \"\"\"\n",
    "    Dataset for Theory of Mind (ToM) tasks from JSONL files.\n",
    "    \n",
    "    Inputs:\n",
    "        jsonl_path: Path to the JSONL file containing prompts\n",
    "        size: Optional limit on number of examples to load (None = load all)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, jsonl_path: str, size: int = None):\n",
    "        self.jsonl_path = Path(jsonl_path)\n",
    "        self.data = []\n",
    "        self.prompts = []\n",
    "        self.completions = []\n",
    "        self.answers = []\n",
    "        self.question_types = []\n",
    "        self.story_types = []\n",
    "        self.requires_tom = []\n",
    "        \n",
    "        # Load data from JSONL file\n",
    "        with open(self.jsonl_path, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if size is not None and i >= size:\n",
    "                    break\n",
    "                item = json.loads(line)\n",
    "                self.data.append(item)\n",
    "                self.prompts.append(item['prompt'])\n",
    "                # Completions are the answers with a leading space\n",
    "                self.completions.append(' ' + item['answer'])\n",
    "                self.answers.append(item['answer'])\n",
    "                self.question_types.append(item['question_type'])\n",
    "                self.story_types.append(item['story_type'])\n",
    "                self.requires_tom.append(item['requires_tom'])\n",
    "        \n",
    "        self.size = len(self.data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"ToMDataset(path={self.jsonl_path.name}, size={self.size}, requires_tom={self.requires_tom[0] if self.size > 0 else 'N/A'})\"\n",
    "\n",
    "\n",
    "# Load the two datasets\n",
    "tom_dataset = ToMDataset(\n",
    "    \"tom_benchmarks/tomi/tomi_pairs/first_order_1_tom_prompts.jsonl\",\n",
    "    size=10\n",
    ")\n",
    "no_tom_dataset = ToMDataset(\n",
    "    \"tom_benchmarks/tomi/tomi_pairs/first_order_1_no_tom_prompts.jsonl\",\n",
    "    size=10\n",
    ")\n",
    "\n",
    "print(f\"ToM dataset: {len(tom_dataset)} samples\")\n",
    "print(f\"No-ToM dataset: {len(no_tom_dataset)} samples\")\n",
    "\n",
    "print(\"ToM Dataset (requires ToM):\")\n",
    "print(f\"  {tom_dataset}\")\n",
    "print(f\"  Example prompt:\\n{tom_dataset.prompts[0][:200]}...\")\n",
    "print(f\"  Example answer: {tom_dataset.answers[0]}\")\n",
    "\n",
    "print(\"\\nNo-ToM Dataset (doesn't require ToM):\")\n",
    "print(f\"  {no_tom_dataset}\")\n",
    "print(f\"  Example prompt:\\n{no_tom_dataset.prompts[0][:200]}...\")\n",
    "print(f\"  Example answer: {no_tom_dataset.answers[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b49e31c",
   "metadata": {},
   "source": [
    "### Usage Notes\n",
    "\n",
    "The `ToMDataset` class has the same interface as `ICLDataset`, so it can be used interchangeably:\n",
    "- `.prompts` - list of prompt strings\n",
    "- `.completions` - list of completion strings (with leading space)\n",
    "- `.answers` - list of answer strings\n",
    "- `.size` - number of examples\n",
    "\n",
    "**Key difference from ICLDataset**: \n",
    "- ToMDataset does **not** have a `create_corrupted_dataset()` method\n",
    "- If you need corrupted data for contrastive analysis, you'll need to implement that separately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b543a562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXAMPLE FROM TOM DATASET (requires ToM reasoning):\n",
      "================================================================================\n",
      "Story:\n",
      "Isabella entered the den.\n",
      "Olivia entered the den.\n",
      "Isabella dislikes the pumpkin\n",
      "The broccoli is in the blue_pantry.\n",
      "Isabella exited the den.\n",
      "Olivia moved the broccoli to the red_drawer.\n",
      "Abigail entered the garden.\n",
      "Isabella entered the garden.\n",
      "\n",
      "Question: Where will Isabella look for the broccoli?\n",
      "Answer:\n",
      "Correct answer: blue_pantry\n",
      "Story type: false_belief\n",
      "Requires ToM: True\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE FROM NO-TOM DATASET (doesn't require ToM):\n",
      "================================================================================\n",
      "Story:\n",
      "Aria entered the front_yard.\n",
      "Aiden entered the front_yard.\n",
      "The grapefruit is in the green_bucket.\n",
      "Aria moved the grapefruit to the blue_container.\n",
      "Aiden exited the front_yard.\n",
      "Noah entered the playroom.\n",
      "\n",
      "Question: Where will Aiden look for the grapefruit?\n",
      "Answer:\n",
      "Correct answer: blue_container\n",
      "Story type: true_belief\n",
      "Requires ToM: False\n"
     ]
    }
   ],
   "source": [
    "# Example: View a complete example from each dataset\n",
    "print(\"=\" * 80)\n",
    "print(\"EXAMPLE FROM TOM DATASET (requires ToM reasoning):\")\n",
    "print(\"=\" * 80)\n",
    "print(tom_dataset[0]['prompt'])\n",
    "print(f\"Correct answer: {tom_dataset[0]['answer']}\")\n",
    "print(f\"Story type: {tom_dataset[0]['story_type']}\")\n",
    "print(f\"Requires ToM: {tom_dataset[0]['requires_tom']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE FROM NO-TOM DATASET (doesn't require ToM):\")\n",
    "print(\"=\" * 80)\n",
    "print(no_tom_dataset[0]['prompt'])\n",
    "print(f\"Correct answer: {no_tom_dataset[0]['answer']}\")\n",
    "print(f\"Story type: {no_tom_dataset[0]['story_type']}\")\n",
    "print(f\"Requires ToM: {no_tom_dataset[0]['requires_tom']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "048b77ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text only: We have a story with characters: Isabella, Olivia, Abigail. Items: broccoli, pumpkin, blue_pantry, red_drawer. The question: \"Where will Isabella look for the broccoli?\" We need to infer from the story. Let's parse the story:\n",
      "\n",
      "- Isabella entered the den.\n",
      "- Olivia entered the den.\n",
      "- Isabella dislikes the pumpkin.\n",
      "- The broccoli is in the blue_pantry.\n",
      "- Isabella exited the den.\n",
      "- Olivia moved the broccoli to the red_drawer.\n",
      "- Abigail entered the garden.\n",
      "- Isabella entered the garden.\n",
      "\n",
      "We need to answer: Where will Isabella look for the broccoli? The story says the broccoli was originally in the blue_pantry. Then Olivia moved the broccoli to the red_drawer. So the broccoli is now in the red_drawer. Isabella is in the garden. So where will she look? She will look in the red_drawer. But maybe she will look in the garden? But the broccoli is not in the garden. The question: \"Where will Isabella look for the broccoli?\" The answer: She will look in the red_drawer. But maybe she will look in the garden? Let's think: The story: Isabella is in the garden. She might look for the broccoli in the garden. But the broccoli is in the red_drawer. So she might look in the red_drawer. The question is ambiguous: \"Where will Isabella look for the broccoli?\" The answer: She will look in the red_drawer. But maybe she will look in the garden? But the broccoli is not in the garden. The question likely expects: She will look in the red_drawer. So answer: The red_drawer. So the answer: She will look in the red_drawer. So the answer: The red_drawer. So the answer: She will look in the red_drawer. So the answer: She will look in the red_drawer. So the answer: She will look in the red_drawer. So the answer: She will look in the red_drawer. So the answer: She will look in the red_drawer. So the answer: She will look in the red_drawer. So the answer: She will look in the red_drawer. So the answer: She will look in the red_drawer. So the answer: She will look in the red_drawer. So the answer: She will look in the red_drawer. So the answer: She will look in the red_draw\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Isabella entered the den.\n",
    "Olivia entered the den.\n",
    "Isabella dislikes the pumpkin\n",
    "The broccoli is in the blue_pantry.\n",
    "Isabella exited the den.\n",
    "Olivia moved the broccoli to the red_drawer.\n",
    "Abigail entered the garden.\n",
    "Isabella entered the garden.\n",
    "\n",
    "Question: Where will Isabella look for the broccoli?\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the prompt and move to GPU\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Use nnsight's generate method (pass tokenized inputs)\n",
    "output = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_new_tokens=500,\n",
    "    do_sample=False,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "\n",
    "# Or decode just the new tokens\n",
    "new_tokens = tokenizer.decode(output[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "print(\"\\nGenerated text only:\", new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cda02bc",
   "metadata": {},
   "source": [
    "# Find the relevant heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba1e4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fn_vectors_and_intervene(\n",
    "    model: LanguageModel,\n",
    "    dataset: ICLDataset,\n",
    "    layers: list[int] | None = None,\n",
    ") -> Float[Tensor, \"layers heads\"]:\n",
    "    \"\"\"\n",
    "    Returns a tensor of shape (layers, heads), containing the CIE for each head.\n",
    "\n",
    "    Inputs:\n",
    "        model: LanguageModel\n",
    "            the transformer you're doing this computation with\n",
    "        dataset: ICLDataset\n",
    "            the dataset of clean prompts from which we'll extract the function vector (we'll also\n",
    "            create a corrupted version of this dataset for interventions)\n",
    "        layers: list[int] | None\n",
    "            the layers which this function will calculate score for (if None, this means all layers)\n",
    "    \"\"\"\n",
    "    layers = range(model.config.n_layer) if (layers is None) else layers\n",
    "    heads = range(model.config.n_head)\n",
    "\n",
    "    # Get corrupted dataset\n",
    "    corrupted_dataset = dataset.create_corrupted_dataset()\n",
    "    N = len(dataset)\n",
    "\n",
    "    # Get correct token ids, so we can get correct token logprobs\n",
    "    correct_completion_ids = [toks[0] for toks in tokenizer(dataset.completions)[\"input_ids\"]]\n",
    "\n",
    "    with model.trace(remote=REMOTE) as tracer:\n",
    "        # Run a forward pass on clean prompts, where we store attention head outputs\n",
    "        z_dict = {}\n",
    "        with tracer.invoke(dataset.prompts):\n",
    "            for layer in layers:\n",
    "                # Get hidden states, reshape to get head dimension, store the mean tensor\n",
    "                z = model.transformer.h[layer].attn.out_proj.input[:, -1]\n",
    "                z_reshaped = z.reshape(N, N_HEADS, D_HEAD).mean(dim=0)\n",
    "                for head in heads:\n",
    "                    z_dict[(layer, head)] = z_reshaped[head]\n",
    "\n",
    "        # Run a forward pass on corrupted prompts, where we don't intervene or store activations (just so we can get the\n",
    "        # correct-token logprobs to compare with our intervention)\n",
    "        with tracer.invoke(corrupted_dataset.prompts):\n",
    "            logits = model.lm_head.output[:, -1]\n",
    "            correct_logprobs_corrupted = logits.log_softmax(dim=-1)[\n",
    "                t.arange(N), correct_completion_ids\n",
    "            ].save()\n",
    "\n",
    "        # For each head, run a forward pass on corrupted prompts (here we need multiple different forward passes, since\n",
    "        # we're doing different interventions each time)\n",
    "        correct_logprobs_dict = {}\n",
    "        for layer in layers:\n",
    "            for head in heads:\n",
    "                with tracer.invoke(corrupted_dataset.prompts):\n",
    "                    # Get hidden states, reshape to get head dimension, then set it to the a-vector\n",
    "                    z = model.transformer.h[layer].attn.out_proj.input[:, -1]\n",
    "                    z.reshape(N, N_HEADS, D_HEAD)[:, head] = z_dict[(layer, head)]\n",
    "                    # Get logprobs at the end, which we'll compare with our corrupted logprobs\n",
    "                    logits = model.lm_head.output[:, -1]\n",
    "                    correct_logprobs_dict[(layer, head)] = logits.log_softmax(dim=-1)[\n",
    "                        t.arange(N), correct_completion_ids\n",
    "                    ].save()\n",
    "\n",
    "    # Get difference between intervention logprobs and corrupted logprobs, and take mean over batch dim\n",
    "    all_correct_logprobs_intervention = einops.rearrange(\n",
    "        t.stack([v for v in correct_logprobs_dict.values()]),\n",
    "        \"(layers heads) batch -> layers heads batch\",\n",
    "        layers=len(layers),\n",
    "    )\n",
    "    logprobs_diff = (\n",
    "        all_correct_logprobs_intervention - correct_logprobs_corrupted\n",
    "    )  # shape [layers heads batch]\n",
    "\n",
    "    # Return mean effect of intervention, over the batch dimension\n",
    "    return logprobs_diff.mean(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d433604e",
   "metadata": {},
   "source": [
    "## Calculate Function Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6822b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fn_vector(\n",
    "    model: LanguageModel,\n",
    "    dataset: ToMDataset,\n",
    "    head_list: list[tuple[int, int]],\n",
    ") -> Float[Tensor, \"D_MODEL\"]:\n",
    "    \"\"\"\n",
    "    Returns a vector of length `D_MODEL`, containing the sum of vectors written to the residual\n",
    "    stream by the attention heads in `head_list`, averaged over all inputs in `dataset`.\n",
    "\n",
    "    Inputs:\n",
    "        model: LanguageModel\n",
    "            the transformer you're doing this computation with\n",
    "        dataset: ToMDataset\n",
    "            the dataset of prompts from which we'll extract the function vector\n",
    "        head_list: list[tuple[int, int]]\n",
    "            list of attention heads we're calculating the function vector from\n",
    "    \"\"\"\n",
    "    # Turn head_list into a dict of {layer: heads we need in this layer}\n",
    "    head_dict = defaultdict(set)\n",
    "    for layer, head in head_list:\n",
    "        head_dict[layer].add(head)\n",
    "\n",
    "    fn_vector_list = []\n",
    "\n",
    "    with model.trace(dataset.prompts, remote=REMOTE):\n",
    "        for layer, heads_in_layer in head_dict.items():\n",
    "            # Get the output projection layer (GPT-OSS uses o_proj, not out_proj)\n",
    "            o_proj = model.model.layers[layer].self_attn.o_proj\n",
    "\n",
    "            # Get the mean output projection input (note, setting values of this tensor will not\n",
    "            # have downstream effects on other tensors)\n",
    "            hidden_states = o_proj.input[:, -1].mean(dim=0)\n",
    "\n",
    "            # Zero-ablate all heads which aren't in our list, then get the output (which\n",
    "            # will be the sum over the heads we actually do want!)\n",
    "            heads_to_ablate = set(range(N_HEADS)) - head_dict[layer]\n",
    "            for head in heads_to_ablate:\n",
    "                hidden_states.reshape(N_HEADS, D_HEAD)[head] = 0.0\n",
    "\n",
    "            # Now that we've zeroed all unimportant heads, get the output & add it to the list\n",
    "            # (we need a single batch dimension so we can use `o_proj`)\n",
    "            o_proj_output = o_proj(hidden_states.unsqueeze(0)).squeeze()\n",
    "            fn_vector_list.append(o_proj_output.save())\n",
    "\n",
    "    # We sum all attention head outputs to get our function vector\n",
    "    fn_vector = sum([v for v in fn_vector_list])\n",
    "\n",
    "    assert fn_vector.shape == (D_MODEL,)\n",
    "    return fn_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "862f827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intervene_with_fn_vector(\n",
    "    model: LanguageModel,\n",
    "    prompt: str,\n",
    "    layer: int,\n",
    "    fn_vector: Float[Tensor, \"D_MODEL\"],\n",
    "    n_tokens: int = 50,\n",
    ") -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Intervenes with a function vector, by adding it at the last sequence position of a generated\n",
    "    prompt.\n",
    "\n",
    "    Inputs:\n",
    "        model: LanguageModel\n",
    "            the transformer you're doing this computation with\n",
    "        prompt: str\n",
    "            The prompt to generate from (no longer template-based, just pass full prompt)\n",
    "        layer: int\n",
    "            The layer we'll make the intervention (by adding the function vector)\n",
    "        fn_vector: Float[Tensor, \"D_MODEL\"]\n",
    "            The vector we'll add to the final sequence position for each new token to be generated\n",
    "        n_tokens: int\n",
    "            The number of additional tokens we'll generate for our unsteered / steered completions\n",
    "\n",
    "    Returns:\n",
    "        completion: str\n",
    "            The full completion (including original prompt) for the no-intervention case\n",
    "        completion_intervention: str\n",
    "            The full completion (including original prompt) for the intervention case\n",
    "    \"\"\"\n",
    "    with model.generate(\n",
    "        remote=REMOTE, max_new_tokens=n_tokens, pad_token_id=tokenizer.pad_token_id\n",
    "    ) as generator:\n",
    "        with model.all():\n",
    "            with generator.invoke(prompt):\n",
    "                tokens = model.generator.output.save()\n",
    "\n",
    "            with generator.invoke(prompt):\n",
    "                # Fixed: Need [:, -1, :] not [:, -1] to match dimensions (batch, seq, hidden)\n",
    "                model.model.layers[layer].output[0][:, -1, :] += fn_vector\n",
    "                tokens_intervention = model.generator.output.save()\n",
    "\n",
    "    completion, completion_intervention = tokenizer.batch_decode(\n",
    "        [tokens.squeeze().tolist(), tokens_intervention.squeeze().tolist()]\n",
    "    )\n",
    "    return completion, completion_intervention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b02585",
   "metadata": {},
   "source": [
    "## Test the intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0ce68e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO INTERVENTION: tell me a short story. Don't feel much..\"\n",
      "\n",
      "They can produce short story with no emotion.\n",
      "\n",
      "We don't have to produce too many paragraphs if needed.\n",
      "\n",
      "We can maybe produce something like:\n",
      "\n",
      "\"At the edge of town, a man named Lian had a bench. The bench had seen decades pass, and the leaves fell upon it. It had a scar. The scar was an old hole where a child had once dropped a coin.\"\n",
      "\n",
      "But no mention of emotion.\n",
      "\n",
      "OK.\n",
      "\n",
      "I will produce simple story in a few sentences, following constraints.\n",
      "\n",
      "We must keep in mind the formatting of the last paragraph: \"The last line has to be something that the main character says about the future or hope for a better world. The last line should not say that the entire story happened. It should feel hopeful or inspiring. Only 'we' could reflect something like 'we will do...\". So final line is a quote: \"We will...\" no \"past\".\n",
      "\n",
      "Given all, I'll write something. And the story should keep all constraints. Let's go.\n",
      "\n",
      "I have to double-check each line for negative words. Let's do that:\n",
      "\n",
      "We must avoid words that can be negative. Negative words list: \"bad\", \"hate\", \"sad\", \"angry\", \"fear\", \"fears\", \"hate\", \"hated\", \"hunger\", \"suffering\", \"pain\", etc. Also \"bad\" etc. Avoid \"fear\" as well. Also \"can't\" as mention \"cannot\"? \"cannot\" maybe considered negative? Might. Let's avoid.\n",
      "\n",
      "Thus avoid \"cannot\", \"can't\", etc. Avoid \"fear\", \"fears\". Avoid \"unfortunate\", \"unlucky\". Avoid \"lost\" synonyms. Avoid \"tragic\". Avoid \"grim\". Avoid \"dark\". Avoid \"dead\". Avoid \"death\". Avoid \"murder\". Avoid \"violence\". Avoid \"kill\". Avoid \"attack\" etc. Avoid \"attack\" with \"attack\".\n",
      "\n",
      "Ok.\n",
      "\n",
      "We shall keep it simple.\n",
      "\n",
      "Now write the short story, maybe 6-7 lines, each line with first letter uppercase, not negative, no \"h\". Let's do.\n",
      "\n",
      "Maybe 5 lines: 1. something, 2. something, 3. something, 4. something, 5. Something with \"We will...\".\n",
      "\n",
      "But must last line not refer to past, but \"We will...\".\n",
      "\n",
      "Ok.\n",
      "\n",
      "Let's produce:\n",
      "\n",
      "\"At the edge of town, a young woman named Lila found a quiet path that\n",
      "INTERVENTION: tell me a short story.\n",
      "\n",
      "Once upon a time, in a far-off kingdom, there lived a brave young knight named Sir William. Sir William was known throughout the land for his skill in battle and his unwavering dedication to protecting the people from harm.\n",
      "\n",
      "One day, a great dragon appeared, and the kingdom was saved by the knight's courage, his swift strategies, and good wit. After the dragon was vanquished, the people celebrated and threw a grand parade in Sir William's honor.\n",
      "\n",
      "But the knight, after the parade and celebration, was unhappy. He felt a calling toward discovering and the desire to gain more knowledge by traveling and learning of different cultures. When his lord suggested a new quest to conquer one of neighboring lands with a new kingdom, Sir William told him that he was not interested anymore.\n",
      "\n",
      "Sir William had another hope for his future: to learn more. He decided to leave the kingdom and began a new chapter in the adventure of his life where he would travel and observe. Some traveled with a caravan while other traveled with a small fleet of vessels. He traveled the world to observe the people's lives and how they live. To discover new knowledge, which was an endless pursuit to him.\n",
      "\n",
      "Many, many years, time was passing. Sir William saw the rise and fall of empires, witnessed the growth of kingdoms, and watched the rise of new leaders all around the world. Even though he saw a lot of chaos in history and events, he discovered how some changes were inevitable.\n",
      "\n",
      "Through all of his observations, he could not understand. However, there were many examples which he shared in the kingdom and was so widely admired with a lot of people. Sir William grew wiser and even had to sacrifice personal pride. He could, however, become content and satisfied with this life. Even though Sir William was happy he continued his life with an open heart and a quest for learning.\n",
      "\n",
      "In the end of their lives, the world learned from the wise Sir William that knowledge can change lives.\n",
      "\n",
      "It seems like you're providing a summary or retelling of a story. This appears to be a narrative about a knight named Sir William, who leaves his kingdom and travels to learn about different cultures and the workings of society, gaining wisdom and contributing to the realm. The summary is a narrative with moral: \"knowledge can change\n",
      "\n",
      "It seems like you're reflecting on the structure and moral of a narrative. If you have any specific aspects of the story you'd like to explore or discuss — perhaps the themes of knowledge and wisdom, or\n"
     ]
    }
   ],
   "source": [
    "# Instead of removing a word, we'll use the ToM datasets directly\n",
    "# The goal is to extract a \"ToM reasoning\" function vector\n",
    "\n",
    "# Define our datasets\n",
    "# We'll use the no_tom_dataset to extract what the model does when NOT using ToM reasoning\n",
    "# (This is analogous to using the clean antonym pairs in the original)\n",
    "dataset = no_tom_dataset  # or tom_dataset, depending on what vector you want to extract\n",
    "\n",
    "# Define the attention heads you'll use\n",
    "# NOTE: You'll need to identify which heads are important for ToM reasoning\n",
    "# This was done in the original via analysis - you may need to run similar analysis\n",
    "head_list = [\n",
    "    (8, 0),\n",
    "    (8, 1),\n",
    "    (9, 14),\n",
    "    (11, 0),\n",
    "    (12, 10),\n",
    "    (13, 12),\n",
    "    (13, 13),\n",
    "    (14, 9),\n",
    "    (15, 5),\n",
    "    (16, 14),\n",
    "]\n",
    "\n",
    "# Extract the function vector\n",
    "fn_vector = calculate_fn_vector(model, dataset, head_list)\n",
    "\n",
    "\n",
    "\n",
    "# test_prompt = tom_dataset.prompts[0]  # or select a specific test case\n",
    "test_prompt = \"tell me a short story.\"\n",
    "\n",
    "# Simple generation with steering at a specific layer\n",
    "with model.generate(remote=REMOTE, max_new_tokens=500, pad_token_id=tokenizer.pad_token_id) as generator:\n",
    "    # No intervention\n",
    "    with generator.invoke(test_prompt):\n",
    "        tokens_no_intervention = model.generator.output.save()\n",
    "    \n",
    "    # With intervention (adding the function vector)\n",
    "    with generator.invoke(test_prompt):\n",
    "        # During generation, layer output at position [0] is 2D: (batch, hidden_dim)\n",
    "        # because it processes one token at a time\n",
    "        # So we just add directly without the sequence dimension\n",
    "        model.model.layers[9].output[0] += 1.5 * fn_vector\n",
    "        tokens_with_intervention = model.generator.output.save()\n",
    "\n",
    "completion_no_intervention = tokenizer.decode(tokens_no_intervention.squeeze().tolist())\n",
    "completion_with_intervention = tokenizer.decode(tokens_with_intervention.squeeze().tolist())\n",
    "\n",
    "print(\"NO INTERVENTION:\", completion_no_intervention)\n",
    "print(\"INTERVENTION:\", completion_with_intervention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58610e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f2bf8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
