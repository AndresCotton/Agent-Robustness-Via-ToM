{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToMi Function Vector Extraction\n",
    "\n",
    "This notebook extracts function vectors for Theory of Mind (ToM) reasoning using the ToMi dataset and NNSight.\n",
    "\n",
    "**Approach:**\n",
    "1. Load ToM-required vs no-ToM-required examples (same question format, different reasoning demands)\n",
    "2. Run both through a model, extracting activations at a target layer\n",
    "3. Compute: `function_vector = mean(tom_activations) - mean(no_tom_activations)`\n",
    "4. Validate by steering model behavior with the function vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from nnsight import LanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths - adjust if needed\n",
    "DATA_DIR = Path('tomi/tomi_pairs')\n",
    "TOM_FILE = DATA_DIR / 'first_order_1_tom.jsonl'\n",
    "NO_TOM_FILE = DATA_DIR / 'first_order_1_no_tom.jsonl'\n",
    "\n",
    "# Verify files exist\n",
    "assert TOM_FILE.exists(), f\"Missing: {TOM_FILE}\"\n",
    "assert NO_TOM_FILE.exists(), f\"Missing: {NO_TOM_FILE}\"\n",
    "print(f\"ToM file: {TOM_FILE}\")\n",
    "print(f\"No-ToM file: {NO_TOM_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are answering questions about a story. Answer with ONLY the location name in <answer> tags. Example: <answer>blue_pantry</answer>\"\"\"\n",
    "\n",
    "def clean_story(story: str) -> str:\n",
    "    \"\"\"Remove line numbers from story.\"\"\"\n",
    "    lines = story.split('\\n')\n",
    "    return '\\n'.join(\n",
    "        line.split(' ', 1)[1] if line[0].isdigit() else line \n",
    "        for line in lines if line.strip()\n",
    "    )\n",
    "\n",
    "def make_prompt(example: dict) -> str:\n",
    "    \"\"\"Format single example as user prompt.\"\"\"\n",
    "    story = clean_story(example['story'])\n",
    "    return f\"\"\"Story:\n",
    "{story}\n",
    "\n",
    "Question: {example['question']}\n",
    "Answer:\"\"\"\n",
    "\n",
    "def load_examples(jsonl_path: str, limit: int = None) -> list:\n",
    "    \"\"\"Load examples and add formatted prompts.\"\"\"\n",
    "    examples = []\n",
    "    with open(jsonl_path) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if limit and i >= limit:\n",
    "                break\n",
    "            ex = json.loads(line)\n",
    "            ex['prompt'] = make_prompt(ex)\n",
    "            examples.append(ex)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(response: str) -> str:\n",
    "    \"\"\"Extract answer from <answer> tags.\"\"\"\n",
    "    match = re.search(r'<answer>\\s*([^<]+)\\s*</answer>', response, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).strip().lower()\n",
    "    # Fallback: look for location pattern (word_word)\n",
    "    match = re.search(r'\\b(\\w+_\\w+)\\b', response)\n",
    "    return match.group(1).lower() if match else response.strip().lower()\n",
    "\n",
    "def score(response: str, correct: str) -> bool:\n",
    "    \"\"\"Check if extracted answer matches correct answer.\"\"\"\n",
    "    extracted = extract_answer(response)\n",
    "    return extracted == correct.lower()\n",
    "\n",
    "def score_batch(responses: list, examples: list) -> dict:\n",
    "    \"\"\"Score a batch of responses.\"\"\"\n",
    "    correct = sum(score(r, ex['answer']) for r, ex in zip(responses, examples))\n",
    "    return {\n",
    "        'accuracy': correct / len(examples),\n",
    "        'correct': correct,\n",
    "        'total': len(examples),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load examples (use limit for faster iteration during development)\n",
    "N_EXAMPLES = 100  # Set to None for all examples\n",
    "\n",
    "tom_examples = load_examples(TOM_FILE, limit=N_EXAMPLES)\n",
    "no_tom_examples = load_examples(NO_TOM_FILE, limit=N_EXAMPLES)\n",
    "\n",
    "print(f\"Loaded {len(tom_examples)} ToM examples\")\n",
    "print(f\"Loaded {len(no_tom_examples)} No-ToM examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a sample\n",
    "print(\"=== ToM Example ===\")\n",
    "print(tom_examples[0]['prompt'])\n",
    "print(f\"\\nCorrect answer: {tom_examples[0]['answer']}\")\n",
    "print(f\"Story type: {tom_examples[0]['story_type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== No-ToM Example ===\")\n",
    "print(no_tom_examples[0]['prompt'])\n",
    "print(f\"\\nCorrect answer: {no_tom_examples[0]['answer']}\")\n",
    "print(f\"Story type: {no_tom_examples[0]['story_type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model - smaller models for faster iteration\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"  # or \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "model = LanguageModel(MODEL_NAME, device_map=\"auto\")\n",
    "print(f\"Loaded {MODEL_NAME}\")\n",
    "print(f\"Number of layers: {len(model.model.layers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target layer for activation extraction (middle-ish layers often work well)\n",
    "TARGET_LAYER = len(model.model.layers) // 2\n",
    "print(f\"Target layer: {TARGET_LAYER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper: Format with Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_prompt(user_prompt: str, system_prompt: str = SYSTEM_PROMPT) -> str:\n",
    "    \"\"\"Format prompt using model's chat template.\"\"\"\n",
    "    if hasattr(model.tokenizer, 'apply_chat_template'):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        return model.tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "    else:\n",
    "        return f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "\n",
    "# Test it\n",
    "test_formatted = format_chat_prompt(tom_examples[0]['prompt'])\n",
    "print(test_formatted[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Activations\n",
    "\n",
    "Run examples through the model and save activations at the target layer (last token position)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_activations(examples: list, target_layer: int = TARGET_LAYER):\n",
    "    \"\"\"Extract activations and generate responses for a list of examples.\"\"\"\n",
    "    responses = []\n",
    "    activations = []\n",
    "    \n",
    "    for ex in tqdm(examples, desc=\"Extracting\"):\n",
    "        full_prompt = format_chat_prompt(ex['prompt'])\n",
    "        \n",
    "        with model.generate(full_prompt, max_new_tokens=20) as gen:\n",
    "            # Save activation at last token position before generation\n",
    "            act = model.model.layers[target_layer].output[0][:, -1, :].save()\n",
    "        \n",
    "        # Decode response\n",
    "        response_tokens = gen.output[0][len(model.tokenizer.encode(full_prompt)):]\n",
    "        response_text = model.tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
    "        \n",
    "        responses.append(response_text)\n",
    "        activations.append(act.value.detach().cpu())\n",
    "    \n",
    "    return responses, activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract from ToM examples\n",
    "print(\"Extracting ToM activations...\")\n",
    "tom_responses, tom_activations = extract_activations(tom_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract from No-ToM examples\n",
    "print(\"Extracting No-ToM activations...\")\n",
    "no_tom_responses, no_tom_activations = extract_activations(no_tom_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tom_scores = score_batch(tom_responses, tom_examples)\n",
    "no_tom_scores = score_batch(no_tom_responses, no_tom_examples)\n",
    "\n",
    "print(f\"ToM accuracy:    {tom_scores['accuracy']:.1%} ({tom_scores['correct']}/{tom_scores['total']})\")\n",
    "print(f\"No-ToM accuracy: {no_tom_scores['accuracy']:.1%} ({no_tom_scores['correct']}/{no_tom_scores['total']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect some responses\n",
    "print(\"=== Sample ToM Responses ===\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nQ: {tom_examples[i]['question']}\")\n",
    "    print(f\"Model: {tom_responses[i]}\")\n",
    "    print(f\"Correct: {tom_examples[i]['answer']}\")\n",
    "    print(f\"Score: {score(tom_responses[i], tom_examples[i]['answer'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Function Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack activations\n",
    "tom_acts_tensor = torch.stack(tom_activations).squeeze(1)  # [N, hidden_dim]\n",
    "no_tom_acts_tensor = torch.stack(no_tom_activations).squeeze(1)\n",
    "\n",
    "print(f\"ToM activations shape: {tom_acts_tensor.shape}\")\n",
    "print(f\"No-ToM activations shape: {no_tom_acts_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute function vector\n",
    "tom_mean = tom_acts_tensor.mean(dim=0)\n",
    "no_tom_mean = no_tom_acts_tensor.mean(dim=0)\n",
    "\n",
    "function_vector = tom_mean - no_tom_mean\n",
    "\n",
    "print(f\"Function vector shape: {function_vector.shape}\")\n",
    "print(f\"Function vector norm: {function_vector.norm():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for later use\n",
    "torch.save({\n",
    "    'function_vector': function_vector,\n",
    "    'tom_mean': tom_mean,\n",
    "    'no_tom_mean': no_tom_mean,\n",
    "    'target_layer': TARGET_LAYER,\n",
    "    'model_name': MODEL_NAME,\n",
    "    'n_tom_examples': len(tom_examples),\n",
    "    'n_no_tom_examples': len(no_tom_examples),\n",
    "}, 'tom_function_vector.pt')\n",
    "\n",
    "print(\"Saved to tom_function_vector.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steering Experiment\n",
    "\n",
    "Test if adding the function vector improves ToM performance on held-out examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_steering(prompt: str, steering_vector: torch.Tensor, \n",
    "                           layer: int, scale: float = 1.0):\n",
    "    \"\"\"Generate with function vector added at target layer.\"\"\"\n",
    "    full_prompt = format_chat_prompt(prompt)\n",
    "    steering_vector = steering_vector.to(model.device)\n",
    "    \n",
    "    with model.generate(full_prompt, max_new_tokens=20) as gen:\n",
    "        # Add steering vector to layer output\n",
    "        model.model.layers[layer].output[0][:, -1, :] += scale * steering_vector\n",
    "    \n",
    "    response_tokens = gen.output[0][len(model.tokenizer.encode(full_prompt)):]\n",
    "    return model.tokenizer.decode(response_tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a few examples\n",
    "SCALE = 1.0  # Adjust this to control steering strength\n",
    "\n",
    "print(\"=== Steering Comparison ===\")\n",
    "for i in range(5):\n",
    "    ex = tom_examples[i]\n",
    "    \n",
    "    # Without steering (use cached response)\n",
    "    baseline = tom_responses[i]\n",
    "    \n",
    "    # With steering\n",
    "    steered = generate_with_steering(ex['prompt'], function_vector, TARGET_LAYER, scale=SCALE)\n",
    "    \n",
    "    print(f\"\\nQ: {ex['question']}\")\n",
    "    print(f\"Correct: {ex['answer']}\")\n",
    "    print(f\"Baseline: {baseline} ({'✓' if score(baseline, ex['answer']) else '✗'})\")\n",
    "    print(f\"Steered:  {steered} ({'✓' if score(steered, ex['answer']) else '✗'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Tune hyperparameters**: Try different layers, steering scales\n",
    "2. **Cross-validation**: Use train split for extraction, test for validation\n",
    "3. **Generalization**: Test function vector on FANToM and SimpleToM\n",
    "4. **Analysis**: Compare first-order vs second-order vectors (cosine similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
