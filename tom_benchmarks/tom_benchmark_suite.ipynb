{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory of Mind Benchmark Suite\n",
    "\n",
    "This notebook evaluates model performance across **four ToM benchmarks** to establish baselines for function vector research.\n",
    "\n",
    "## Benchmarks\n",
    "\n",
    "| Benchmark | Source | Format | Focus |\n",
    "|-----------|--------|--------|-------|\n",
    "| **ToMi** | Facebook Research | Short stories + questions | First/second-order false belief |\n",
    "| **FANToM** | Allen AI | Multi-party conversations | Information asymmetry in dialogue |\n",
    "| **SimpleToM** | Allen AI / HuggingFace | 2-sentence narratives | Explicit + applied ToM |\n",
    "| **ToMBench** | Tsinghua/Chen et al. | Diverse social scenarios | 8 ToM tasks, 31 ATOMS abilities |\n",
    "\n",
    "## Models Evaluated\n",
    "- **Target model**: gpt-oss-20b (local)\n",
    "- **Positive control**: Claude (via OpenRouter API)\n",
    "\n",
    "## Contents\n",
    "1. [Setup](#1-setup)\n",
    "2. [Model Backends](#2-model-backends)\n",
    "3. [Evaluation Functions](#3-evaluation-functions)\n",
    "4. [ToMi Evaluation](#4-tomi)\n",
    "5. [FANToM Evaluation](#5-fantom)\n",
    "6. [SimpleToM Evaluation](#6-simpletom)\n",
    "7. [ToMBench Evaluation](#7-tombench)\n",
    "8. [Cross-Benchmark Comparison](#8-comparison)\n",
    "9. [Save Results](#9-save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup <a name=\"1-setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Environment configured\n"
     ]
    }
   ],
   "source": [
    "# Environment configuration\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "print(\"âœ“ Environment configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import gc\n",
    "import subprocess\n",
    "from abc import ABC, abstractmethod\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from rich.table import Table\n",
    "from rich.console import Console\n",
    "from rich import box\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\n",
    "    \"mps\" if torch.backends.mps.is_available() \n",
    "    else \"cuda\" if torch.cuda.is_available() \n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "console = Console()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory available: 47.53 GB total\n",
      "GPU Memory allocated: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# if MODELS_TO_EVALUATE:\n",
    "#     del MODELS_TO_EVALUATE['local']\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# del all_tomi_results, all_fantom_results, all_simpletom_results, all_tombench_results\n",
    "# gc.collect()\n",
    "\n",
    "# del tom_dataset, no_tom_dataset, fantom_dataset, tombench_dataset\n",
    "# gc.collect()\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU Memory available: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB total\")\n",
    "    print(f\"GPU Memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ openai package available\n"
     ]
    }
   ],
   "source": [
    "# Install openai for OpenRouter API\n",
    "try:\n",
    "    import openai\n",
    "    print(\"âœ“ openai package available\")\n",
    "except ImportError:\n",
    "    print(\"Installing openai package...\")\n",
    "    !pip install openai -q\n",
    "    import openai\n",
    "    print(\"âœ“ openai package installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Model Backends <a name=\"2-model-backends\"></a>\n",
    "\n",
    "Abstracted model interface supporting both local HuggingFace models and API-based models (via OpenRouter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model backend classes defined\n"
     ]
    }
   ],
   "source": [
    "class ModelBackend(ABC):\n",
    "    \"\"\"Abstract base class for model backends.\"\"\"\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def name(self) -> str:\n",
    "        \"\"\"Model identifier for logging.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def generate(self, prompt: str, max_tokens: int = 500) -> Tuple[str, float, int, int]:\n",
    "        \"\"\"\n",
    "        Generate response from model.\n",
    "        \n",
    "        Returns: (response_text, elapsed_time, input_tokens, output_tokens)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class LocalModelBackend(ModelBackend):\n",
    "    \"\"\"Backend for local HuggingFace models.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_id: str, device: torch.device):\n",
    "        self.model_id = model_id\n",
    "        self.device = device\n",
    "        \n",
    "        print(f\"Loading model: {model_id}\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=\"auto\",\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            local_files_only=True,\n",
    "            low_cpu_mem_usage=True,\n",
    "        )\n",
    "        \n",
    "        print(\"Loading tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id, local_files_only=True)\n",
    "        print(f\"âœ“ Model loaded!\")\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return self.model_id.split('/')[-1]\n",
    "    \n",
    "    def format_prompt(self, user_message: str) -> str:\n",
    "        \"\"\"Apply the chat template to format prompts properly for the model.\"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        return self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "    \n",
    "    def generate(self, prompt: str, max_tokens: int = 500) -> Tuple[str, float, int, int]:\n",
    "        # Apply chat template to the prompt\n",
    "        formatted_prompt = self.format_prompt(prompt)\n",
    "        inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.device)\n",
    "        input_len = inputs[\"input_ids\"].shape[1]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            output = self.model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_new_tokens=max_tokens,\n",
    "                do_sample=False,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        response = self.tokenizer.decode(output[0][input_len:], skip_special_tokens=True)\n",
    "        output_len = output.shape[1] - input_len\n",
    "        \n",
    "        return response, elapsed, input_len, output_len\n",
    "\n",
    "\n",
    "class OpenRouterBackend(ModelBackend):\n",
    "    \"\"\"Backend for OpenRouter API (Claude and other models).\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"anthropic/claude-sonnet-4\", api_key: str = None):\n",
    "        self.model = model\n",
    "        self.client = openai.OpenAI(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            api_key=api_key or os.environ.get(\"OPENROUTER_API_KEY\"),\n",
    "        )\n",
    "        print(f\"âœ“ OpenRouter backend initialized: {model}\")\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return self.model.split(\"/\")[-1]\n",
    "    \n",
    "    def generate(self, prompt: str, max_tokens: int = 500) -> Tuple[str, float, int, int]:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                max_tokens=max_tokens,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            text = response.choices[0].message.content if response.choices else \"\"\n",
    "            input_tokens = response.usage.prompt_tokens if response.usage else 0\n",
    "            output_tokens = response.usage.completion_tokens if response.usage else 0\n",
    "            \n",
    "            return text, elapsed, input_tokens, output_tokens\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"API Error: {e}\")\n",
    "            return f\"ERROR: {e}\", time.time() - start_time, 0, 0\n",
    "\n",
    "\n",
    "print(\"âœ“ Model backend classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local model...\n",
      "Loading model: ../gpt-oss-20b/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eff1e39ff724bb5940e32823d5a7e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "âœ“ Model loaded!\n",
      "âœ“ OpenRouter backend initialized: anthropic/claude-sonnet-4\n",
      "\n",
      "âœ“ Models to evaluate: ['local', 'claude']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION: Choose which models to evaluate\n",
    "# ============================================================\n",
    "\n",
    "# Set your OpenRouter API key here or via environment variable\n",
    "os.environ[\"OPENROUTER_API_KEY\"] = \"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\"\n",
    "\n",
    "MODELS_TO_EVALUATE = {}\n",
    "\n",
    "# Local model (comment out if not available)\n",
    "LOCAL_MODEL_ID = \"../gpt-oss-20b/\"\n",
    "LOAD_LOCAL_MODEL = Path(LOCAL_MODEL_ID).exists()\n",
    "\n",
    "if LOAD_LOCAL_MODEL:\n",
    "    print(\"Loading local model...\")\n",
    "    MODELS_TO_EVALUATE['local'] = LocalModelBackend(LOCAL_MODEL_ID, device)\n",
    "else:\n",
    "    print(f\"âš  Local model not found at {LOCAL_MODEL_ID}\")\n",
    "\n",
    "# Claude via OpenRouter as positive control\n",
    "LOAD_CLAUDE = True  # Set to False to skip Claude evaluation\n",
    "\n",
    "if LOAD_CLAUDE:\n",
    "    try:\n",
    "        # Available Claude models on OpenRouter:\n",
    "        # - anthropic/claude-sonnet-4 (Claude Sonnet 4)\n",
    "        # - anthropic/claude-opus-4 (Claude Opus 4)\n",
    "        # - anthropic/claude-3.5-sonnet (Claude 3.5 Sonnet)\n",
    "        # - anthropic/claude-3-opus (Claude 3 Opus)\n",
    "        MODELS_TO_EVALUATE['claude'] = OpenRouterBackend(model=\"anthropic/claude-sonnet-4\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Could not initialize OpenRouter backend: {e}\")\n",
    "        print(\"  Set OPENROUTER_API_KEY environment variable or pass api_key parameter\")\n",
    "\n",
    "print(f\"\\nâœ“ Models to evaluate: {list(MODELS_TO_EVALUATE.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Evaluation Functions <a name=\"3-evaluation-functions\"></a>\n",
    "\n",
    "Shared utilities for all benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class EvalResult:\n",
    "    \"\"\"Single evaluation result.\"\"\"\n",
    "    idx: int\n",
    "    prompt: str\n",
    "    correct_answer: str\n",
    "    model_response: str\n",
    "    extracted_answer: str\n",
    "    is_correct: bool\n",
    "    time_sec: float\n",
    "    input_tokens: int\n",
    "    output_tokens: int\n",
    "    metadata: Dict[str, Any] = None\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'idx': self.idx,\n",
    "            'prompt': self.prompt,\n",
    "            'correct_answer': self.correct_answer,\n",
    "            'model_response': self.model_response,\n",
    "            'extracted_answer': self.extracted_answer,\n",
    "            'is_correct': self.is_correct,\n",
    "            'time_sec': self.time_sec,\n",
    "            'input_tokens': self.input_tokens,\n",
    "            'output_tokens': self.output_tokens,\n",
    "            'metadata': self.metadata or {}\n",
    "        }\n",
    "\n",
    "\n",
    "def extract_answer_tags(response: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract answer from <answer> tags, skipping placeholders like '??'.\n",
    "    Falls back to location pattern or first word.\n",
    "    \"\"\"\n",
    "    # Find all <answer> tags\n",
    "    matches = re.findall(r'<answer>\\s*([^<]+)\\s*</answer>', response, re.IGNORECASE)\n",
    "    \n",
    "    # Return first non-placeholder answer\n",
    "    for match in matches:\n",
    "        content = match.strip().lower()\n",
    "        if content and content not in ('??', '???', '?'):\n",
    "            return content\n",
    "    \n",
    "    # Fallback: location pattern (word_word)\n",
    "    match = re.search(r'\\b(\\w+_\\w+)\\b', response)\n",
    "    if match:\n",
    "        return match.group(1).lower()\n",
    "    \n",
    "    # Last resort: first word\n",
    "    return response.strip().lower().split()[0] if response.strip() else \"\"\n",
    "\n",
    "\n",
    "def extract_answer_choice(response: str, choices: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Extract answer for multiple choice questions.\n",
    "    Looks for choice letters (A, B, C, D) or exact choice text.\n",
    "    \"\"\"\n",
    "    response_lower = response.lower().strip()\n",
    "    \n",
    "    # Check for letter answers like \"A\", \"(A)\", \"A.\", \"A:\"\n",
    "    letter_match = re.search(r'^\\s*\\(?([a-d])\\)?[.:\\s]', response_lower)\n",
    "    if letter_match:\n",
    "        letter = letter_match.group(1).upper()\n",
    "        idx = ord(letter) - ord('A')\n",
    "        if idx < len(choices):\n",
    "            return choices[idx]\n",
    "    \n",
    "    # Check if response starts with a choice\n",
    "    for choice in choices:\n",
    "        if response_lower.startswith(choice.lower()):\n",
    "            return choice\n",
    "    \n",
    "    # Check if any choice appears in response\n",
    "    for choice in choices:\n",
    "        if choice.lower() in response_lower:\n",
    "            return choice\n",
    "    \n",
    "    return response_lower.split()[0] if response_lower else \"\"\n",
    "\n",
    "\n",
    "def extract_answer_binary(response: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract yes/no or true/false answer.\n",
    "    \"\"\"\n",
    "    response_lower = response.lower().strip()\n",
    "    \n",
    "    # Check for explicit yes/no\n",
    "    if response_lower.startswith('yes') or 'yes' in response_lower[:20]:\n",
    "        return 'yes'\n",
    "    if response_lower.startswith('no') or response_lower.startswith('not '):\n",
    "        return 'no'\n",
    "    if 'true' in response_lower[:20]:\n",
    "        return 'yes'\n",
    "    if 'false' in response_lower[:20]:\n",
    "        return 'no'\n",
    "    \n",
    "    return response_lower.split()[0] if response_lower else \"\"\n",
    "\n",
    "\n",
    "print(\"âœ“ Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Display, analysis, and save functions defined\n"
     ]
    }
   ],
   "source": [
    "def show_results_summary(results: List[EvalResult], title: str):\n",
    "    \"\"\"Display summary statistics for evaluation results.\"\"\"\n",
    "    if not results:\n",
    "        print(f\"No results for {title}\")\n",
    "        return\n",
    "    \n",
    "    n = len(results)\n",
    "    correct = sum(1 for r in results if r.is_correct)\n",
    "    accuracy = correct / n\n",
    "    avg_time = sum(r.time_sec for r in results) / n\n",
    "    total_time = sum(r.time_sec for r in results)\n",
    "    \n",
    "    table = Table(title=title, box=box.ROUNDED)\n",
    "    table.add_column(\"Metric\", style=\"cyan\")\n",
    "    table.add_column(\"Value\", style=\"green\")\n",
    "    \n",
    "    table.add_row(\"Accuracy\", f\"{accuracy:.1%}\")\n",
    "    table.add_row(\"Correct / Total\", f\"{correct}/{n}\")\n",
    "    table.add_row(\"Total Time\", f\"{total_time:.1f}s\")\n",
    "    table.add_row(\"Avg Time/Example\", f\"{avg_time:.2f}s\")\n",
    "    \n",
    "    console.print(table)\n",
    "    return {'accuracy': accuracy, 'correct': correct, 'total': n, 'time': total_time}\n",
    "\n",
    "\n",
    "def save_results(\n",
    "    results: List[EvalResult], \n",
    "    benchmark_name: str, \n",
    "    model_name: str,\n",
    "    output_dir: str = \"eval_outputs\",\n",
    "    save_json: bool = True,\n",
    "    save_csv: bool = True\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Save evaluation results to JSON and/or CSV files.\n",
    "    \n",
    "    Args:\n",
    "        results: List of EvalResult objects\n",
    "        benchmark_name: Name of the benchmark (e.g., \"tomi_tom\", \"fantom_belief\")\n",
    "        model_name: Name of the model being evaluated\n",
    "        output_dir: Directory to save outputs\n",
    "        save_json: Whether to save JSON format\n",
    "        save_csv: Whether to save CSV format\n",
    "    \n",
    "    Returns:\n",
    "        Dict with paths to saved files\n",
    "    \"\"\"\n",
    "    import csv\n",
    "    \n",
    "    if not results:\n",
    "        print(f\"No results to save for {benchmark_name}\")\n",
    "        return {}\n",
    "    \n",
    "    # Create output directory\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Generate filename with timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    base_name = f\"{benchmark_name}_{model_name}_{timestamp}\"\n",
    "    \n",
    "    saved_files = {}\n",
    "    \n",
    "    # Convert results to list of dicts\n",
    "    results_data = [r.to_dict() for r in results]\n",
    "    \n",
    "    # Add summary stats\n",
    "    accuracy = sum(1 for r in results if r.is_correct) / len(results)\n",
    "    summary = {\n",
    "        'benchmark': benchmark_name,\n",
    "        'model': model_name,\n",
    "        'timestamp': timestamp,\n",
    "        'total_examples': len(results),\n",
    "        'correct': sum(1 for r in results if r.is_correct),\n",
    "        'accuracy': accuracy,\n",
    "        'total_time_sec': sum(r.time_sec for r in results)\n",
    "    }\n",
    "    \n",
    "    # Save JSON (includes full data)\n",
    "    if save_json:\n",
    "        json_path = output_path / f\"{base_name}.json\"\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'summary': summary,\n",
    "                'results': results_data\n",
    "            }, f, indent=2, ensure_ascii=False)\n",
    "        saved_files['json'] = str(json_path)\n",
    "        print(f\"  ðŸ’¾ Saved JSON: {json_path}\")\n",
    "    \n",
    "    # Save CSV (flattened for spreadsheet viewing)\n",
    "    if save_csv:\n",
    "        csv_path = output_path / f\"{base_name}.csv\"\n",
    "        \n",
    "        # Flatten the results for CSV\n",
    "        fieldnames = [\n",
    "            'idx', 'is_correct', 'correct_answer', 'extracted_answer',\n",
    "            'model_response', 'prompt', 'time_sec', 'input_tokens', 'output_tokens'\n",
    "        ]\n",
    "        \n",
    "        with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')\n",
    "            writer.writeheader()\n",
    "            for r in results_data:\n",
    "                # Flatten metadata into the row if needed\n",
    "                row = {k: v for k, v in r.items() if k != 'metadata'}\n",
    "                writer.writerow(row)\n",
    "        \n",
    "        saved_files['csv'] = str(csv_path)\n",
    "        print(f\"  ðŸ’¾ Saved CSV:  {csv_path}\")\n",
    "    \n",
    "    return saved_files\n",
    "\n",
    "\n",
    "def show_sample_results(\n",
    "    results: List[EvalResult], \n",
    "    title: str, \n",
    "    n_correct: int = 3, \n",
    "    n_incorrect: int = 5,\n",
    "    prompt_max_len: int = 200,\n",
    "    response_max_len: int = 150\n",
    "):\n",
    "    \"\"\"\n",
    "    Display sample results for debugging, showing both correct and incorrect examples.\n",
    "    \n",
    "    Args:\n",
    "        results: List of evaluation results\n",
    "        title: Title for the display\n",
    "        n_correct: Number of correct examples to show\n",
    "        n_incorrect: Number of incorrect examples to show\n",
    "        prompt_max_len: Max characters to show from prompt\n",
    "        response_max_len: Max characters to show from response\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(f\"No results for {title}\")\n",
    "        return\n",
    "    \n",
    "    correct_results = [r for r in results if r.is_correct]\n",
    "    incorrect_results = [r for r in results if not r.is_correct]\n",
    "    \n",
    "    def truncate(text: str, max_len: int) -> str:\n",
    "        text = text.replace('\\n', ' â†µ ')\n",
    "        if len(text) > max_len:\n",
    "            return text[:max_len] + \"...\"\n",
    "        return text\n",
    "    \n",
    "    # Show incorrect examples first (more important for debugging)\n",
    "    if incorrect_results:\n",
    "        table = Table(\n",
    "            title=f\"âŒ {title} - INCORRECT Examples ({len(incorrect_results)} total)\",\n",
    "            box=box.ROUNDED,\n",
    "            show_lines=True,\n",
    "            width=120\n",
    "        )\n",
    "        table.add_column(\"#\", style=\"dim\", width=4)\n",
    "        table.add_column(\"Expected\", style=\"green\", width=15)\n",
    "        table.add_column(\"Extracted\", style=\"red\", width=15)\n",
    "        table.add_column(\"Response\", style=\"yellow\", width=40)\n",
    "        table.add_column(\"Prompt (end)\", style=\"dim\", width=40)\n",
    "        \n",
    "        for r in incorrect_results[:n_incorrect]:\n",
    "            # Show end of prompt (usually contains the question)\n",
    "            prompt_end = r.prompt[-prompt_max_len:] if len(r.prompt) > prompt_max_len else r.prompt\n",
    "            table.add_row(\n",
    "                str(r.idx),\n",
    "                r.correct_answer[:15],\n",
    "                r.extracted_answer[:15] if r.extracted_answer else \"(empty)\",\n",
    "                truncate(r.model_response, response_max_len),\n",
    "                truncate(prompt_end, prompt_max_len)\n",
    "            )\n",
    "        \n",
    "        console.print(table)\n",
    "    else:\n",
    "        print(f\"âœ“ {title}: No incorrect examples!\")\n",
    "    \n",
    "    # Show a few correct examples for comparison\n",
    "    if correct_results and n_correct > 0:\n",
    "        table = Table(\n",
    "            title=f\"âœ“ {title} - CORRECT Examples (sample of {len(correct_results)} total)\",\n",
    "            box=box.SIMPLE,\n",
    "            show_lines=True,\n",
    "            width=120\n",
    "        )\n",
    "        table.add_column(\"#\", style=\"dim\", width=4)\n",
    "        table.add_column(\"Expected\", style=\"green\", width=15)\n",
    "        table.add_column(\"Extracted\", style=\"green\", width=15)\n",
    "        table.add_column(\"Response\", style=\"cyan\", width=40)\n",
    "        \n",
    "        for r in correct_results[:n_correct]:\n",
    "            table.add_row(\n",
    "                str(r.idx),\n",
    "                r.correct_answer[:15],\n",
    "                r.extracted_answer[:15],\n",
    "                truncate(r.model_response, response_max_len)\n",
    "            )\n",
    "        \n",
    "        console.print(table)\n",
    "\n",
    "\n",
    "def analyze_failure_patterns(results: List[EvalResult], title: str):\n",
    "    \"\"\"Analyze common failure patterns in results.\"\"\"\n",
    "    incorrect = [r for r in results if not r.is_correct]\n",
    "    \n",
    "    if not incorrect:\n",
    "        print(f\"âœ“ {title}: No failures to analyze!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nðŸ“Š {title} - Failure Analysis ({len(incorrect)} failures)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Check for empty extractions\n",
    "    empty_extractions = sum(1 for r in incorrect if not r.extracted_answer or r.extracted_answer in ('', '?', '??'))\n",
    "    if empty_extractions:\n",
    "        print(f\"  â€¢ Empty/invalid extractions: {empty_extractions}\")\n",
    "    \n",
    "    # Check for partial matches (one contains the other)\n",
    "    partial_matches = sum(1 for r in incorrect \n",
    "                         if r.extracted_answer and r.correct_answer.lower() in r.extracted_answer.lower())\n",
    "    if partial_matches:\n",
    "        print(f\"  â€¢ Partial matches (answer in extraction): {partial_matches}\")\n",
    "    \n",
    "    reverse_partial = sum(1 for r in incorrect \n",
    "                         if r.extracted_answer and r.extracted_answer.lower() in r.correct_answer.lower())\n",
    "    if reverse_partial:\n",
    "        print(f\"  â€¢ Partial matches (extraction in answer): {reverse_partial}\")\n",
    "    \n",
    "    # Check for responses that contain correct answer but extraction failed\n",
    "    answer_in_response = sum(1 for r in incorrect \n",
    "                            if r.correct_answer.lower() in r.model_response.lower())\n",
    "    if answer_in_response:\n",
    "        print(f\"  â€¢ Correct answer in response but extraction failed: {answer_in_response}\")\n",
    "    \n",
    "    # Show unique extracted answers for failures\n",
    "    extracted_counts = {}\n",
    "    for r in incorrect:\n",
    "        ext = r.extracted_answer if r.extracted_answer else \"(empty)\"\n",
    "        extracted_counts[ext] = extracted_counts.get(ext, 0) + 1\n",
    "    \n",
    "    if extracted_counts:\n",
    "        print(f\"\\n  Top incorrect extractions:\")\n",
    "        for ext, count in sorted(extracted_counts.items(), key=lambda x: -x[1])[:5]:\n",
    "            print(f\"    '{ext}': {count}x\")\n",
    "\n",
    "\n",
    "def compute_accuracy(results: List[EvalResult]) -> float:\n",
    "    if not results:\n",
    "        return 0.0\n",
    "    return sum(1 for r in results if r.is_correct) / len(results)\n",
    "\n",
    "\n",
    "print(\"âœ“ Display, analysis, and save functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. ToMi Evaluation <a name=\"4-tomi\"></a>\n",
    "\n",
    "**ToMi** (Theory of Mind Inventory) tests first-order and second-order false belief understanding through procedurally generated short stories.\n",
    "\n",
    "- **ToM condition**: Questions require tracking a character's (false) belief\n",
    "- **No-ToM condition**: Questions only require tracking what actually happened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ToMi evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "class ToMiDataset:\n",
    "    \"\"\"Dataset loader for ToMi benchmark.\"\"\"\n",
    "    \n",
    "    def __init__(self, jsonl_path: str, size: int = None):\n",
    "        self.jsonl_path = Path(jsonl_path)\n",
    "        self.data = []\n",
    "        \n",
    "        with open(self.jsonl_path, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if size is not None and i >= size:\n",
    "                    break\n",
    "                self.data.append(json.loads(line))\n",
    "        \n",
    "        self.size = len(self.data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"ToMiDataset(n={self.size})\"\n",
    "\n",
    "\n",
    "# Improved ToMi prompt - emphasizes perspective-taking and cleaner output format\n",
    "TOMI_SYSTEM_PROMPT = \"\"\"You are answering a question about what a CHARACTER BELIEVES, not what is actually true.\n",
    "\n",
    "CRITICAL: The question asks where a character THINKS something is located.\n",
    "- Characters ONLY know about events they WITNESSED\n",
    "- If a character LEFT before an object was moved, they still believe it's in the ORIGINAL location\n",
    "- Track what each character SAW, not what actually happened\n",
    "\n",
    "First reason briefly in <think> tags (2-3 sentences max).\n",
    "Then give your final answer in <answer> tags with ONLY the location name.\n",
    "\n",
    "IMPORTANT: \n",
    "- Use <answer> tags exactly ONCE with your final answer\n",
    "- Do NOT put ?? or placeholders in the answer tags\n",
    "- Format: <answer>location_name</answer>\n",
    "\n",
    "Example: <answer>blue_pantry</answer>\"\"\"\n",
    "\n",
    "\n",
    "def normalize_tomi_answer(answer: str) -> str:\n",
    "    \"\"\"Normalize ToMi answer by converting underscores to spaces and lowercasing.\"\"\"\n",
    "    return answer.lower().replace('_', ' ').strip()\n",
    "\n",
    "\n",
    "def extract_answer_tags(response: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract answer from <answer> tags, skipping placeholders like '??'.\n",
    "    Falls back to location pattern or first word.\n",
    "    \"\"\"\n",
    "    # Find all <answer> tags\n",
    "    matches = re.findall(r'<answer>\\s*([^<]+)\\s*</answer>', response, re.IGNORECASE)\n",
    "    \n",
    "    # Return LAST non-placeholder answer (model often puts real answer at end)\n",
    "    for match in reversed(matches):\n",
    "        content = match.strip().lower()\n",
    "        if content and content not in ('??', '???', '?', '', 'blank', '...'):\n",
    "            return content\n",
    "    \n",
    "    # Fallback: look for location pattern ONLY after </think> if present\n",
    "    think_end = response.lower().find('</think>')\n",
    "    search_text = response[think_end:] if think_end >= 0 else response\n",
    "    \n",
    "    # Look for location pattern (word_word)\n",
    "    match = re.search(r'\\b([a-z]+_[a-z]+)\\b', search_text.lower())\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    # Last resort: first word after </think>\n",
    "    if think_end >= 0:\n",
    "        after_think = response[think_end + 8:].strip()\n",
    "        first_word = after_think.split()[0] if after_think.split() else \"\"\n",
    "        return first_word.lower()\n",
    "    \n",
    "    return response.strip().lower().split()[0] if response.strip() else \"\"\n",
    "\n",
    "\n",
    "def evaluate_tomi(\n",
    "    dataset: ToMiDataset, \n",
    "    backend: ModelBackend,\n",
    "    desc: str = \"ToMi\",\n",
    "    max_examples: int = None\n",
    ") -> List[EvalResult]:\n",
    "    \"\"\"Evaluate model on ToMi dataset.\"\"\"\n",
    "    results = []\n",
    "    n = min(len(dataset), max_examples) if max_examples else len(dataset)\n",
    "    \n",
    "    for i in tqdm(range(n), desc=desc):\n",
    "        example = dataset[i]\n",
    "        prompt = f\"{TOMI_SYSTEM_PROMPT}\\n\\nStory:\\n{example['prompt']}\"\n",
    "        \n",
    "        response, elapsed, in_tok, out_tok = backend.generate(prompt)\n",
    "        extracted = extract_answer_tags(response)\n",
    "        \n",
    "        # Normalize both answers to handle underscore vs space differences\n",
    "        is_correct = normalize_tomi_answer(extracted) == normalize_tomi_answer(example['answer'])\n",
    "        \n",
    "        results.append(EvalResult(\n",
    "            idx=i,\n",
    "            prompt=prompt,\n",
    "            correct_answer=example['answer'],\n",
    "            model_response=response,\n",
    "            extracted_answer=extracted,\n",
    "            is_correct=is_correct,\n",
    "            time_sec=elapsed,\n",
    "            input_tokens=in_tok,\n",
    "            output_tokens=out_tok,\n",
    "            metadata={'story_type': example.get('story_type'), 'question_type': example.get('question_type')}\n",
    "        ))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"âœ“ ToMi evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ToMi data found at ../tom_benchmarks/tomi/tomi_pairs\n",
      "-rw-r--r-- 1 root root 1599533 Jan 22 14:41 ../tom_benchmarks/tomi/tomi_pairs/all_no_tom.jsonl\n",
      "-rw-r--r-- 1 root root  471563 Jan 22 14:41 ../tom_benchmarks/tomi/tomi_pairs/all_tom.jsonl\n",
      "-rw-r--r-- 1 root root  468215 Jan 22 14:41 ../tom_benchmarks/tomi/tomi_pairs/first_order_0_no_tom.jsonl\n",
      "-rw-r--r-- 1 root root  452339 Jan 22 14:41 ../tom_benchmarks/tomi/tomi_pairs/first_order_0_no_tom_prompts.jsonl\n",
      "-rw-r--r-- 1 root root  285728 Jan 22 14:41 ../tom_benchmarks/tomi/tomi_pairs/first_order_1_no_tom.jsonl\n",
      "-rw-r--r-- 1 root root  275842 Jan 22 14:41 ../tom_benchmarks/tomi/tomi_pairs/first_order_1_no_tom_prompts.jsonl\n",
      "-rw-r--r-- 1 root root  180769 Jan 22 14:41 ../tom_benchmarks/tomi/tomi_pairs/first_order_1_tom.jsonl\n",
      "-rw-r--r-- 1 root root  174779 Jan 22 14:41 ../tom_benchmarks/tomi/tomi_pairs/first_order_1_tom_prompts.jsonl\n",
      "-rw-r--r-- 1 root root  362752 Jan 22 14:41 ../tom_benchmarks/tomi/tomi_pairs/second_order_0_no_tom.jsonl\n",
      "-rw-r--r-- 1 root root  350782 Jan 22 14:41 ../tom_benchmarks/tomi/tomi_pairs/second_order_0_no_tom_prompts.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Configure ToMi paths - adjust as needed\n",
    "TOMI_DIR = Path('../tom_benchmarks/tomi/tomi_pairs')\n",
    "\n",
    "# Check if data exists\n",
    "if TOMI_DIR.exists():\n",
    "    print(f\"âœ“ ToMi data found at {TOMI_DIR}\")\n",
    "    !ls -la {TOMI_DIR}/*.jsonl | head -10\n",
    "else:\n",
    "    print(f\"âš  ToMi data not found at {TOMI_DIR}\")\n",
    "    print(\"  Run the ToMi extractor script first, or adjust TOMI_DIR path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluating ToMi with LOCAL (100 examples per condition)\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ToMi-ToM (local): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [32:57<00:00, 19.78s/it]\n",
      "ToMi-NoToM (local):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [12:06<20:37, 19.65s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m all_tomi_results[model_name] = {}\n\u001b[32m     21\u001b[39m all_tomi_results[model_name][\u001b[33m'\u001b[39m\u001b[33mtom\u001b[39m\u001b[33m'\u001b[39m] = evaluate_tomi(tom_dataset, backend, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mToMi-ToM (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m, N_TOMI)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m all_tomi_results[model_name][\u001b[33m'\u001b[39m\u001b[33mno_tom\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mevaluate_tomi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mno_tom_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mToMi-NoToM (\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_TOMI\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmodel_name.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ToMi RESULTS:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m tom_stats = show_results_summary(all_tomi_results[model_name][\u001b[33m'\u001b[39m\u001b[33mtom\u001b[39m\u001b[33m'\u001b[39m], \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mToMi: ToM (false belief) - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 96\u001b[39m, in \u001b[36mevaluate_tomi\u001b[39m\u001b[34m(dataset, backend, desc, max_examples)\u001b[39m\n\u001b[32m     93\u001b[39m example = dataset[i]\n\u001b[32m     94\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTOMI_SYSTEM_PROMPT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStory:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample[\u001b[33m'\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m response, elapsed, in_tok, out_tok = \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m extracted = extract_answer_tags(response)\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# Normalize both answers to handle underscore vs space differences\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mLocalModelBackend.generate\u001b[39m\u001b[34m(self, prompt, max_tokens)\u001b[39m\n\u001b[32m     49\u001b[39m start_time = time.time()\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m elapsed = time.time() - start_time\n\u001b[32m     60\u001b[39m response = \u001b[38;5;28mself\u001b[39m.tokenizer.decode(output[\u001b[32m0\u001b[39m][input_len:], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/transformers/generation/utils.py:2539\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2528\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m GenerationMixin.generate(\n\u001b[32m   2529\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2530\u001b[39m         inputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2534\u001b[39m         **kwargs,\n\u001b[32m   2535\u001b[39m     )\n\u001b[32m   2537\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n\u001b[32m   2538\u001b[39m     \u001b[38;5;66;03m# 11. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2539\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2540\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2541\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2542\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2543\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2544\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2545\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2546\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2547\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2549\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2550\u001b[39m     \u001b[38;5;66;03m# 11. run beam sample\u001b[39;00m\n\u001b[32m   2551\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._beam_search(\n\u001b[32m   2552\u001b[39m         input_ids,\n\u001b[32m   2553\u001b[39m         logits_processor=prepared_logits_processor,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2557\u001b[39m         **model_kwargs,\n\u001b[32m   2558\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/transformers/generation/utils.py:2870\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2868\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2869\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2870\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2872\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   2873\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2874\u001b[39m     outputs,\n\u001b[32m   2875\u001b[39m     model_kwargs,\n\u001b[32m   2876\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2877\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/transformers/utils/generic.py:940\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    939\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    942\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/transformers/models/gpt_oss/modeling_gpt_oss.py:663\u001b[39m, in \u001b[36mGptOssForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_router_logits, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    658\u001b[39m output_router_logits = (\n\u001b[32m    659\u001b[39m     output_router_logits \u001b[38;5;28;01mif\u001b[39;00m output_router_logits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_router_logits\n\u001b[32m    660\u001b[39m )\n\u001b[32m    662\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m outputs: MoeModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_router_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_router_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    675\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    676\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/transformers/utils/generic.py:1064\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m                 module.forward = make_capture_wrapper(module, original_forward, key, specs.index)\n\u001b[32m   1062\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;66;03m# Restore original forward methods\u001b[39;00m\n\u001b[32m   1066\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module, original_forward \u001b[38;5;129;01min\u001b[39;00m monkey_patched_layers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/transformers/models/gpt_oss/modeling_gpt_oss.py:502\u001b[39m, in \u001b[36mGptOssModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    499\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    512\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m MoeModelOutputWithPast(\n\u001b[32m    514\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    515\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    516\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/transformers/models/gpt_oss/modeling_gpt_oss.py:366\u001b[39m, in \u001b[36mGptOssDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    364\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    365\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m hidden_states, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    378\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/transformers/models/gpt_oss/modeling_gpt_oss.py:337\u001b[39m, in \u001b[36mGptOssAttention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[39m\n\u001b[32m    323\u001b[39m attn_output, attn_weights = attention_interface(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    325\u001b[39m     query_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    333\u001b[39m     **kwargs,\n\u001b[32m    334\u001b[39m )\n\u001b[32m    336\u001b[39m attn_output = attn_output.reshape(*input_shape, -\u001b[32m1\u001b[39m).contiguous()\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m attn_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mo_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, attn_weights\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/arena-env/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run ToMi evaluation\n",
    "N_TOMI = 100  # Examples per condition\n",
    "\n",
    "all_tomi_results = {}  # model_name -> {tom: [...], no_tom: [...]}\n",
    "\n",
    "if TOMI_DIR.exists():\n",
    "    # Load datasets\n",
    "    tom_file = TOMI_DIR / 'first_order_1_tom_prompts.jsonl'\n",
    "    no_tom_file = TOMI_DIR / 'first_order_1_no_tom_prompts.jsonl'\n",
    "    \n",
    "    if tom_file.exists() and no_tom_file.exists():\n",
    "        tom_dataset = ToMiDataset(tom_file, size=N_TOMI)\n",
    "        no_tom_dataset = ToMiDataset(no_tom_file, size=N_TOMI)\n",
    "        \n",
    "        for model_name, backend in MODELS_TO_EVALUATE.items():\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Evaluating ToMi with {model_name.upper()} ({N_TOMI} examples per condition)\")\n",
    "            print(f\"{'='*60}\\n\")\n",
    "            \n",
    "            all_tomi_results[model_name] = {}\n",
    "            all_tomi_results[model_name]['tom'] = evaluate_tomi(tom_dataset, backend, f\"ToMi-ToM ({model_name})\", N_TOMI)\n",
    "            all_tomi_results[model_name]['no_tom'] = evaluate_tomi(no_tom_dataset, backend, f\"ToMi-NoToM ({model_name})\", N_TOMI)\n",
    "            \n",
    "            print(f\"\\n{model_name.upper()} ToMi RESULTS:\")\n",
    "            tom_stats = show_results_summary(all_tomi_results[model_name]['tom'], f\"ToMi: ToM (false belief) - {model_name}\")\n",
    "            print()\n",
    "            no_tom_stats = show_results_summary(all_tomi_results[model_name]['no_tom'], f\"ToMi: No-ToM (true belief) - {model_name}\")\n",
    "            \n",
    "            if tom_stats and no_tom_stats:\n",
    "                gap = no_tom_stats['accuracy'] - tom_stats['accuracy']\n",
    "                print(f\"\\nðŸ“Š ToM Gap: {gap:+.1%} (expected: No-ToM > ToM)\")\n",
    "            \n",
    "            # Save results to files\n",
    "            print(f\"\\nðŸ“ Saving results...\")\n",
    "            save_results(all_tomi_results[model_name]['tom'], \"tomi_tom\", model_name)\n",
    "            save_results(all_tomi_results[model_name]['no_tom'], \"tomi_no_tom\", model_name)\n",
    "            \n",
    "            # Show sample results for debugging\n",
    "            print(f\"\\n{'â”€'*60}\")\n",
    "            print(f\"SAMPLE RESULTS FOR DEBUGGING - {model_name.upper()}\")\n",
    "            print(f\"{'â”€'*60}\")\n",
    "            show_sample_results(all_tomi_results[model_name]['tom'], f\"ToMi-ToM ({model_name})\")\n",
    "            analyze_failure_patterns(all_tomi_results[model_name]['tom'], f\"ToMi-ToM ({model_name})\")\n",
    "            print()\n",
    "            show_sample_results(all_tomi_results[model_name]['no_tom'], f\"ToMi-NoToM ({model_name})\")\n",
    "            analyze_failure_patterns(all_tomi_results[model_name]['no_tom'], f\"ToMi-NoToM ({model_name})\")\n",
    "    else:\n",
    "        print(f\"âš  ToMi prompt files not found. Available files:\")\n",
    "        !ls {TOMI_DIR}\n",
    "else:\n",
    "    print(\"â­ Skipping ToMi (data not found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. FANToM Evaluation <a name=\"5-fantom\"></a>\n",
    "\n",
    "**FANToM** (False-belief ANd Theory of Mind) tests ToM in multi-party **conversations** with information asymmetry.\n",
    "\n",
    "Characters join/leave conversations, creating natural false beliefs about shared information.\n",
    "\n",
    "Question types:\n",
    "- **BeliefQ**: What does character X believe?\n",
    "- **AnswerabilityQ**: Can character X answer question Y?\n",
    "- **InfoAccessQ**: Who has access to information Z?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download FANToM if not present\n",
    "# FANToM data is downloaded via their dataset_loader.py script\n",
    "import sys\n",
    "FANTOM_DIR = Path('fantom')\n",
    "FANTOM_DATA_DIR = FANTOM_DIR / 'task' / 'data' / 'fantom'\n",
    "\n",
    "if not (FANTOM_DATA_DIR / 'fantom_v1.json').exists():\n",
    "    print(\"Downloading FANToM benchmark data...\")\n",
    "    # Add the task directory to path and use their loader\n",
    "    sys.path.insert(0, str(FANTOM_DIR / 'task'))\n",
    "    try:\n",
    "        from dataset_loader import load as load_fantom\n",
    "        fantom_df = load_fantom()  # This downloads and returns DataFrame\n",
    "        print(f\"âœ“ FANToM downloaded: {len(fantom_df)} conversations\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Could not download FANToM: {e}\")\n",
    "        print(\"  Try manually: cd fantom/task && python dataset_loader.py\")\n",
    "    finally:\n",
    "        sys.path.pop(0)\n",
    "else:\n",
    "    print(f\"âœ“ FANToM data exists at {FANTOM_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FANToMDataset:\n",
    "    \"\"\"Dataset loader for FANToM benchmark.\"\"\"\n",
    "    \n",
    "    def __init__(self, json_path: str, size: int = None, use_short_context: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            json_path: Path to fantom_v1.json\n",
    "            size: Max items to load (None for all)\n",
    "            use_short_context: Use short_context (True) or full_context (False)\n",
    "        \"\"\"\n",
    "        self.json_path = Path(json_path)\n",
    "        self.use_short_context = use_short_context\n",
    "        \n",
    "        # Load as DataFrame then convert to list of dicts\n",
    "        import pandas as pd\n",
    "        df = pd.read_json(self.json_path)\n",
    "        \n",
    "        self.data = []\n",
    "        for _, row in df.iterrows():\n",
    "            self.data.append(row.to_dict())\n",
    "            if size and len(self.data) >= size:\n",
    "                break\n",
    "        \n",
    "        self.size = len(self.data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "FANTOM_SYSTEM_PROMPT = \"\"\"You are answering questions about a conversation. Pay attention to who was present when information was shared.\n",
    "\n",
    "Answer the question directly and concisely.\"\"\"\n",
    "\n",
    "\n",
    "def format_fantom_prompt(item: dict, question_type: str = 'belief', use_short_context: bool = True) -> Tuple[str, str, List[str]]:\n",
    "    \"\"\"\n",
    "    Format a FANToM item into a prompt.\n",
    "    \n",
    "    Args:\n",
    "        item: Data item from FANToM dataset\n",
    "        question_type: 'belief' for beliefQAs, 'fact' for factQA\n",
    "        use_short_context: Use short_context (True) or full_context (False)\n",
    "    \n",
    "    Returns: (prompt, correct_answer, choices) or (None, None, None) if no question available\n",
    "    \"\"\"\n",
    "    # Get conversation context\n",
    "    context_key = 'short_context' if use_short_context else 'full_context'\n",
    "    conversation = item.get(context_key, '')\n",
    "    \n",
    "    if question_type == 'belief':\n",
    "        # beliefQAs is a list of question dicts\n",
    "        belief_qas = item.get('beliefQAs', [])\n",
    "        if not belief_qas:\n",
    "            return None, None, None\n",
    "        \n",
    "        # Take first belief question\n",
    "        q_data = belief_qas[0]\n",
    "        question = q_data.get('question', '')\n",
    "        correct = q_data.get('correct_answer', '')\n",
    "        wrong = q_data.get('wrong_answer', '')\n",
    "        \n",
    "        # Create binary choice\n",
    "        choices = [correct, wrong]\n",
    "        \n",
    "        prompt = f\"{FANTOM_SYSTEM_PROMPT}\\n\\nConversation:\\n{conversation}\\n\\n\"\n",
    "        prompt += f\"Question: {question}\\n\\n\"\n",
    "        prompt += f\"A. {correct}\\n\"\n",
    "        prompt += f\"B. {wrong}\\n\"\n",
    "        prompt += \"\\nAnswer with the letter (A or B):\"\n",
    "        \n",
    "        return prompt, 'A', choices  # Correct answer is always 'A' (first choice)\n",
    "    \n",
    "    elif question_type == 'fact':\n",
    "        # factQA is a single dict\n",
    "        fact_qa = item.get('factQA', {})\n",
    "        if not fact_qa:\n",
    "            return None, None, None\n",
    "        \n",
    "        question = fact_qa.get('question', '')\n",
    "        correct = fact_qa.get('correct_answer', '')\n",
    "        wrong = fact_qa.get('wrong_answer', '')\n",
    "        \n",
    "        choices = [correct, wrong]\n",
    "        \n",
    "        prompt = f\"{FANTOM_SYSTEM_PROMPT}\\n\\nConversation:\\n{conversation}\\n\\n\"\n",
    "        prompt += f\"Question: {question}\\n\\n\"\n",
    "        prompt += f\"A. {correct}\\n\"\n",
    "        prompt += f\"B. {wrong}\\n\"\n",
    "        prompt += \"\\nAnswer with the letter (A or B):\"\n",
    "        \n",
    "        return prompt, 'A', choices\n",
    "    \n",
    "    return None, None, None\n",
    "\n",
    "\n",
    "def extract_fantom_answer(response: str) -> str:\n",
    "    \"\"\"Extract A or B from response.\"\"\"\n",
    "    response_upper = response.strip().upper()\n",
    "    \n",
    "    # Check for A or B at start\n",
    "    if response_upper.startswith('A'):\n",
    "        return 'A'\n",
    "    if response_upper.startswith('B'):\n",
    "        return 'B'\n",
    "    \n",
    "    # Check for (A) or (B) pattern\n",
    "    if '(A)' in response_upper or 'ANSWER: A' in response_upper or 'ANSWER IS A' in response_upper:\n",
    "        return 'A'\n",
    "    if '(B)' in response_upper or 'ANSWER: B' in response_upper or 'ANSWER IS B' in response_upper:\n",
    "        return 'B'\n",
    "    \n",
    "    # Check which appears first\n",
    "    pos_a = response_upper.find('A')\n",
    "    pos_b = response_upper.find('B')\n",
    "    \n",
    "    if pos_a >= 0 and (pos_b < 0 or pos_a < pos_b):\n",
    "        return 'A'\n",
    "    if pos_b >= 0:\n",
    "        return 'B'\n",
    "    \n",
    "    return response.strip()[:1].upper() if response.strip() else \"\"\n",
    "\n",
    "\n",
    "def evaluate_fantom(\n",
    "    dataset: FANToMDataset,\n",
    "    backend: ModelBackend,\n",
    "    question_type: str = 'belief',\n",
    "    max_examples: int = None\n",
    ") -> List[EvalResult]:\n",
    "    \"\"\"Evaluate model on FANToM dataset.\"\"\"\n",
    "    results = []\n",
    "    n = min(len(dataset), max_examples) if max_examples else len(dataset)\n",
    "    \n",
    "    for i in tqdm(range(n), desc=f\"FANToM-{question_type}\"):\n",
    "        item = dataset[i]\n",
    "        prompt, correct, choices = format_fantom_prompt(item, question_type)\n",
    "        \n",
    "        if prompt is None:\n",
    "            continue\n",
    "        \n",
    "        response, elapsed, in_tok, out_tok = backend.generate(prompt, max_tokens=50)\n",
    "        \n",
    "        extracted = extract_fantom_answer(response)\n",
    "        is_correct = extracted == correct\n",
    "        \n",
    "        results.append(EvalResult(\n",
    "            idx=i,\n",
    "            prompt=prompt,\n",
    "            correct_answer=correct,\n",
    "            model_response=response,\n",
    "            extracted_answer=extracted,\n",
    "            is_correct=is_correct,\n",
    "            time_sec=elapsed,\n",
    "            input_tokens=in_tok,\n",
    "            output_tokens=out_tok,\n",
    "            metadata={'question_type': question_type}\n",
    "        ))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"âœ“ FANToM evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run FANToM evaluation\n",
    "N_FANTOM = 30\n",
    "\n",
    "all_fantom_results = {}  # model_name -> {belief: [...], fact: [...]}\n",
    "\n",
    "# FANToM data file path\n",
    "fantom_data_file = FANTOM_DATA_DIR / 'fantom_v1.json'\n",
    "\n",
    "if fantom_data_file.exists():\n",
    "    print(f\"Loading FANToM from {fantom_data_file}\")\n",
    "    fantom_dataset = FANToMDataset(fantom_data_file, size=N_FANTOM)\n",
    "    print(f\"Loaded {len(fantom_dataset)} conversations\")\n",
    "    \n",
    "    for model_name, backend in MODELS_TO_EVALUATE.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Evaluating FANToM with {model_name.upper()}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        all_fantom_results[model_name] = {}\n",
    "        all_fantom_results[model_name]['belief'] = evaluate_fantom(fantom_dataset, backend, 'belief', N_FANTOM)\n",
    "        all_fantom_results[model_name]['fact'] = evaluate_fantom(fantom_dataset, backend, 'fact', N_FANTOM)\n",
    "        \n",
    "        print(f\"\\n{model_name.upper()} FANToM RESULTS:\")\n",
    "        show_results_summary(all_fantom_results[model_name]['belief'], f\"FANToM: Belief Questions - {model_name}\")\n",
    "        print()\n",
    "        show_results_summary(all_fantom_results[model_name]['fact'], f\"FANToM: Fact Questions - {model_name}\")\n",
    "        \n",
    "        # Save results to files\n",
    "        print(f\"\\nðŸ“ Saving results...\")\n",
    "        save_results(all_fantom_results[model_name]['belief'], \"fantom_belief\", model_name)\n",
    "        save_results(all_fantom_results[model_name]['fact'], \"fantom_fact\", model_name)\n",
    "        \n",
    "        # Show sample results for debugging\n",
    "        print(f\"\\n{'â”€'*60}\")\n",
    "        print(f\"SAMPLE RESULTS FOR DEBUGGING - {model_name.upper()}\")\n",
    "        print(f\"{'â”€'*60}\")\n",
    "        show_sample_results(all_fantom_results[model_name]['belief'], f\"FANToM-Belief ({model_name})\")\n",
    "        analyze_failure_patterns(all_fantom_results[model_name]['belief'], f\"FANToM-Belief ({model_name})\")\n",
    "        print()\n",
    "        show_sample_results(all_fantom_results[model_name]['fact'], f\"FANToM-Fact ({model_name})\")\n",
    "        analyze_failure_patterns(all_fantom_results[model_name]['fact'], f\"FANToM-Fact ({model_name})\")\n",
    "else:\n",
    "    print(f\"â­ Skipping FANToM (data not found at {fantom_data_file})\")\n",
    "    print(\"  Run the download cell above first, or manually: cd fantom/task && python dataset_loader.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. SimpleToM Evaluation <a name=\"6-simpletom\"></a>\n",
    "\n",
    "**SimpleToM** provides minimal, controlled ToM evaluation through brief 2-sentence narratives.\n",
    "\n",
    "- **Explicit ToM**: Direct questions about character beliefs\n",
    "- **Applied ToM**: Questions requiring ToM to answer correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ SimpleToM mental-state-qa loaded: 1147 examples\n",
      "  Columns: ['id', 'story', 'question', 'scenario_name', 'choices', 'answerKey']\n",
      "âœ“ SimpleToM behavior-qa loaded: 1147 examples\n"
     ]
    }
   ],
   "source": [
    "# Load SimpleToM from HuggingFace\n",
    "# SimpleToM has multiple configs: 'mental-state-qa', 'behavior-qa', 'judgment-qa', 'story-data'\n",
    "try:\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    # Load the mental-state-qa config (core ToM task)\n",
    "    simpletom_mental = load_dataset(\"allenai/SimpleToM\", \"mental-state-qa\", split=\"test\")\n",
    "    print(f\"âœ“ SimpleToM mental-state-qa loaded: {len(simpletom_mental)} examples\")\n",
    "    print(f\"  Columns: {simpletom_mental.column_names}\")\n",
    "    \n",
    "    # Optionally load behavior-qa (applied ToM)\n",
    "    simpletom_behavior = load_dataset(\"allenai/SimpleToM\", \"behavior-qa\", split=\"test\")\n",
    "    print(f\"âœ“ SimpleToM behavior-qa loaded: {len(simpletom_behavior)} examples\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš  Could not load SimpleToM: {e}\")\n",
    "    simpletom_mental = None\n",
    "    simpletom_behavior = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ SimpleToM evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "SIMPLETOM_SYSTEM_PROMPT = \"\"\"Answer the question about the story. Choose the best answer from the options provided.\n",
    "\n",
    "You may reason through the problem, but you MUST end your response with ONLY the letter of your final answer on the last line.\n",
    "\n",
    "Format your response like this:\n",
    "[Your reasoning here if needed]\n",
    "\n",
    "Final Answer: [Letter]\n",
    "\n",
    "Answer with ONLY the letter (A or B) at the end.\"\"\"\n",
    "\n",
    "\n",
    "def format_simpletom_prompt(example: dict) -> Tuple[str, str, List[str]]:\n",
    "    \"\"\"\n",
    "    Format a SimpleToM example into a prompt.\n",
    "    \n",
    "    Args:\n",
    "        example: Dataset example with 'story', 'question', 'choices', 'answerKey'\n",
    "    \n",
    "    Returns: (prompt, correct_answer_letter, choice_texts)\n",
    "    \"\"\"\n",
    "    story = example['story']\n",
    "    question = example['question']\n",
    "    choices = example['choices']\n",
    "    correct_key = example['answerKey']\n",
    "    \n",
    "    # choices is a dict with 'text' and 'label' lists\n",
    "    choice_texts = choices['text']\n",
    "    choice_labels = choices['label']\n",
    "    \n",
    "    prompt = f\"{SIMPLETOM_SYSTEM_PROMPT}\\n\\nStory: {story}\\n\\nQuestion: {question}\\n\\n\"\n",
    "    for label, text in zip(choice_labels, choice_texts):\n",
    "        prompt += f\"{label}. {text}\\n\"\n",
    "    prompt += \"\\nAnswer:\"\n",
    "    \n",
    "    return prompt, correct_key, choice_texts\n",
    "\n",
    "\n",
    "def extract_simpletom_answer(response: str, valid_labels: List[str] = ['A', 'B']) -> str:\n",
    "    \"\"\"\n",
    "    Extract answer letter from response.\n",
    "    Optimized for reasoning models that provide explanations before answering.\n",
    "    Searches from the END of the response backwards.\n",
    "    \"\"\"\n",
    "    response_clean = response.strip()\n",
    "    response_upper = response_clean.upper()\n",
    "    \n",
    "    # Strategy 1: Check if last character is a valid label\n",
    "    if response_clean and response_clean[-1].upper() in valid_labels:\n",
    "        return response_clean[-1].upper()\n",
    "    \n",
    "    # Strategy 2: Check last few lines for the answer\n",
    "    lines = response_clean.split('\\n')\n",
    "    for line in reversed(lines[-5:]):  # Check last 5 lines\n",
    "        line_stripped = line.strip().upper()\n",
    "        \n",
    "        # Check for exact match (just the letter)\n",
    "        if line_stripped in valid_labels:\n",
    "            return line_stripped\n",
    "        \n",
    "        # Check for \"Final Answer: A\" or \"Answer: A\" patterns\n",
    "        for pattern in [r'FINAL\\s*ANSWER\\s*:\\s*([AB])', r'ANSWER\\s*:\\s*([AB])', r'ANSWER\\s*IS\\s*([AB])']:\n",
    "            match = re.search(pattern, line_stripped)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "        \n",
    "        # Check if line ends with a valid label\n",
    "        words = line_stripped.split()\n",
    "        if words and words[-1] in valid_labels:\n",
    "            return words[-1]\n",
    "    \n",
    "    # Strategy 3: Find LAST occurrence of a standalone valid label\n",
    "    # Use word boundaries to avoid matching labels in quoted options like \"A.\"\n",
    "    last_pos = -1\n",
    "    last_label = \"\"\n",
    "    for label in valid_labels:\n",
    "        # Look for label as a standalone word (not \"A.\" from options)\n",
    "        for match in re.finditer(rf'\\b{label}\\b', response_upper):\n",
    "            if match.start() > last_pos:\n",
    "                last_pos = match.start()\n",
    "                last_label = label\n",
    "    \n",
    "    if last_label:\n",
    "        return last_label\n",
    "    \n",
    "    # Strategy 4: Check for letter at start (only if not followed by \".\")\n",
    "    # This avoids picking up \"A.\" when model quotes option A\n",
    "    for label in valid_labels:\n",
    "        if re.match(rf'^{label}(?![.])', response_upper):\n",
    "            return label\n",
    "    \n",
    "    # Ultimate fallback: return empty string if no valid label found\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def evaluate_simpletom(\n",
    "    dataset,\n",
    "    backend: ModelBackend,\n",
    "    max_examples: int,\n",
    "    desc: str\n",
    ") -> List[EvalResult]:\n",
    "    \"\"\"Evaluate model on SimpleToM dataset.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i in tqdm(range(min(len(dataset), max_examples)), desc=desc):\n",
    "        example = dataset[i]\n",
    "        prompt, correct, choices = format_simpletom_prompt(example)\n",
    "        \n",
    "        response, elapsed, in_tok, out_tok = backend.generate(prompt, max_tokens=600)\n",
    "        \n",
    "        extracted = extract_simpletom_answer(response)\n",
    "        is_correct = extracted == correct\n",
    "        \n",
    "        results.append(EvalResult(\n",
    "            idx=i,\n",
    "            prompt=prompt,\n",
    "            correct_answer=correct,\n",
    "            model_response=response,\n",
    "            extracted_answer=extracted,\n",
    "            is_correct=is_correct,\n",
    "            time_sec=elapsed,\n",
    "            input_tokens=in_tok,\n",
    "            output_tokens=out_tok,\n",
    "            metadata={\n",
    "                'scenario': example.get('scenario_name', ''),\n",
    "                'id': example.get('id', '')\n",
    "            }\n",
    "        ))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"âœ“ SimpleToM evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluating SimpleToM with LOCAL (30 examples)\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SimpleToM-MentalState (local): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [05:02<00:00, 10.09s/it]\n",
      "SimpleToM-Behavior (local): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [06:55<00:00, 13.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOCAL SimpleToM RESULTS:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">SimpleToM: Mental State QA - </span>\n",
       "<span style=\"font-style: italic\">            local            </span>\n",
       "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
       "â”‚<span style=\"font-weight: bold\"> Metric           </span>â”‚<span style=\"font-weight: bold\"> Value  </span>â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Accuracy         </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> 100.0% </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Correct / Total  </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> 30/30  </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Total Time       </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> 302.6s </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Avg Time/Example </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> 10.09s </span>â”‚\n",
       "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3mSimpleToM: Mental State QA - \u001b[0m\n",
       "\u001b[3m            local            \u001b[0m\n",
       "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
       "â”‚\u001b[1m \u001b[0m\u001b[1mMetric          \u001b[0m\u001b[1m \u001b[0mâ”‚\u001b[1m \u001b[0m\u001b[1mValue \u001b[0m\u001b[1m \u001b[0mâ”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mAccuracy        \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32m100.0%\u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mCorrect / Total \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32m30/30 \u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mTotal Time      \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32m302.6s\u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mAvg Time/Example\u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32m10.09s\u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">  SimpleToM: Behavior QA -   </span>\n",
       "<span style=\"font-style: italic\">            local            </span>\n",
       "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
       "â”‚<span style=\"font-weight: bold\"> Metric           </span>â”‚<span style=\"font-weight: bold\"> Value  </span>â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Accuracy         </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> 53.3%  </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Correct / Total  </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> 16/30  </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Total Time       </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> 415.9s </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Avg Time/Example </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> 13.86s </span>â”‚\n",
       "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m  SimpleToM: Behavior QA -   \u001b[0m\n",
       "\u001b[3m            local            \u001b[0m\n",
       "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
       "â”‚\u001b[1m \u001b[0m\u001b[1mMetric          \u001b[0m\u001b[1m \u001b[0mâ”‚\u001b[1m \u001b[0m\u001b[1mValue \u001b[0m\u001b[1m \u001b[0mâ”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mAccuracy        \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32m53.3% \u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mCorrect / Total \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32m16/30 \u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mTotal Time      \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32m415.9s\u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mAvg Time/Example\u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32m13.86s\u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Saving results...\n",
      "  ðŸ’¾ Saved JSON: eval_outputs/simpletom_mental_state_local_20260122_225455.json\n",
      "  ðŸ’¾ Saved CSV:  eval_outputs/simpletom_mental_state_local_20260122_225455.csv\n",
      "  ðŸ’¾ Saved JSON: eval_outputs/simpletom_behavior_local_20260122_225455.json\n",
      "  ðŸ’¾ Saved CSV:  eval_outputs/simpletom_behavior_local_20260122_225455.csv\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "SAMPLE RESULTS FOR DEBUGGING - LOCAL\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ“ SimpleToM-MentalState (local): No incorrect examples!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                        âœ“ SimpleToM-MentalState (local) - CORRECT Examples (sample of 30 total)                    </span>\n",
       "                                                                                                                   \n",
       " <span style=\"font-weight: bold\"> #       </span> <span style=\"font-weight: bold\"> Expected               </span> <span style=\"font-weight: bold\"> Extracted              </span> <span style=\"font-weight: bold\"> Response                                             </span>\n",
       " â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
       " <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0       </span> <span style=\"color: #008000; text-decoration-color: #008000\"> B                      </span> <span style=\"color: #008000; text-decoration-color: #008000\"> B                      </span> <span style=\"color: #008080; text-decoration-color: #008080\"> analysisWe need to determine if Mary is likely to be </span>\n",
       " <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span> <span style=\"color: #008000; text-decoration-color: #008000\">                        </span> <span style=\"color: #008000; text-decoration-color: #008000\">                        </span> <span style=\"color: #008080; text-decoration-color: #008080\"> aware that the bag of potato chips has moldy chips in</span>\n",
       " <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span> <span style=\"color: #008000; text-decoration-color: #008000\">                        </span> <span style=\"color: #008000; text-decoration-color: #008000\">                        </span> <span style=\"color: #008080; text-decoration-color: #008080\"> it. The story: \"The bag of potato chips has...       </span>\n",
       "                                                                                                                   \n",
       " <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1       </span> <span style=\"color: #008000; text-decoration-color: #008000\"> B                      </span> <span style=\"color: #008000; text-decoration-color: #008000\"> B                      </span> <span style=\"color: #008080; text-decoration-color: #008080\"> analysisWe need to determine if Mary is likely to be </span>\n",
       " <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span> <span style=\"color: #008000; text-decoration-color: #008000\">                        </span> <span style=\"color: #008000; text-decoration-color: #008000\">                        </span> <span style=\"color: #008080; text-decoration-color: #008080\"> aware that the bag of potato chips has a dead cockroa</span>\n",
       " <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span> <span style=\"color: #008000; text-decoration-color: #008000\">                        </span> <span style=\"color: #008000; text-decoration-color: #008000\">                        </span> <span style=\"color: #008080; text-decoration-color: #008080\"> in it. The story: \"The bag of potato chip...         </span>\n",
       "                                                                                                                   \n",
       " <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2       </span> <span style=\"color: #008000; text-decoration-color: #008000\"> A                      </span> <span style=\"color: #008000; text-decoration-color: #008000\"> A                      </span> <span style=\"color: #008080; text-decoration-color: #008080\"> analysisWe need to answer whether Mary is likely to b</span>\n",
       " <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span> <span style=\"color: #008000; text-decoration-color: #008000\">                        </span> <span style=\"color: #008000; text-decoration-color: #008000\">                        </span> <span style=\"color: #008080; text-decoration-color: #008080\"> aware that the bag contains fried rats. The story: \"T</span>\n",
       " <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span> <span style=\"color: #008000; text-decoration-color: #008000\">                        </span> <span style=\"color: #008000; text-decoration-color: #008000\">                        </span> <span style=\"color: #008080; text-decoration-color: #008080\"> sealed bag of potato chips has several ...           </span>\n",
       "                                                                                                                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                        âœ“ SimpleToM-MentalState (local) - CORRECT Examples (sample of 30 total)                    \u001b[0m\n",
       "                                                                                                                   \n",
       " \u001b[1m \u001b[0m\u001b[1m#      \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mExpected              \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mExtracted             \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mResponse                                             \u001b[0m\n",
       " â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
       " \u001b[2m \u001b[0m\u001b[2m0      \u001b[0m\u001b[2m \u001b[0m \u001b[32m \u001b[0m\u001b[32mB                     \u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32mB                     \u001b[0m\u001b[32m \u001b[0m \u001b[36m \u001b[0m\u001b[36manalysisWe need to determine if Mary is likely to be \u001b[0m\n",
       " \u001b[2m         \u001b[0m \u001b[32m                        \u001b[0m \u001b[32m                        \u001b[0m \u001b[36m \u001b[0m\u001b[36maware that the bag of potato chips has moldy chips in\u001b[0m\n",
       " \u001b[2m         \u001b[0m \u001b[32m                        \u001b[0m \u001b[32m                        \u001b[0m \u001b[36m \u001b[0m\u001b[36mit. The story: \"The bag of potato chips has...       \u001b[0m\n",
       "                                                                                                                   \n",
       " \u001b[2m \u001b[0m\u001b[2m1      \u001b[0m\u001b[2m \u001b[0m \u001b[32m \u001b[0m\u001b[32mB                     \u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32mB                     \u001b[0m\u001b[32m \u001b[0m \u001b[36m \u001b[0m\u001b[36manalysisWe need to determine if Mary is likely to be \u001b[0m\n",
       " \u001b[2m         \u001b[0m \u001b[32m                        \u001b[0m \u001b[32m                        \u001b[0m \u001b[36m \u001b[0m\u001b[36maware that the bag of potato chips has a dead cockroa\u001b[0m\n",
       " \u001b[2m         \u001b[0m \u001b[32m                        \u001b[0m \u001b[32m                        \u001b[0m \u001b[36m \u001b[0m\u001b[36min it. The story: \"The bag of potato chip...         \u001b[0m\n",
       "                                                                                                                   \n",
       " \u001b[2m \u001b[0m\u001b[2m2      \u001b[0m\u001b[2m \u001b[0m \u001b[32m \u001b[0m\u001b[32mA                     \u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32mA                     \u001b[0m\u001b[32m \u001b[0m \u001b[36m \u001b[0m\u001b[36manalysisWe need to answer whether Mary is likely to b\u001b[0m\n",
       " \u001b[2m         \u001b[0m \u001b[32m                        \u001b[0m \u001b[32m                        \u001b[0m \u001b[36m \u001b[0m\u001b[36maware that the bag contains fried rats. The story: \"T\u001b[0m\n",
       " \u001b[2m         \u001b[0m \u001b[32m                        \u001b[0m \u001b[32m                        \u001b[0m \u001b[36m \u001b[0m\u001b[36msealed bag of potato chips has several ...           \u001b[0m\n",
       "                                                                                                                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ SimpleToM-MentalState (local): No failures to analyze!\n"
     ]
    }
   ],
   "source": [
    "# Run SimpleToM evaluation\n",
    "N_SIMPLETOM = 100\n",
    "\n",
    "all_simpletom_results = {}  # model_name -> {mental_state: [...], behavior: [...]}\n",
    "\n",
    "if simpletom_mental is not None:\n",
    "    for model_name, backend in MODELS_TO_EVALUATE.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Evaluating SimpleToM with {model_name.upper()} ({N_SIMPLETOM} examples)\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        all_simpletom_results[model_name] = {}\n",
    "        \n",
    "        # Mental-state QA (core ToM: \"Is X aware that...?\")\n",
    "        all_simpletom_results[model_name]['mental_state'] = evaluate_simpletom(\n",
    "            simpletom_mental, \n",
    "            backend,\n",
    "            N_SIMPLETOM,\n",
    "            f\"SimpleToM-MentalState ({model_name})\"\n",
    "        )\n",
    "        \n",
    "        # Behavior QA (applied ToM: \"What will X do next?\")\n",
    "        if simpletom_behavior is not None:\n",
    "            all_simpletom_results[model_name]['behavior'] = evaluate_simpletom(\n",
    "                simpletom_behavior,\n",
    "                backend,\n",
    "                N_SIMPLETOM,\n",
    "                f\"SimpleToM-Behavior ({model_name})\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n{model_name.upper()} SimpleToM RESULTS:\")\n",
    "        show_results_summary(all_simpletom_results[model_name].get('mental_state', []), f\"SimpleToM: Mental State QA - {model_name}\")\n",
    "        if 'behavior' in all_simpletom_results[model_name]:\n",
    "            print()\n",
    "            show_results_summary(all_simpletom_results[model_name]['behavior'], f\"SimpleToM: Behavior QA - {model_name}\")\n",
    "        \n",
    "        # Save results to files\n",
    "        print(f\"\\nðŸ“ Saving results...\")\n",
    "        save_results(all_simpletom_results[model_name].get('mental_state', []), \"simpletom_mental_state\", model_name)\n",
    "        if 'behavior' in all_simpletom_results[model_name]:\n",
    "            save_results(all_simpletom_results[model_name]['behavior'], \"simpletom_behavior\", model_name)\n",
    "        \n",
    "        # Show sample results for debugging\n",
    "        print(f\"\\n{'â”€'*60}\")\n",
    "        print(f\"SAMPLE RESULTS FOR DEBUGGING - {model_name.upper()}\")\n",
    "        print(f\"{'â”€'*60}\")\n",
    "        show_sample_results(all_simpletom_results[model_name].get('mental_state', []), f\"SimpleToM-MentalState ({model_name})\")\n",
    "        analyze_failure_patterns(all_simpletom_results[model_name].get('mental_state', []), f\"SimpleToM-MentalState ({model_name})\")\n",
    "        \n",
    "        if 'behavior' in all_simpletom_results[model_name]:\n",
    "            print()\n",
    "            show_sample_results(all_simpletom_results[model_name]['behavior'], f\"SimpleToM-Behavior ({model_name})\")\n",
    "            analyze_failure_patterns(all_simpletom_results[model_name]['behavior'], f\"SimpleToM-Behavior ({model_name})\")\n",
    "else:\n",
    "    print(\"â­ Skipping SimpleToM (could not load dataset)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. ToMBench Evaluation <a name=\"7-tombench\"></a>\n",
    "\n",
    "**ToMBench** (Chen et al., 2024) is a comprehensive bilingual ToM benchmark with 2,860 testing samples.\n",
    "\n",
    "### 8 ToM Tasks:\n",
    "1. **Unexpected Outcome Test**: Infer mental states when expected â‰  actual emotions\n",
    "2. **Scalar Implicature Task**: \"Some\" implies \"not all\"\n",
    "3. **Persuasion Story Task**: Choose effective persuasion strategies\n",
    "4. **False Belief Task**: Distinguish own beliefs from others' false beliefs\n",
    "5. **Ambiguous Story Task**: Understand mental states in uncertain situations\n",
    "6. **Hinting Test**: Infer mental states from indirect hints\n",
    "7. **Strange Story Task**: Complex social communications (lies, irony, jokes)\n",
    "8. **Faux-pas Recognition Test**: Recognize social faux pas\n",
    "\n",
    "### 6 ATOMS Ability Categories (31 specific abilities):\n",
    "- Emotion, Desire, Intention, Knowledge, Belief, Non-Literal Communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToMBench setup\n",
    "TOMBENCH_DIR = Path('tombench')\n",
    "\n",
    "# Clone if not present\n",
    "if not TOMBENCH_DIR.exists() or not (TOMBENCH_DIR / 'data').exists():\n",
    "    print(\"Downloading ToMBench benchmark...\")\n",
    "    !rm -rf {TOMBENCH_DIR}\n",
    "    !git clone --depth 1 https://github.com/zhchen18/ToMBench.git {TOMBENCH_DIR}\n",
    "    print(\"âœ“ ToMBench downloaded\")\n",
    "else:\n",
    "    print(f\"âœ“ ToMBench already exists at {TOMBENCH_DIR}\")\n",
    "\n",
    "# List available task files\n",
    "if (TOMBENCH_DIR / 'data').exists():\n",
    "    print(\"\\nAvailable ToMBench tasks:\")\n",
    "    for f in sorted((TOMBENCH_DIR / 'data').glob('*.jsonl')):\n",
    "        print(f\"  - {f.stem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToMBenchDataset:\n",
    "    \"\"\"\n",
    "    Dataset loader for ToMBench benchmark.\n",
    "    \n",
    "    ToMBench has multiple JSONL files, one per task type.\n",
    "    Each item has bilingual content (Chinese + English).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: str, tasks: List[str] = None, size_per_task: int = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir: Path to tombench/data directory\n",
    "            tasks: List of task names to load (None = all tasks)\n",
    "            size_per_task: Max items per task (None = all)\n",
    "        \"\"\"\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.data = []\n",
    "        self.task_counts = {}\n",
    "        \n",
    "        # Get all task files\n",
    "        task_files = sorted(self.data_dir.glob('*.jsonl'))\n",
    "        \n",
    "        for task_file in task_files:\n",
    "            task_name = task_file.stem\n",
    "            \n",
    "            # Filter by task list if provided\n",
    "            if tasks and task_name not in tasks:\n",
    "                continue\n",
    "            \n",
    "            count = 0\n",
    "            with open(task_file, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    if size_per_task and count >= size_per_task:\n",
    "                        break\n",
    "                    item = json.loads(line)\n",
    "                    item['_task'] = task_name  # Add task name for tracking\n",
    "                    self.data.append(item)\n",
    "                    count += 1\n",
    "            \n",
    "            self.task_counts[task_name] = count\n",
    "        \n",
    "        self.size = len(self.data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"ToMBenchDataset(n={self.size}, tasks={len(self.task_counts)})\"\n",
    "\n",
    "\n",
    "# Improved prompt - more forceful about format, prevents common failure modes\n",
    "TOMBENCH_SYSTEM_PROMPT = \"\"\"You will read a story and answer a multiple choice question.\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "- Output ONLY a single letter: A, B, C, or D\n",
    "- Do NOT output the option text\n",
    "- Do NOT explain your reasoning\n",
    "- Do NOT output placeholder text like [...] or ...\n",
    "- Your entire response must be exactly one character\n",
    "\n",
    "Example correct response: B\"\"\"\n",
    "\n",
    "\n",
    "def format_tombench_prompt(item: dict) -> Tuple[str, str, List[str]]:\n",
    "    \"\"\"\n",
    "    Format a ToMBench item into a prompt.\n",
    "    \n",
    "    Returns: (prompt, correct_answer_letter, choices)\n",
    "    \"\"\"\n",
    "    # Use English fields\n",
    "    story = item.get('STORY', '')\n",
    "    question = item.get('QUESTION', '')\n",
    "    \n",
    "    # Get options\n",
    "    options = []\n",
    "    for letter in ['A', 'B', 'C', 'D']:\n",
    "        opt = item.get(f'OPTION-{letter}', '')\n",
    "        if opt:\n",
    "            options.append((letter, opt))\n",
    "    \n",
    "    # Get answer - handle the bilingual key\n",
    "    answer_key = 'ç­”æ¡ˆ\\nANSWER'\n",
    "    correct = item.get(answer_key, item.get('ANSWER', ''))\n",
    "    \n",
    "    if not story or not question or not options:\n",
    "        return None, None, None\n",
    "    \n",
    "    prompt = f\"{TOMBENCH_SYSTEM_PROMPT}\\n\\nStory: {story}\\n\\nQuestion: {question}\\n\\n\"\n",
    "    for letter, text in options:\n",
    "        prompt += f\"{letter}. {text}\\n\"\n",
    "    prompt += \"\\nYour answer (single letter only):\"\n",
    "    \n",
    "    return prompt, correct, [opt for _, opt in options]\n",
    "\n",
    "\n",
    "def extract_tombench_answer(response: str) -> str:\n",
    "    \"\"\"Extract A, B, C, or D from response.\"\"\"\n",
    "    # Clean common noise patterns first\n",
    "    response_clean = response.strip()\n",
    "    \n",
    "    # Remove common placeholder patterns\n",
    "    for noise in ['[??]', '[blank]', '[...]', '...?', '...', '**', '??']:\n",
    "        response_clean = response_clean.replace(noise, '').strip()\n",
    "    \n",
    "    response_upper = response_clean.upper()\n",
    "    \n",
    "    # Check for standalone letter at start (most reliable)\n",
    "    if response_upper and response_upper[0] in 'ABCD':\n",
    "        # Make sure it's not part of a word\n",
    "        if len(response_upper) == 1 or not response_upper[1].isalpha():\n",
    "            return response_upper[0]\n",
    "    \n",
    "    # Check for pattern like \"(A)\" or \"Answer: A\"\n",
    "    import re\n",
    "    patterns = [\n",
    "        r'^\\s*([ABCD])\\s*[.\\):\\s]',  # Letter at start with punctuation\n",
    "        r'\\(([ABCD])\\)',              # (A), (B), etc.\n",
    "        r'ANSWER[:\\s]+([ABCD])',      # \"Answer: A\"\n",
    "        r'IS\\s+([ABCD])\\b',           # \"is A\"\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, response_upper)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    \n",
    "    # Last resort: first A/B/C/D found\n",
    "    for char in response_upper:\n",
    "        if char in 'ABCD':\n",
    "            return char\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def evaluate_tombench(\n",
    "    dataset: ToMBenchDataset,\n",
    "    backend: ModelBackend,\n",
    "    max_examples: int = None,\n",
    "    desc: str = \"ToMBench\"\n",
    ") -> List[EvalResult]:\n",
    "    \"\"\"Evaluate model on ToMBench dataset.\"\"\"\n",
    "    results = []\n",
    "    n = min(len(dataset), max_examples) if max_examples else len(dataset)\n",
    "    \n",
    "    for i in tqdm(range(n), desc=desc):\n",
    "        item = dataset[i]\n",
    "        prompt, correct, choices = format_tombench_prompt(item)\n",
    "        \n",
    "        if prompt is None:\n",
    "            continue\n",
    "        \n",
    "        response, elapsed, in_tok, out_tok = backend.generate(prompt, max_tokens=50)\n",
    "        \n",
    "        extracted = extract_tombench_answer(response)\n",
    "        is_correct = extracted == correct\n",
    "        \n",
    "        results.append(EvalResult(\n",
    "            idx=i,\n",
    "            prompt=prompt,\n",
    "            correct_answer=correct,\n",
    "            model_response=response,\n",
    "            extracted_answer=extracted,\n",
    "            is_correct=is_correct,\n",
    "            time_sec=elapsed,\n",
    "            input_tokens=in_tok,\n",
    "            output_tokens=out_tok,\n",
    "            metadata={\n",
    "                'task': item.get('_task', 'unknown'),\n",
    "                'ability': item.get('èƒ½åŠ›\\nABILITY', item.get('ABILITY', 'unknown'))\n",
    "            }\n",
    "        ))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def analyze_tombench_by_task(results: List[EvalResult]) -> Dict[str, Dict]:\n",
    "    \"\"\"Break down ToMBench results by task type.\"\"\"\n",
    "    task_results = {}\n",
    "    \n",
    "    for r in results:\n",
    "        task = r.metadata.get('task', 'unknown')\n",
    "        if task not in task_results:\n",
    "            task_results[task] = {'correct': 0, 'total': 0}\n",
    "        task_results[task]['total'] += 1\n",
    "        if r.is_correct:\n",
    "            task_results[task]['correct'] += 1\n",
    "    \n",
    "    for task in task_results:\n",
    "        t = task_results[task]\n",
    "        t['accuracy'] = t['correct'] / t['total'] if t['total'] > 0 else 0.0\n",
    "    \n",
    "    return task_results\n",
    "\n",
    "\n",
    "def analyze_tombench_by_ability(results: List[EvalResult]) -> Dict[str, Dict]:\n",
    "    \"\"\"Break down ToMBench results by ATOMS ability category.\"\"\"\n",
    "    ability_results = {}\n",
    "    \n",
    "    for r in results:\n",
    "        ability = r.metadata.get('ability', 'unknown')\n",
    "        if ability not in ability_results:\n",
    "            ability_results[ability] = {'correct': 0, 'total': 0}\n",
    "        ability_results[ability]['total'] += 1\n",
    "        if r.is_correct:\n",
    "            ability_results[ability]['correct'] += 1\n",
    "    \n",
    "    for ability in ability_results:\n",
    "        a = ability_results[ability]\n",
    "        a['accuracy'] = a['correct'] / a['total'] if a['total'] > 0 else 0.0\n",
    "    \n",
    "    return ability_results\n",
    "\n",
    "\n",
    "print(\"âœ“ ToMBench evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ToMBench evaluation\n",
    "N_TOMBENCH_PER_TASK = 5  # Examples per task (20 tasks = ~100 total)\n",
    "\n",
    "all_tombench_results = {}  # model_name -> [EvalResult, ...]\n",
    "\n",
    "tombench_data_dir = TOMBENCH_DIR / 'data'\n",
    "\n",
    "if tombench_data_dir.exists() and list(tombench_data_dir.glob('*.jsonl')):\n",
    "    print(f\"Loading ToMBench from {tombench_data_dir}\")\n",
    "    \n",
    "    # Load dataset (sample from each task for efficiency)\n",
    "    tombench_dataset = ToMBenchDataset(tombench_data_dir, size_per_task=N_TOMBENCH_PER_TASK)\n",
    "    print(f\"Loaded {len(tombench_dataset)} examples from {len(tombench_dataset.task_counts)} tasks\")\n",
    "    print(f\"Tasks: {list(tombench_dataset.task_counts.keys())[:5]}...\")\n",
    "    \n",
    "    for model_name, backend in MODELS_TO_EVALUATE.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Evaluating ToMBench with {model_name.upper()} ({len(tombench_dataset)} examples)\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        all_tombench_results[model_name] = evaluate_tombench(\n",
    "            tombench_dataset, \n",
    "            backend, \n",
    "            max_examples=len(tombench_dataset),\n",
    "            desc=f\"ToMBench ({model_name})\"\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{model_name.upper()} ToMBench RESULTS:\")\n",
    "        show_results_summary(all_tombench_results[model_name], f\"ToMBench Overall - {model_name}\")\n",
    "        \n",
    "        # Show breakdown by task\n",
    "        task_breakdown = analyze_tombench_by_task(all_tombench_results[model_name])\n",
    "        if task_breakdown:\n",
    "            print(f\"\\nðŸ“Š Breakdown by Task ({model_name}):\")\n",
    "            for task, stats in sorted(task_breakdown.items()):\n",
    "                print(f\"   {task}: {stats['accuracy']:.1%} ({stats['correct']}/{stats['total']})\")\n",
    "        \n",
    "        # Save results to files\n",
    "        print(f\"\\nðŸ“ Saving results...\")\n",
    "        save_results(all_tombench_results[model_name], \"tombench\", model_name)\n",
    "        \n",
    "        # Show sample results for debugging\n",
    "        print(f\"\\n{'â”€'*60}\")\n",
    "        print(f\"SAMPLE RESULTS FOR DEBUGGING - {model_name.upper()}\")\n",
    "        print(f\"{'â”€'*60}\")\n",
    "        show_sample_results(all_tombench_results[model_name], f\"ToMBench ({model_name})\")\n",
    "        analyze_failure_patterns(all_tombench_results[model_name], f\"ToMBench ({model_name})\")\n",
    "else:\n",
    "    print(\"â­ Skipping ToMBench (data not found)\")\n",
    "    print(\"  Run the download cell above, or manually clone:\")\n",
    "    print(\"  git clone https://github.com/zhchen18/ToMBench.git tombench\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Cross-Benchmark Comparison <a name=\"8-comparison\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results for cross-model comparison\n",
    "comparison_data = []\n",
    "\n",
    "for model_name in MODELS_TO_EVALUATE.keys():\n",
    "    # ToMi\n",
    "    if model_name in all_tomi_results:\n",
    "        if all_tomi_results[model_name].get('tom'):\n",
    "            comparison_data.append((model_name, 'ToMi', 'ToM (false belief)', \n",
    "                                    compute_accuracy(all_tomi_results[model_name]['tom']), \n",
    "                                    len(all_tomi_results[model_name]['tom'])))\n",
    "        if all_tomi_results[model_name].get('no_tom'):\n",
    "            comparison_data.append((model_name, 'ToMi', 'No-ToM (true belief)', \n",
    "                                    compute_accuracy(all_tomi_results[model_name]['no_tom']), \n",
    "                                    len(all_tomi_results[model_name]['no_tom'])))\n",
    "    \n",
    "    # FANToM\n",
    "    if model_name in all_fantom_results:\n",
    "        if all_fantom_results[model_name].get('belief'):\n",
    "            comparison_data.append((model_name, 'FANToM', 'Belief Questions', \n",
    "                                    compute_accuracy(all_fantom_results[model_name]['belief']), \n",
    "                                    len(all_fantom_results[model_name]['belief'])))\n",
    "        if all_fantom_results[model_name].get('fact'):\n",
    "            comparison_data.append((model_name, 'FANToM', 'Fact Questions', \n",
    "                                    compute_accuracy(all_fantom_results[model_name]['fact']), \n",
    "                                    len(all_fantom_results[model_name]['fact'])))\n",
    "    \n",
    "    # SimpleToM (updated field names)\n",
    "    if model_name in all_simpletom_results:\n",
    "        if all_simpletom_results[model_name].get('mental_state'):\n",
    "            comparison_data.append((model_name, 'SimpleToM', 'Mental State QA', \n",
    "                                    compute_accuracy(all_simpletom_results[model_name]['mental_state']), \n",
    "                                    len(all_simpletom_results[model_name]['mental_state'])))\n",
    "        if all_simpletom_results[model_name].get('behavior'):\n",
    "            comparison_data.append((model_name, 'SimpleToM', 'Behavior QA', \n",
    "                                    compute_accuracy(all_simpletom_results[model_name]['behavior']), \n",
    "                                    len(all_simpletom_results[model_name]['behavior'])))\n",
    "    \n",
    "    # ToMBench\n",
    "    if model_name in all_tombench_results:\n",
    "        comparison_data.append((model_name, 'ToMBench', 'Overall', \n",
    "                                compute_accuracy(all_tombench_results[model_name]), \n",
    "                                len(all_tombench_results[model_name])))\n",
    "\n",
    "# Display comparison table\n",
    "if comparison_data:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CROSS-BENCHMARK COMPARISON: ALL MODELS\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    table = Table(title=\"ToM Benchmark Results\", box=box.ROUNDED)\n",
    "    table.add_column(\"Model\", style=\"magenta\")\n",
    "    table.add_column(\"Benchmark\", style=\"cyan\")\n",
    "    table.add_column(\"Condition\", style=\"white\")\n",
    "    table.add_column(\"Accuracy\", style=\"green\")\n",
    "    table.add_column(\"N\", style=\"dim\")\n",
    "    \n",
    "    for model, benchmark, condition, acc, n in comparison_data:\n",
    "        table.add_row(model, benchmark, condition, f\"{acc:.1%}\", str(n))\n",
    "    \n",
    "    console.print(table)\n",
    "    \n",
    "    # Model comparison summary\n",
    "    print(\"\\nðŸ“Š Key Observations:\")\n",
    "    print(\"   - Claude serves as positive control (expected: high accuracy)\")\n",
    "    print(\"   - ToM conditions should show lower accuracy than control conditions\")\n",
    "    print(\"   - Gap between models indicates relative ToM capability\")\n",
    "    print(\"   - Consistent patterns across benchmarks suggest robust ToM deficit/capability\")\n",
    "else:\n",
    "    print(\"No results to compare - run evaluations first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create side-by-side comparison table for easier analysis\n",
    "if len(MODELS_TO_EVALUATE) > 1 and comparison_data:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL HEAD-TO-HEAD COMPARISON\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Group by benchmark/condition\n",
    "    from collections import defaultdict\n",
    "    grouped = defaultdict(dict)\n",
    "    for model, benchmark, condition, acc, n in comparison_data:\n",
    "        key = f\"{benchmark}: {condition}\"\n",
    "        grouped[key][model] = (acc, n)\n",
    "    \n",
    "    # Display\n",
    "    table = Table(title=\"Head-to-Head Accuracy\", box=box.ROUNDED)\n",
    "    table.add_column(\"Benchmark / Condition\", style=\"cyan\")\n",
    "    for model_name in MODELS_TO_EVALUATE.keys():\n",
    "        table.add_column(model_name.upper(), style=\"green\")\n",
    "    table.add_column(\"Î”\", style=\"yellow\")  # Difference if 2 models\n",
    "    \n",
    "    for key in sorted(grouped.keys()):\n",
    "        row = [key]\n",
    "        accs = []\n",
    "        for model_name in MODELS_TO_EVALUATE.keys():\n",
    "            if model_name in grouped[key]:\n",
    "                acc, n = grouped[key][model_name]\n",
    "                row.append(f\"{acc:.1%} (n={n})\")\n",
    "                accs.append(acc)\n",
    "            else:\n",
    "                row.append(\"â€”\")\n",
    "                accs.append(None)\n",
    "        \n",
    "        # Calculate difference if exactly 2 models\n",
    "        if len(accs) == 2 and all(a is not None for a in accs):\n",
    "            diff = accs[1] - accs[0]  # claude - local typically\n",
    "            row.append(f\"{diff:+.1%}\")\n",
    "        else:\n",
    "            row.append(\"â€”\")\n",
    "        \n",
    "        table.add_row(*row)\n",
    "    \n",
    "    console.print(table)\n",
    "elif comparison_data:\n",
    "    print(\"(Head-to-head comparison requires 2+ models)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Save Results <a name=\"9-save\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results for export\n",
    "all_results = {\n",
    "    'models': list(MODELS_TO_EVALUATE.keys()),\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'benchmarks': {}\n",
    "}\n",
    "\n",
    "for model_name in MODELS_TO_EVALUATE.keys():\n",
    "    all_results['benchmarks'][model_name] = {}\n",
    "    \n",
    "    # ToMi\n",
    "    if model_name in all_tomi_results:\n",
    "        all_results['benchmarks'][model_name]['tomi'] = {\n",
    "            'tom': {\n",
    "                'accuracy': compute_accuracy(all_tomi_results[model_name].get('tom', [])),\n",
    "                'n': len(all_tomi_results[model_name].get('tom', [])),\n",
    "                'results': [r.to_dict() for r in all_tomi_results[model_name].get('tom', [])]\n",
    "            },\n",
    "            'no_tom': {\n",
    "                'accuracy': compute_accuracy(all_tomi_results[model_name].get('no_tom', [])),\n",
    "                'n': len(all_tomi_results[model_name].get('no_tom', [])),\n",
    "                'results': [r.to_dict() for r in all_tomi_results[model_name].get('no_tom', [])]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # FANToM\n",
    "    if model_name in all_fantom_results:\n",
    "        all_results['benchmarks'][model_name]['fantom'] = {\n",
    "            'belief': {\n",
    "                'accuracy': compute_accuracy(all_fantom_results[model_name].get('belief', [])),\n",
    "                'n': len(all_fantom_results[model_name].get('belief', [])),\n",
    "                'results': [r.to_dict() for r in all_fantom_results[model_name].get('belief', [])]\n",
    "            },\n",
    "            'fact': {\n",
    "                'accuracy': compute_accuracy(all_fantom_results[model_name].get('fact', [])),\n",
    "                'n': len(all_fantom_results[model_name].get('fact', [])),\n",
    "                'results': [r.to_dict() for r in all_fantom_results[model_name].get('fact', [])]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # SimpleToM (updated field names)\n",
    "    if model_name in all_simpletom_results:\n",
    "        all_results['benchmarks'][model_name]['simpletom'] = {\n",
    "            'mental_state': {\n",
    "                'accuracy': compute_accuracy(all_simpletom_results[model_name].get('mental_state', [])),\n",
    "                'n': len(all_simpletom_results[model_name].get('mental_state', [])),\n",
    "                'results': [r.to_dict() for r in all_simpletom_results[model_name].get('mental_state', [])]\n",
    "            }\n",
    "        }\n",
    "        if 'behavior' in all_simpletom_results[model_name]:\n",
    "            all_results['benchmarks'][model_name]['simpletom']['behavior'] = {\n",
    "                'accuracy': compute_accuracy(all_simpletom_results[model_name].get('behavior', [])),\n",
    "                'n': len(all_simpletom_results[model_name].get('behavior', [])),\n",
    "                'results': [r.to_dict() for r in all_simpletom_results[model_name].get('behavior', [])]\n",
    "            }\n",
    "    \n",
    "    # ToMBench\n",
    "    if model_name in all_tombench_results:\n",
    "        results = all_tombench_results[model_name]\n",
    "        task_breakdown = analyze_tombench_by_task(results)\n",
    "        ability_breakdown = analyze_tombench_by_ability(results)\n",
    "        \n",
    "        all_results['benchmarks'][model_name]['tombench'] = {\n",
    "            'overall': {\n",
    "                'accuracy': compute_accuracy(results),\n",
    "                'n': len(results),\n",
    "            },\n",
    "            'by_task': task_breakdown,\n",
    "            'by_ability': ability_breakdown,\n",
    "            'results': [r.to_dict() for r in results]\n",
    "        }\n",
    "\n",
    "# Save\n",
    "outfile = f\"tom_benchmark_results_{datetime.now().strftime('%Y%m%d_%H%M')}.json\"\n",
    "with open(outfile, 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Results saved to {outfile}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Notes\n",
    "\n",
    "### Expected Patterns\n",
    "- **ToMi**: No-ToM accuracy should be higher than ToM accuracy\n",
    "- **FANToM**: Fact accuracy should be higher than Belief accuracy\n",
    "- **SimpleToM**: Applied ToM may be harder than Explicit ToM\n",
    "- **ToMBench**: Claude should significantly outperform smaller models across all 8 tasks\n",
    "\n",
    "### Positive Control Interpretation\n",
    "- Claude Opus 4.5 establishes an upper bound for expected performance\n",
    "- Large gaps between Claude and target model indicate areas for improvement\n",
    "- If Claude also struggles on specific tasks, those may be genuinely difficult ToM problems\n",
    "\n",
    "### Next Steps for Function Vector Research\n",
    "1. Use ToMi ToM/No-ToM pairs to extract function vectors\n",
    "2. Test steering on all four benchmarks\n",
    "3. Check if ToM function vectors generalize across benchmarks\n",
    "4. Compare first-order vs second-order ToM vectors\n",
    "5. Use ToMBench's ATOMS ability breakdown to identify specific ToM components\n",
    "\n",
    "### References\n",
    "- ToMi: Le et al. (2019) - github.com/facebookresearch/ToMi\n",
    "- FANToM: Kim et al. (2023) - github.com/skywalker023/fantom\n",
    "- SimpleToM: Gu et al. (2024) - huggingface.co/datasets/allenai/SimpleToM\n",
    "- ToMBench: Chen et al. (2024) - github.com/zhchen18/ToMBench - arXiv:2402.15052"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
