{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Candidate ToM Benchmarks\n",
    "\n",
    "This notebook explores several Theory of Mind (ToM) benchmarks for evaluating language models' ability to reason about mental states, beliefs, and intentions of agents.\n",
    "\n",
    "**Benchmarks covered:**\n",
    "- **ToMi** - Theory of Mind Inventory with first/second-order belief tracking\n",
    "- **FANToM** - A benchmark for stress-testing machine ToM in conversations\n",
    "- **SimpleToM** - Allen AI's simplified ToM evaluation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install required dependencies for loading and evaluating datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install evaluate datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ToMi (Theory of Mind Inventory)\n",
    "\n",
    "ToMi is a benchmark from Facebook Research that tests models on false-belief understanding through story-based question answering.\n",
    "\n",
    "**Key features:**\n",
    "- **First-order beliefs**: \"Where does X think the object is?\"\n",
    "- **Second-order beliefs**: \"Where does X think Y thinks the object is?\"\n",
    "- **ToM vs No-ToM**: Questions that require mental state reasoning vs. simple memory retrieval\n",
    "\n",
    "**Paper:** [ToMi: A Test of Theory of Mind in Language Models](https://arxiv.org/abs/1909.01871)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install ToMi and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/facebookresearch/ToMi.git tomi\n",
    "! rm -rf tomi/.git\n",
    "! pip install tqdm pandas matplotlib seaborn numpy jupyterlab openpyxl transformers datasets evaluate wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip tomi/tomi_balanced_story_types.zip -d tomi/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Question-Answer Pairs\n",
    "\n",
    "The extractor script processes the raw story traces into structured JSONL files, organized by:\n",
    "- Order (first-order vs second-order belief questions)\n",
    "- Story type (0 or 1)\n",
    "- ToM requirement (whether mental state reasoning is needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python tomi/tomi_pair_extractor.py --data_dir ./tomi/tomi_balanced_story_types --output_dir ./tomi/tomi_pairs --split test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Data for Model Inference\n",
    "\n",
    "Convert the extracted JSONL files into a prompt format suitable for LLM evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def format_for_inference(jsonl_path: str, output_path: str = None):\n",
    "    \"\"\"Convert ToMi JSONL to inference-ready format.\n",
    "    \n",
    "    Args:\n",
    "        jsonl_path: Path to input JSONL file with story/question pairs\n",
    "        output_path: Optional output path (defaults to input path with '_prompts' suffix)\n",
    "    \n",
    "    Returns:\n",
    "        List of formatted examples with prompts and metadata\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    with open(jsonl_path) as f:\n",
    "        for line in f:\n",
    "            ex = json.loads(line)\n",
    "            \n",
    "            # Clean up story (remove line numbers)\n",
    "            story_lines = ex['story'].split('\\n')\n",
    "            clean_story = '\\n'.join(\n",
    "                line.split(' ', 1)[1] if line[0].isdigit() else line \n",
    "                for line in story_lines\n",
    "            )\n",
    "            \n",
    "            # Format prompt\n",
    "            prompt = f\"\"\"Story:\n",
    "{clean_story}\n",
    "\n",
    "Question: {ex['question']}\n",
    "Answer:\"\"\"\n",
    "            \n",
    "            examples.append({\n",
    "                'prompt': prompt,\n",
    "                'answer': ex['answer'],\n",
    "                'question_type': ex['question_type'],\n",
    "                'story_type': ex['story_type'],\n",
    "                'requires_tom': ex['requires_tom'],\n",
    "            })\n",
    "    \n",
    "    # Save\n",
    "    out_path = output_path or jsonl_path.replace('.jsonl', '_prompts.jsonl')\n",
    "    with open(out_path, 'w') as f:\n",
    "        for ex in examples:\n",
    "            f.write(json.dumps(ex) + '\\n')\n",
    "    \n",
    "    print(f\"Saved {len(examples)} prompts to {out_path}\")\n",
    "    return examples\n",
    "\n",
    "\n",
    "def evaluate_response(response: str, correct_answer: str) -> bool:\n",
    "    \"\"\"Check if model response contains the correct answer.\n",
    "    \n",
    "    Uses case-insensitive substring matching.\n",
    "    \"\"\"\n",
    "    response_lower = response.lower().strip()\n",
    "    answer_lower = correct_answer.lower()\n",
    "    return answer_lower in response_lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Prompt Files\n",
    "\n",
    "Process all ToMi data splits into inference-ready prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First-order belief questions\n",
    "format_for_inference('tomi/tomi_pairs/first_order_0_no_tom.jsonl')\n",
    "format_for_inference('tomi/tomi_pairs/first_order_1_no_tom.jsonl')\n",
    "format_for_inference('tomi/tomi_pairs/first_order_1_tom.jsonl')\n",
    "\n",
    "# Second-order belief questions\n",
    "format_for_inference('tomi/tomi_pairs/second_order_0_tom.jsonl')\n",
    "format_for_inference('tomi/tomi_pairs/second_order_0_no_tom.jsonl')\n",
    "format_for_inference('tomi/tomi_pairs/second_order_1_tom.jsonl')\n",
    "format_for_inference('tomi/tomi_pairs/second_order_1_no_tom.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## FANToM\n",
    "\n",
    "FANToM (FAct-tracking in NaTural cOnversations with Models) is a benchmark that tests ToM capabilities through naturalistic multi-party conversations.\n",
    "\n",
    "**Key features:**\n",
    "- Realistic conversational scenarios with multiple speakers\n",
    "- Information asymmetry between conversation participants\n",
    "- Tests whether models can track who knows what\n",
    "\n",
    "**Paper:** [FANToM: A Benchmark for Stress-Testing Machine Theory of Mind in Interactions](https://arxiv.org/abs/2310.15421)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Run FANToM Evaluation\n",
    "\n",
    "FANToM requires a separate conda environment due to specific dependency requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda env create -f fantom/environment.yml\n",
    "! conda activate fantom\n",
    "! python fantom/eval_fantom.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SimpleToM\n",
    "\n",
    "SimpleToM from Allen AI provides a clean, focused evaluation of ToM capabilities across three question types.\n",
    "\n",
    "**Question types:**\n",
    "- **Mental-state QA**: Questions about characters' beliefs and knowledge\n",
    "- **Behavior QA**: Predicting actions based on mental states\n",
    "- **Judgment QA**: Evaluating decisions given information asymmetry\n",
    "\n",
    "**Paper:** [SimpleToM: Exposing the Gap between Explicit ToM Inference and Implicit ToM Application in LLMs](https://arxiv.org/abs/2410.13648)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SimpleToM Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tom_mental = load_dataset(\"allenai/SimpleToM\", 'mental-state-qa')\n",
    "simple_tom_behavior = load_dataset(\"allenai/SimpleToM\", 'behavior-qa')\n",
    "simple_tom_judgment = load_dataset(\"allenai/SimpleToM\", 'judgment-qa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mental-state QA:\", simple_tom_mental)\n",
    "print(\"\\nBehavior QA:\", simple_tom_behavior)\n",
    "print(\"\\nJudgment QA:\", simple_tom_judgment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
