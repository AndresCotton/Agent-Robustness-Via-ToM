{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Candidate ToM Benchmarks\n",
    "\n",
    "This notebook explores several Theory of Mind (ToM) benchmarks for evaluating language models' ability to reason about mental states, beliefs, and intentions of agents.\n",
    "\n",
    "**Benchmarks covered:**\n",
    "- **ToMi** - Theory of Mind Inventory with first/second-order belief tracking\n",
    "- **FANToM** - A benchmark for stress-testing machine ToM in conversations\n",
    "- **SimpleToM** - Allen AI's simplified ToM evaluation dataset\n",
    "- **ToMBench** - Comprehensive bilingual ToM benchmark covering 8 tasks and 31 abilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install required dependencies for loading and evaluating datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install evaluate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /root/ARENA_3.0/capstone/Agent-Robustness-Via-ToM/tom_benchmarks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Ensure working directory is the notebook's directory\n",
    "# This makes relative paths like 'tomi/tomi_pairs/...' work correctly\n",
    "NOTEBOOK_DIR = '/root/ARENA_3.0/capstone/Agent-Robustness-Via-ToM/tom_benchmarks'\n",
    "os.chdir(NOTEBOOK_DIR)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ToMi (Theory of Mind Inventory)\n",
    "\n",
    "ToMi is a benchmark from Facebook Research that tests models on false-belief understanding through story-based question answering.\n",
    "\n",
    "**Key features:**\n",
    "- **First-order beliefs**: \"Where does X think the object is?\"\n",
    "- **Second-order beliefs**: \"Where does X think Y thinks the object is?\"\n",
    "- **ToM vs No-ToM**: Questions that require mental state reasoning vs. simple memory retrieval\n",
    "\n",
    "**Paper:** [ToMi: A Test of Theory of Mind in Language Models](https://arxiv.org/abs/1909.01871)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install ToMi and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/facebookresearch/ToMi.git tomi\n",
    "! rm -rf tomi/.git\n",
    "! pip install tqdm pandas matplotlib seaborn numpy jupyterlab openpyxl transformers datasets evaluate wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip tomi/tomi_balanced_story_types.zip -d tomi/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Question-Answer Pairs\n",
    "\n",
    "The extractor script processes the raw story traces into structured JSONL files, organized by:\n",
    "- Order (first-order vs second-order belief questions)\n",
    "- Story type (0 or 1)\n",
    "- ToM requirement (whether mental state reasoning is needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python tomi/tomi_pair_extractor.py --data_dir ./tomi/tomi_balanced_story_types --output_dir ./tomi/tomi_pairs --split test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Data for Model Inference\n",
    "\n",
    "Convert the extracted JSONL files into a prompt format suitable for LLM evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def format_for_inference(jsonl_path: str, output_path: str = None):\n",
    "    \"\"\"Convert ToMi JSONL to inference-ready format.\n",
    "    \n",
    "    Args:\n",
    "        jsonl_path: Path to input JSONL file with story/question pairs\n",
    "        output_path: Optional output path (defaults to input path with '_prompts' suffix)\n",
    "    \n",
    "    Returns:\n",
    "        List of formatted examples with prompts and metadata\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    with open(jsonl_path) as f:\n",
    "        for line in f:\n",
    "            ex = json.loads(line)\n",
    "            \n",
    "            # Clean up story (remove line numbers)\n",
    "            story_lines = ex['story'].split('\\n')\n",
    "            clean_story = '\\n'.join(\n",
    "                line.split(' ', 1)[1] if line[0].isdigit() else line \n",
    "                for line in story_lines\n",
    "            )\n",
    "            \n",
    "            # Format prompt\n",
    "            prompt = f\"\"\"Story:\n",
    "{clean_story}\n",
    "\n",
    "Question: {ex['question']}\n",
    "Answer:\"\"\"\n",
    "            \n",
    "            examples.append({\n",
    "                'prompt': prompt,\n",
    "                'answer': ex['answer'],\n",
    "                'question_type': ex['question_type'],\n",
    "                'story_type': ex['story_type'],\n",
    "                'requires_tom': ex['requires_tom'],\n",
    "            })\n",
    "    \n",
    "    # Save\n",
    "    out_path = output_path or jsonl_path.replace('.jsonl', '_prompts.jsonl')\n",
    "    with open(out_path, 'w') as f:\n",
    "        for ex in examples:\n",
    "            f.write(json.dumps(ex) + '\\n')\n",
    "    \n",
    "    print(f\"Saved {len(examples)} prompts to {out_path}\")\n",
    "    return examples\n",
    "\n",
    "\n",
    "def evaluate_response(response: str, correct_answer: str) -> bool:\n",
    "    \"\"\"Check if model response contains the correct answer.\n",
    "    \n",
    "    Uses case-insensitive substring matching.\n",
    "    \"\"\"\n",
    "    response_lower = response.lower().strip()\n",
    "    answer_lower = correct_answer.lower()\n",
    "    return answer_lower in response_lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Prompt Files\n",
    "\n",
    "Process all ToMi data splits into inference-ready prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First-order belief questions\n",
    "format_for_inference('tomi/tomi_pairs/first_order_0_no_tom.jsonl')\n",
    "format_for_inference('tomi/tomi_pairs/first_order_1_no_tom.jsonl')\n",
    "format_for_inference('tomi/tomi_pairs/first_order_1_tom.jsonl')\n",
    "\n",
    "# Second-order belief questions\n",
    "format_for_inference('tomi/tomi_pairs/second_order_0_tom.jsonl')\n",
    "format_for_inference('tomi/tomi_pairs/second_order_0_no_tom.jsonl')\n",
    "format_for_inference('tomi/tomi_pairs/second_order_1_tom.jsonl')\n",
    "format_for_inference('tomi/tomi_pairs/second_order_1_no_tom.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data from ToMi\n",
    "\n",
    "View representative examples from the ToMi dataset across different question types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def load_tomi_samples(filepath: str, n: int = 2) -> list[dict]:\n",
    "    \"\"\"Load n samples from a ToMi JSONL file.\"\"\"\n",
    "    samples = []\n",
    "    with open(filepath) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= n:\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            # Clean up story for display\n",
    "            story_lines = data['story'].split('\\n')\n",
    "            clean_story = '\\n'.join(\n",
    "                line.split(' ', 1)[1] if line[0].isdigit() else line\n",
    "                for line in story_lines\n",
    "            )\n",
    "            samples.append({\n",
    "                'Story': clean_story,\n",
    "                'Question': data['question'],\n",
    "                'Answer': data['answer'],\n",
    "                'Story Type': data['story_type'],\n",
    "                'Requires ToM': data['requires_tom']\n",
    "            })\n",
    "    return samples\n",
    "\n",
    "def display_tomi_table(title: str, samples: list[dict]):\n",
    "    \"\"\"Display samples in a formatted table.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"  {title}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    df = pd.DataFrame(samples)\n",
    "    df['Story'] = df['Story'].apply(lambda x: x.replace('\\n', '<br>'))\n",
    "    display(HTML(df.to_html(escape=False, index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  First-Order ToM (False Belief)\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Story</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Story Type</th>\n",
       "      <th>Requires ToM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Isabella entered the den.<br>Olivia entered the den.<br>Isabella dislikes the pumpkin<br>The broccoli is in the blue_pantry.<br>Isabella exited the den.<br>Olivia moved the broccoli to the red_drawer.<br>Abigail entered the garden.<br>Isabella entered the garden.</td>\n",
       "      <td>Where will Isabella look for the broccoli?</td>\n",
       "      <td>blue_pantry</td>\n",
       "      <td>false_belief</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Amelia entered the pantry.<br>Ethan loves the pumpkin<br>Liam entered the pantry.<br>The turnip is in the red_basket.<br>Ethan entered the closet.<br>Liam exited the pantry.<br>Amelia moved the turnip to the blue_pantry.</td>\n",
       "      <td>Where will Liam look for the turnip?</td>\n",
       "      <td>red_basket</td>\n",
       "      <td>false_belief</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  First-Order No-ToM (True Belief)\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Story</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Story Type</th>\n",
       "      <th>Requires ToM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Aria entered the front_yard.<br>Aiden entered the front_yard.<br>The grapefruit is in the green_bucket.<br>Aria moved the grapefruit to the blue_container.<br>Aiden exited the front_yard.<br>Noah entered the playroom.</td>\n",
       "      <td>Where will Aria look for the grapefruit?</td>\n",
       "      <td>blue_container</td>\n",
       "      <td>true_belief</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Olivia entered the closet.<br>Isla dislikes the hall<br>Aria entered the closet.<br>Isla entered the closet.<br>The orange is in the green_drawer.<br>Aria exited the closet.<br>Olivia moved the orange to the blue_crate.<br>Isla dislikes the cucumber<br>Isla exited the closet.<br>Isla entered the hall.</td>\n",
       "      <td>Where will Olivia look for the orange?</td>\n",
       "      <td>blue_crate</td>\n",
       "      <td>true_belief</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  Second-Order ToM\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Story</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Story Type</th>\n",
       "      <th>Requires ToM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Isabella entered the den.<br>Olivia entered the den.<br>Isabella dislikes the pumpkin<br>The broccoli is in the blue_pantry.<br>Isabella exited the den.<br>Olivia moved the broccoli to the red_drawer.<br>Abigail entered the garden.<br>Isabella entered the garden.</td>\n",
       "      <td>Where does Olivia think that Isabella searches for the broccoli?</td>\n",
       "      <td>blue_pantry</td>\n",
       "      <td>false_belief</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Amelia entered the pantry.<br>Ethan loves the pumpkin<br>Liam entered the pantry.<br>The turnip is in the red_basket.<br>Ethan entered the closet.<br>Liam exited the pantry.<br>Amelia moved the turnip to the blue_pantry.</td>\n",
       "      <td>Where does Amelia think that Liam searches for the turnip?</td>\n",
       "      <td>red_basket</td>\n",
       "      <td>false_belief</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First-order ToM (false belief) - requires reasoning about what someone THINKS\n",
    "first_order_tom = load_tomi_samples('tomi/tomi_pairs/first_order_1_tom.jsonl', n=2)\n",
    "display_tomi_table(\"First-Order ToM (False Belief)\", first_order_tom)\n",
    "\n",
    "# First-order No-ToM (true belief) - simple memory/tracking\n",
    "first_order_no_tom = load_tomi_samples('tomi/tomi_pairs/first_order_0_no_tom.jsonl', n=2)\n",
    "display_tomi_table(\"First-Order No-ToM (True Belief)\", first_order_no_tom)\n",
    "\n",
    "# Second-order ToM - \"What does X think Y thinks?\"\n",
    "second_order_tom = load_tomi_samples('tomi/tomi_pairs/second_order_0_tom.jsonl', n=2)\n",
    "display_tomi_table(\"Second-Order ToM\", second_order_tom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToMi Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_jsonl_lines(filepath: str) -> int:\n",
    "    with open(filepath) as f:\n",
    "        return sum(1 for _ in f)\n",
    "\n",
    "tomi_dir = Path('tomi/tomi_pairs')\n",
    "stats = []\n",
    "for jsonl_file in sorted(tomi_dir.glob('*.jsonl')):\n",
    "    if '_prompts' not in jsonl_file.name and jsonl_file.name not in ['all_tom.jsonl', 'all_no_tom.jsonl']:\n",
    "        count = count_jsonl_lines(jsonl_file)\n",
    "        name = jsonl_file.stem\n",
    "        order = 'First' if 'first' in name else 'Second'\n",
    "        requires_tom = 'No' if 'no_tom' in name else 'Yes'\n",
    "        stats.append({\n",
    "            'File': jsonl_file.name,\n",
    "            'Order': order,\n",
    "            'Requires ToM': requires_tom,\n",
    "            'Count': count\n",
    "        })\n",
    "\n",
    "stats_df = pd.DataFrame(stats)\n",
    "print(\"ToMi Dataset Statistics:\\n\")\n",
    "display(stats_df)\n",
    "print(f\"\\nTotal examples: {stats_df['Count'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## FANToM\n",
    "\n",
    "FANToM (FAct-tracking in NaTural cOnversations with Models) is a benchmark that tests ToM capabilities through naturalistic multi-party conversations.\n",
    "\n",
    "**Key features:**\n",
    "- Realistic conversational scenarios with multiple speakers\n",
    "- Information asymmetry between conversation participants\n",
    "- Tests whether models can track who knows what\n",
    "\n",
    "**Paper:** [FANToM: A Benchmark for Stress-Testing Machine Theory of Mind in Interactions](https://arxiv.org/abs/2310.15421)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load FANToM Dataset\n",
    "\n",
    "FANToM data is automatically downloaded from Google Cloud Storage on first use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already built at data/fantom. version 1.0\n",
      "Loaded FANToM dataset with 870 conversation sets\n"
     ]
    }
   ],
   "source": [
    "# Add fantom to path and load the dataset\n",
    "import sys\n",
    "sys.path.insert(0, 'fantom')\n",
    "\n",
    "from task.dataset_loader import load\n",
    "fantom_df = load()\n",
    "print(f\"Loaded FANToM dataset with {len(fantom_df)} conversation sets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data from FANToM\n",
    "\n",
    "View a sample conversation with its associated questions. FANToM tests multiple question types:\n",
    "- **Belief Questions**: What does a character believe about a fact?\n",
    "- **Answerability Questions**: Can a character answer a given question?\n",
    "- **Info Accessibility Questions**: Does a character have access to certain information?\n",
    "- **Fact Questions**: Basic factual recall from the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "  SAMPLE CONVERSATION (Short Context)\n",
      "================================================================================\n",
      "Gianna: Guys, I've really enjoyed sharing our pet stories, but I need to excuse myself. I need to change clothes for a meeting later. Talk to you later!\n",
      "Sara: Sure thing, Gianna. Take care!\n",
      "Javier: Catch you later, Gianna.\n",
      "Sara: So Javier, have you ever tried training Bruno?\n",
      "Javier: Yes, I did actually. It was a challenge at times, but rewarding nevertheless. How about you? Did you try training Snowflake?\n",
      "Sara: Oh gosh, trying to train a cat is a whole different ball game. But I did manage to teach her a few commands and tricks. She was quite an intelligent little furball.\n",
      "Gianna: Hey guys, I'm back, couldn't miss out on more pet stories. Speaking of teaching and training pets, it is amazing how that further strengthens the bond between us and our pets, right?\n",
      "Sara: Absolutely, Gianna! The fact that they trust us enough to learn from us is really special.\n",
      "Javier: I can't agree more. I believe that's one of the ways Bruno conveyed his love and trust towards me. It also gave me a sense of responsibility towards him.\n",
      "Gianna: Just like Chirpy. Once she began to imitate me, we connected in a way I never imagined. She would repeat words that I was studying for exams and that somehow made studying less stressful.\n",
      "Javier: Pets are indeed lifesavers in so many ways.\n",
      "Sara: They bring so much joy and laughter too into our lives. I mean, imagine a little kitten stuck in a vase! I couldn't have asked for a better stress buster during my college days.\n",
      "Gianna: Totally, they all are so amazing in their unique ways. It's so nice to have these memories to look back on.\n",
      "\n",
      "================================================================================\n",
      "  QUESTIONS FOR THIS CONVERSATION\n",
      "================================================================================\n",
      "\n",
      "--- Fact Question ---\n",
      "Q: Who discussed their experiences training their pets, Bruno and Snowflake?\n",
      "A: Sara and Javier discussed their experiences training their pets, Bruno and Snowflake.\n",
      "\n",
      "--- Belief Questions (ToM) ---\n",
      "\n",
      "Q1: What does Gianna believe about who discussed their experiences training their pets, Bruno and Snowflake?\n",
      "Correct: Gianna knows that Javier discussed training his pet, Bruno. However, Gianna will not know training a pet named Snowflake.\n",
      "Wrong (false belief): Gianna believes that Sara and Javier discussed their experiences training their pets, Bruno and Snowflake.\n",
      "ToM Type: first-order\n",
      "\n",
      "Q2: What does Sara believe about Javier's belief regarding who discussed their experiences training their pets, Bruno and Snowflake?\n",
      "Correct: Sara believes that Javier thinks they both discussed their experiences training their pets, Bruno and Snowflake.\n",
      "Wrong (false belief): Sara is unaware about Javier's belief regarding who discussed their experiences training their pets, Bruno and Snowflake.\n",
      "ToM Type: second-order:accessible\n",
      "\n",
      "--- Answerability Questions ---\n",
      "List Q: List all the characters who know the precise correct answer to this question.\n",
      "Can answer: ['Javier', 'Sara']\n",
      "Cannot answer: ['Gianna', 'Alondra', 'Angela']\n",
      "\n",
      "--- Info Accessibility Questions ---\n",
      "List Q: List all the characters who know this information.\n",
      "Has access: ['Javier', 'Sara']\n",
      "No access: ['Gianna', 'Alondra', 'Angela']\n"
     ]
    }
   ],
   "source": [
    "# Display a sample conversation set from FANToM\n",
    "sample_set = fantom_df.iloc[0]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"  SAMPLE CONVERSATION (Short Context)\")\n",
    "print(\"=\"*80)\n",
    "print(sample_set['short_context'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"  QUESTIONS FOR THIS CONVERSATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Fact Question\n",
    "print(\"\\n--- Fact Question ---\")\n",
    "print(f\"Q: {sample_set['factQA']['question']}\")\n",
    "print(f\"A: {sample_set['factQA']['correct_answer']}\")\n",
    "\n",
    "# Belief Questions\n",
    "print(\"\\n--- Belief Questions (ToM) ---\")\n",
    "for i, bq in enumerate(sample_set['beliefQAs'][:2]):  # Show first 2\n",
    "    print(f\"\\nQ{i+1}: {bq['question']}\")\n",
    "    print(f\"Correct: {bq['correct_answer']}\")\n",
    "    print(f\"Wrong (false belief): {bq['wrong_answer']}\")\n",
    "    print(f\"ToM Type: {bq['tom_type']}\")\n",
    "\n",
    "# Answerability Questions\n",
    "print(\"\\n--- Answerability Questions ---\")\n",
    "print(f\"List Q: {sample_set['answerabilityQA_list']['question']}\")\n",
    "print(f\"Can answer: {sample_set['answerabilityQA_list']['correct_answer']}\")\n",
    "print(f\"Cannot answer: {sample_set['answerabilityQA_list']['wrong_answer']}\")\n",
    "\n",
    "# Info Accessibility Questions  \n",
    "print(\"\\n--- Info Accessibility Questions ---\")\n",
    "print(f\"List Q: {sample_set['infoAccessibilityQA_list']['question']}\")\n",
    "print(f\"Has access: {sample_set['infoAccessibilityQA_list']['correct_answer']}\")\n",
    "print(f\"No access: {sample_set['infoAccessibilityQA_list']['wrong_answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FANToM Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FANToM Dataset Statistics:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question Type</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Conversation Sets</td>\n",
       "      <td>870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fact Questions</td>\n",
       "      <td>870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Belief Questions (ToM)</td>\n",
       "      <td>1540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Answerability List Questions</td>\n",
       "      <td>870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Answerability Binary Questions</td>\n",
       "      <td>3571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Info Accessibility List Questions</td>\n",
       "      <td>870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Info Accessibility Binary Questions</td>\n",
       "      <td>3571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Question Type  Count\n",
       "0                    Conversation Sets    870\n",
       "1                       Fact Questions    870\n",
       "2               Belief Questions (ToM)   1540\n",
       "3         Answerability List Questions    870\n",
       "4       Answerability Binary Questions   3571\n",
       "5    Info Accessibility List Questions    870\n",
       "6  Info Accessibility Binary Questions   3571"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total questions: 11292\n"
     ]
    }
   ],
   "source": [
    "# Count questions per conversation set\n",
    "total_belief_qs = sum(len(row['beliefQAs']) for _, row in fantom_df.iterrows())\n",
    "total_answerability_binary = sum(len(row['answerabilityQAs_binary']) for _, row in fantom_df.iterrows())\n",
    "total_info_binary = sum(len(row['infoAccessibilityQAs_binary']) for _, row in fantom_df.iterrows())\n",
    "\n",
    "fantom_stats = [\n",
    "    {'Question Type': 'Conversation Sets', 'Count': len(fantom_df)},\n",
    "    {'Question Type': 'Fact Questions', 'Count': len(fantom_df)},  # 1 per set\n",
    "    {'Question Type': 'Belief Questions (ToM)', 'Count': total_belief_qs},\n",
    "    {'Question Type': 'Answerability List Questions', 'Count': len(fantom_df)},  # 1 per set\n",
    "    {'Question Type': 'Answerability Binary Questions', 'Count': total_answerability_binary},\n",
    "    {'Question Type': 'Info Accessibility List Questions', 'Count': len(fantom_df)},  # 1 per set\n",
    "    {'Question Type': 'Info Accessibility Binary Questions', 'Count': total_info_binary},\n",
    "]\n",
    "\n",
    "fantom_stats_df = pd.DataFrame(fantom_stats)\n",
    "print(\"FANToM Dataset Statistics:\\n\")\n",
    "display(fantom_stats_df)\n",
    "\n",
    "total_questions = sum(s['Count'] for s in fantom_stats[1:])  # Exclude conversation sets count\n",
    "print(f\"\\nTotal questions: {total_questions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SimpleToM\n",
    "\n",
    "SimpleToM from Allen AI provides a clean, focused evaluation of ToM capabilities across three question types.\n",
    "\n",
    "**Question types:**\n",
    "- **Mental-state QA**: Questions about characters' beliefs and knowledge\n",
    "- **Behavior QA**: Predicting actions based on mental states\n",
    "- **Judgment QA**: Evaluating decisions given information asymmetry\n",
    "\n",
    "**Paper:** [SimpleToM: Exposing the Gap between Explicit ToM Inference and Implicit ToM Application in LLMs](https://arxiv.org/abs/2410.13648)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SimpleToM Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tom_mental = load_dataset(\"allenai/SimpleToM\", 'mental-state-qa')\n",
    "simple_tom_behavior = load_dataset(\"allenai/SimpleToM\", 'behavior-qa')\n",
    "simple_tom_judgment = load_dataset(\"allenai/SimpleToM\", 'judgment-qa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mental-state QA: DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['id', 'story', 'question', 'scenario_name', 'choices', 'answerKey'],\n",
      "        num_rows: 1147\n",
      "    })\n",
      "})\n",
      "\n",
      "Behavior QA: DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['id', 'story', 'question', 'scenario_name', 'choices', 'answerKey'],\n",
      "        num_rows: 1147\n",
      "    })\n",
      "})\n",
      "\n",
      "Judgment QA: DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['id', 'story', 'question', 'scenario_name', 'choices', 'answerKey'],\n",
      "        num_rows: 1147\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"Mental-state QA:\", simple_tom_mental)\n",
    "print(\"\\nBehavior QA:\", simple_tom_behavior)\n",
    "print(\"\\nJudgment QA:\", simple_tom_judgment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data from SimpleToM\n",
    "\n",
    "View representative examples from each question type in SimpleToM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  Mental-State QA (Awareness Questions)\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Story</th>\n",
       "      <th>Question</th>\n",
       "      <th>Choices</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Scenario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>The bag of potato chips has moldy chips in it. Mary picks up the bag in the supermarket and walks to the cashier.</td>\n",
       "      <td>Is Mary likely to be aware that \"The bag of potato chips has moldy chips in it.\"?</td>\n",
       "      <td>(A) Yes | (B) No</td>\n",
       "      <td>B</td>\n",
       "      <td>food_item_in_grocery_store</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>The bag of potato chips has a dead cockroach in it. Mary picks up the bag in the supermarket and walks to the cashier.</td>\n",
       "      <td>Is Mary likely to be aware that \"The bag of potato chips has a dead cockroach in it.\"?</td>\n",
       "      <td>(A) Yes | (B) No</td>\n",
       "      <td>B</td>\n",
       "      <td>food_item_in_grocery_store</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>The sealed bag of potato chips has several fried rats in it. Mary picks up the bag in the supermarket and walks to the cashier.</td>\n",
       "      <td>Is Mary likely to be aware that \"The sealed bag of potato chips has several fried rats in it.\"?</td>\n",
       "      <td>(A) No | (B) Yes</td>\n",
       "      <td>A</td>\n",
       "      <td>food_item_in_grocery_store</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  Behavior QA (Action Prediction)\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Story</th>\n",
       "      <th>Question</th>\n",
       "      <th>Choices</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Scenario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>The bag of potato chips has moldy chips in it. Mary picks up the bag in the supermarket and walks to the cashier.</td>\n",
       "      <td>What will Mary likely do next?</td>\n",
       "      <td>(A) pay for the chips | (B) report the moldy chips</td>\n",
       "      <td>A</td>\n",
       "      <td>food_item_in_grocery_store</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>The bag of potato chips has a dead cockroach in it. Mary picks up the bag in the supermarket and walks to the cashier.</td>\n",
       "      <td>What will Mary likely do next?</td>\n",
       "      <td>(A) report the dead cockroach | (B) pay for the chips</td>\n",
       "      <td>B</td>\n",
       "      <td>food_item_in_grocery_store</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>The sealed bag of potato chips has several fried rats in it. Mary picks up the bag in the supermarket and walks to the cashier.</td>\n",
       "      <td>What will Mary likely do next?</td>\n",
       "      <td>(A) report the fried rats | (B) pay for the chips</td>\n",
       "      <td>B</td>\n",
       "      <td>food_item_in_grocery_store</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  Judgment QA (Behavior Evaluation)\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Story</th>\n",
       "      <th>Question</th>\n",
       "      <th>Choices</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Scenario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>The bag of potato chips has moldy chips in it. Mary picks up the bag in the supermarket and walks to the cashier.</td>\n",
       "      <td>Next, Mary \"pay for the chips\" and not \"report the moldy chips\". The behavior of Mary can be best described as:</td>\n",
       "      <td>(A) Reasonable | (B) Not reasonable</td>\n",
       "      <td>A</td>\n",
       "      <td>food_item_in_grocery_store</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>The bag of potato chips has a dead cockroach in it. Mary picks up the bag in the supermarket and walks to the cashier.</td>\n",
       "      <td>Next, Mary \"pay for the chips\" and not \"report the dead cockroach\". The behavior of Mary can be best described as:</td>\n",
       "      <td>(A) Not reasonable | (B) Reasonable</td>\n",
       "      <td>B</td>\n",
       "      <td>food_item_in_grocery_store</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>The sealed bag of potato chips has several fried rats in it. Mary picks up the bag in the supermarket and walks to the cashier.</td>\n",
       "      <td>Next, Mary \"pay for the chips\" and not \"report the fried rats\". The behavior of Mary can be best described as:</td>\n",
       "      <td>(A) Reasonable | (B) Not reasonable</td>\n",
       "      <td>A</td>\n",
       "      <td>food_item_in_grocery_store</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_simpletom_samples(dataset, title: str, n: int = 2):\n",
    "    \"\"\"Display n samples from a SimpleToM dataset split.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"  {title}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    samples = []\n",
    "    for i in range(min(n, len(dataset['test']))):\n",
    "        ex = dataset['test'][i]\n",
    "        choices_text = ' | '.join([f\"({label}) {text}\" for label, text in \n",
    "                                   zip(ex['choices']['label'], ex['choices']['text'])])\n",
    "        samples.append({\n",
    "            'Story': ex['story'],\n",
    "            'Question': ex['question'],\n",
    "            'Choices': choices_text,\n",
    "            'Answer': ex['answerKey'],\n",
    "            'Scenario': ex['scenario_name']\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(samples)\n",
    "    display(HTML(df.to_html(escape=False, index=False)))\n",
    "\n",
    "# Mental-state QA - Tests awareness/knowledge attribution\n",
    "display_simpletom_samples(simple_tom_mental, \"Mental-State QA (Awareness Questions)\", n=3)\n",
    "\n",
    "# Behavior QA - Tests action prediction based on mental states  \n",
    "display_simpletom_samples(simple_tom_behavior, \"Behavior QA (Action Prediction)\", n=3)\n",
    "\n",
    "# Judgment QA - Tests evaluation of behavior given information asymmetry\n",
    "display_simpletom_samples(simple_tom_judgment, \"Judgment QA (Behavior Evaluation)\", n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleToM Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpletom_stats = []\n",
    "for name, dataset in [('Mental-State QA', simple_tom_mental), \n",
    "                       ('Behavior QA', simple_tom_behavior),\n",
    "                       ('Judgment QA', simple_tom_judgment)]:\n",
    "    for split in dataset.keys():\n",
    "        simpletom_stats.append({\n",
    "            'Question Type': name,\n",
    "            'Split': split,\n",
    "            'Count': len(dataset[split])\n",
    "        })\n",
    "\n",
    "simpletom_stats_df = pd.DataFrame(simpletom_stats)\n",
    "print(\"SimpleToM Dataset Statistics:\\n\")\n",
    "display(simpletom_stats_df)\n",
    "print(f\"\\nTotal examples: {simpletom_stats_df['Count'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ToMBench\n",
    "\n",
    "ToMBench is a comprehensive bilingual (Chinese/English) Theory of Mind benchmark containing 2,860 testing samples across diverse real-world social scenarios.\n",
    "\n",
    "**Key features:**\n",
    "- **8 ToM Tasks**: False Belief, Strange Story, Faux-pas Recognition, Hinting, Scalar Implicature, Persuasion, Ambiguous Story, Unexpected Outcome\n",
    "- **31 ToM Abilities**: Based on the ATOMS framework covering Emotion, Desire, Intention, Knowledge, Belief, and Non-Literal Communication\n",
    "- **Bilingual**: Full Chinese and English versions of all questions\n",
    "\n",
    "**Paper:** [ToMBench: Benchmarking Theory of Mind in Large Language Models](https://arxiv.org/abs/2402.15052)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ToMBench Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all ToMBench JSONL files\n",
    "tombench_dir = Path('tombench/data')\n",
    "tombench_data = {}\n",
    "\n",
    "for jsonl_file in sorted(tombench_dir.glob('*.jsonl')):\n",
    "    task_name = jsonl_file.stem\n",
    "    examples = []\n",
    "    with open(jsonl_file) as f:\n",
    "        for line in f:\n",
    "            examples.append(json.loads(line))\n",
    "    tombench_data[task_name] = examples\n",
    "    \n",
    "print(f\"Loaded {len(tombench_data)} ToMBench task files:\")\n",
    "for task, examples in tombench_data.items():\n",
    "    print(f\"  - {task}: {len(examples)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data from ToMBench\n",
    "\n",
    "ToMBench includes both Chinese and English versions. Here are examples from different task types (showing English versions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tombench_sample(task_name: str, examples: list, n: int = 2):\n",
    "    \"\"\"Display n samples from a ToMBench task (English version).\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"  {task_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for i, ex in enumerate(examples[:n]):\n",
    "        print(f\"\\n--- Example {i+1} ---\")\n",
    "        print(f\"Ability: {ex.get('能力\\\\nABILITY', 'N/A')}\")\n",
    "        print(f\"\\nStory: {ex.get('STORY', 'N/A')}\")\n",
    "        print(f\"\\nQuestion: {ex.get('QUESTION', 'N/A')}\")\n",
    "        \n",
    "        # Display options (handle NaN values)\n",
    "        options = []\n",
    "        for opt in ['OPTION-A', 'OPTION-B', 'OPTION-C', 'OPTION-D']:\n",
    "            val = ex.get(opt)\n",
    "            if val and str(val) != 'nan' and pd.notna(val):\n",
    "                options.append(f\"{opt[-1]}) {val}\")\n",
    "        print(f\"Options: {' | '.join(options)}\")\n",
    "        print(f\"Answer: {ex.get('答案\\\\nANSWER', 'N/A')}\")\n",
    "\n",
    "# Show samples from key task types\n",
    "display_tombench_sample(\"False Belief Task\", tombench_data.get('False Belief Task', []), n=2)\n",
    "display_tombench_sample(\"Strange Story Task (Irony/Sarcasm)\", tombench_data.get('Strange Story Task', []), n=2)\n",
    "display_tombench_sample(\"Faux-pas Recognition Test\", tombench_data.get('Faux-pas Recognition Test', []), n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToMBench Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tombench_stats = []\n",
    "for task_name, examples in sorted(tombench_data.items()):\n",
    "    tombench_stats.append({\n",
    "        'Task': task_name,\n",
    "        'Count': len(examples)\n",
    "    })\n",
    "\n",
    "tombench_stats_df = pd.DataFrame(tombench_stats)\n",
    "print(\"ToMBench Dataset Statistics:\\n\")\n",
    "display(tombench_stats_df)\n",
    "print(f\"\\nTotal examples: {tombench_stats_df['Count'].sum()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
